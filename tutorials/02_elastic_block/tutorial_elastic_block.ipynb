{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUTORIAL 02 - Elastic block problem\n",
    "**_Keywords: POD-Galerkin method, vector problem_**\n",
    "\n",
    "### 1. Introduction\n",
    "In this Tutorial we consider a linear elasticity problem in a two-dimensional square domain $\\Omega$.\n",
    "\n",
    "The domain is partioned in nine square subdomains.\n",
    "\n",
    "Parameters of this problem include Young moduli of each subdomain, as well as lateral traction on the right side of square. In particular:\n",
    "* the ratio between the Young modulus of the each subdomain $\\Omega_{p+1}$, $p=0,\\dots,7$ and the top-right subdomain $\\Omega_9$ is denoted by $\\mu_p$, being\n",
    "\n",
    "$$\n",
    "\\mu_p \\in \\left[1, 100\\right] \\qquad \\text{for }p=0,\\dots,7.\n",
    "$$\n",
    "\n",
    "* the horizontal tractions on each boundary $\\Gamma_{p-6}$, $p=8,\\dots,10$, being\n",
    "\n",
    "$$\n",
    "\\mu_p \\in \\left[-1,1\\right] \\qquad \\text{for } p=8,\\dots, 10.\n",
    "$$\n",
    "\n",
    "For what concerns the remaining boundaries, the left boundary $\\Gamma_6$ is clamped, while the top and bottom boundaries $\\Gamma_1 \\cup \\Gamma_5$ are traction free.\n",
    "\n",
    "The parameter vector $\\boldsymbol{\\mu}$ is thus given by \n",
    "$$\n",
    "\\boldsymbol{\\mu} = (\\mu_0, \\cdots,\\mu_{10})\n",
    "$$\n",
    "on the parameter domain\n",
    "$$\n",
    "\\mathbb{P}=[1,100]^8\\times[-1,1]^3.\n",
    "$$\n",
    "\n",
    "In order to obtain a faster approximation of the problem we pursue a model reduction by means of a POD-Galerkin reduced order method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parametrized formulation\n",
    "\n",
    "Let $\\boldsymbol{u}(\\boldsymbol{\\mu})$ be the displacement in the domain $\\Omega$.\n",
    "\n",
    "In each subdomain $\\Omega_{p+1}$, $p=0,\\dots,7$, we assume an isotropic linear elastic material, characterized by the following Lam√® constants for plane strain\n",
    "$$\\lambda_1(\\mu_p) = \\frac{\\mu_p \\nu}{(1+\\nu)(1-2\\nu)},$$\n",
    "$$\\lambda_2(\\mu_p) = \\frac{\\mu_p}{2(1+\\nu)},$$\n",
    "for $\\nu=0.30$, with the following Piola-Kirchhoff tensor\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\pi}(\\boldsymbol{u}; \\mu_p) = \n",
    "\\lambda_1(\\mu_p)\\;\\text{tr}\\left[\\nabla_{S}\\boldsymbol{u}\\right]\\; \\boldsymbol{I} +\n",
    "2\\;\\lambda_2(\\mu_p)\\;\\nabla_{S}\\boldsymbol{u}\n",
    "$$\n",
    "where $\\nabla_{S}$ denotes the symmetric part of the gradient.\n",
    "\n",
    "Similarly, the Piola-Kirchhoff tensor in the top right subdomain $\\Omega_9$ is given by $\\boldsymbol{\\pi}(\\boldsymbol{u}; 1)$.\n",
    "\n",
    "Thus, the Piola-Kirchhoff tensor on the domain $\\Omega$ can be obtained as\n",
    "$$\n",
    "\\boldsymbol{P}(\\boldsymbol{u}; \\boldsymbol{\\mu}) = \n",
    "\\Lambda_1(\\boldsymbol{\\mu})\\;\\text{tr}\\left[\\nabla_{S}\\boldsymbol{u}\\right]\\; \\boldsymbol{I} +\n",
    "2\\;\\Lambda_2(\\boldsymbol{\\mu})\\;\\nabla_{S}\\boldsymbol{u}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\Lambda_1(\\boldsymbol{\\mu}) = \\sum_{p=0}^{7} \\lambda_1(\\mu_p) \\mathbb{1}_{\\Omega_{p+1}} + \\lambda_1(1) \\mathbb{1}_{\\Omega_{9}}\n",
    "$$\n",
    "$$\n",
    "\\Lambda_2(\\boldsymbol{\\mu}) = \\sum_{p=0}^{7} \\lambda_2(\\mu_p) \\mathbb{1}_{\\Omega_{p+1}} + \\lambda_2(1) \\mathbb{1}_{\\Omega_{9}}\n",
    "$$\n",
    "\n",
    "The strong formulation of the parametrized problem is given by: for a given parameter $\\boldsymbol{\\mu}\\in\\mathbb{P}$, find $\\boldsymbol{u}(\\boldsymbol{\\mu})$ such that\n",
    "$$\n",
    "\\begin{cases}\n",
    "\t- \\text{div} \\boldsymbol{P}(\\boldsymbol{u}(\\boldsymbol{\\mu}); \\boldsymbol{\\mu})) = 0 & \\text{in } \\Omega,\\\\\n",
    "    \\boldsymbol{P}(\\boldsymbol{u}(\\boldsymbol{\\mu}); \\boldsymbol{\\mu})) \\mathbf{n} = \\mathbf{0} & \\text{on } \\Gamma_{1},\\\\\n",
    "\t\\boldsymbol{P}(\\boldsymbol{u}(\\boldsymbol{\\mu}); \\boldsymbol{\\mu})) \\mathbf{n} = \\mu_p \\mathbf{n} & \\text{on } \\Gamma_{p-6}, p=8,\\dots, 10,\\\\\n",
    "    \\boldsymbol{P}(\\boldsymbol{u}(\\boldsymbol{\\mu}); \\boldsymbol{\\mu})) \\mathbf{n} = \\mathbf{0} & \\text{on } \\Gamma_{5},\\\\\n",
    "\t\\boldsymbol{u}(\\boldsymbol{\\mu}) = \\boldsymbol{0} & \\text{on } \\Gamma_{6},\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "where $\\mathbf{n}$ denotes the outer normal to the boundary $\\partial\\Omega$.\n",
    "\n",
    "The corresponding weak formulation reads: for a given parameter $\\boldsymbol{\\mu}\\in\\mathbb{P}$, find $u(\\boldsymbol{\\mu})\\in\\mathbb{V}$ such that\n",
    "\n",
    "$$a\\left(u(\\boldsymbol{\\mu}),v;\\boldsymbol{\\mu}\\right)=f(v;\\boldsymbol{\\mu})\\quad \\forall v\\in\\mathbb{V}$$\n",
    "\n",
    "where\n",
    "\n",
    "* the function space $\\mathbb{V}$ is defined as\n",
    "$$\n",
    "\\mathbb{V} = \\{\\boldsymbol{v}\\in H^1(\\Omega; \\mathbb{R}^2) : \\boldsymbol{v}|_{\\Gamma_{6}}=\\boldsymbol{0}\\}\n",
    "$$\n",
    "* the parametrized bilinear form $a(\\cdot, \\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$a(\\boldsymbol{u}, \\boldsymbol{v}; \\boldsymbol{\\mu})=\\int_{\\Omega}\n",
    "\\left\\{\n",
    "\\Lambda_1(\\boldsymbol{\\mu})\\;\\text{tr}\\left[\\nabla_{S}\\boldsymbol{u}\\right]\\;\\text{tr}\\left[\\nabla_{S}\\boldsymbol{v}\\right] + 2\\;\\Lambda_2(\\boldsymbol{\\mu})\\;\\nabla_{S}\\boldsymbol{u} : \\nabla_{S}\\boldsymbol{v}\n",
    "\\right\\}  d\\boldsymbol{x}\n",
    "$$,\n",
    "* the parametrized linear form $f(\\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$f(\\boldsymbol{v}; \\boldsymbol{\\mu})= \\sum_{p=8}^{10} \\mu_p \\int_{\\Gamma_{p-6}} \\boldsymbol{v} \\cdot \\mathbf{n} \\ ds$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from mlnics import NN, Losses, Normalization, RONNData, IO, Training, ErrorAnalysis\n",
    "from dolfin import *\n",
    "from rbnics import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Affine decomposition\n",
    "\n",
    "For this problem the affine decomposition is straightforward. Indeed, owing to the definitions of $\\Lambda_1(\\boldsymbol{\\mu})$ and $\\Lambda_2(\\boldsymbol{\\mu})$, we have:\n",
    "$$\n",
    "a(\\boldsymbol{u}, \\boldsymbol{v}; \\boldsymbol{\\mu}) = \\sum_{p=0}^7 \\underbrace{\\mu_{\\color{red} p}}_{\\Theta^{a}_{\\color{red} p}(\\boldsymbol{\\mu})} \\underbrace{\\int_{\\Omega_{\\color{red}{p + 1}}}\n",
    "\\left\\{\\lambda_1(1)\\;\\text{tr}\\left[\\nabla_{S}\\boldsymbol{u}\\right]\\;\\text{tr}\\left[\\nabla_{S}\\boldsymbol{v}\\right] + 2\\;\\lambda_2(1)\\;\\nabla_{S}\\boldsymbol{u} : \\nabla_{S}\\boldsymbol{v} \\right\\}  d\\boldsymbol{x}}_{a_{\\color{red} p}(\\boldsymbol{u}, \\boldsymbol{v})} +\\\\\n",
    "\\underbrace{1}_{\\Theta^{a}_{\\color{red} 8}(\\boldsymbol{\\mu})} \\underbrace{\\int_{\\Omega_{\\color{red} 9}}\n",
    "\\left\\{\\lambda_1(1)\\;\\text{tr}\\left[\\nabla_{S}\\boldsymbol{u}\\right]\\;\\text{tr}\\left[\\nabla_{S}\\boldsymbol{v}\\right] + 2\\;\\lambda_2(1)\\;\\nabla_{S}\\boldsymbol{u} : \\nabla_{S}\\boldsymbol{v} \\right\\}  d\\boldsymbol{x}}_{a_{\\color{red} 8}(\\boldsymbol{u}, \\boldsymbol{v})}\\\\\n",
    "$$\n",
    "$$\n",
    "f(\\boldsymbol{v}; \\boldsymbol{\\mu}) = \n",
    "\\sum_{p=8}^{10} \\underbrace{\\mu_{\\color{red} p}}_{\\Theta^{f}_{\\color{red}{p-8}}(\\boldsymbol{\\mu})} \\underbrace{\\int_{\\Gamma_{\\color{red}{p-6}}} \\boldsymbol{v} \\cdot \\mathbf{n}}_{f_{\\color{red}{p-8}}(\\boldsymbol{v})}.\n",
    "$$\n",
    "\n",
    "We will implement the numerical discretization of the problem in the class\n",
    "```\n",
    "class ElasticBlock(EllipticCoerciveProblem):\n",
    "```\n",
    "by specifying the coefficients $\\Theta^{a}_*(\\boldsymbol{\\mu})$ and $\\Theta^{f}_*(\\boldsymbol{\\mu})$ in the method\n",
    "```\n",
    "    def compute_theta(self, term):     \n",
    "```\n",
    "and the bilinear forms $a_*(\\boldsymbol{u}, \\boldsymbol{v})$ and linear forms $f_*(\\boldsymbol{v})$ in\n",
    "```\n",
    "    def assemble_operator(self, term):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticBlock(EllipticCoerciveProblem):\n",
    "\n",
    "    # Default initialization of members\n",
    "    def __init__(self, V, **kwargs):\n",
    "        # Call the standard initialization\n",
    "        EllipticCoerciveProblem.__init__(self, V, **kwargs)\n",
    "        # ... and also store FEniCS data structures for assembly\n",
    "        assert \"subdomains\" in kwargs\n",
    "        assert \"boundaries\" in kwargs\n",
    "        self.subdomains, self.boundaries = kwargs[\"subdomains\"], kwargs[\"boundaries\"]\n",
    "        self.u = TrialFunction(V)\n",
    "        self.v = TestFunction(V)\n",
    "        self.dx = Measure(\"dx\")(subdomain_data=self.subdomains)\n",
    "        self.ds = Measure(\"ds\")(subdomain_data=self.boundaries)\n",
    "        # ...\n",
    "        self.f = Constant((1.0, 0.0))\n",
    "        self.E = 1.0\n",
    "        self.nu = 0.3\n",
    "        self.lambda_1 = self.E * self.nu / ((1.0 + self.nu) * (1.0 - 2.0 * self.nu))\n",
    "        self.lambda_2 = self.E / (2.0 * (1.0 + self.nu))\n",
    "\n",
    "    # Return custom problem name\n",
    "    def name(self):\n",
    "        return \"ElasticBlock\"\n",
    "\n",
    "    # Return theta multiplicative terms of the affine expansion of the problem.\n",
    "    def compute_theta(self, term):\n",
    "        mu = self.mu\n",
    "        if term == \"a\":\n",
    "            theta_a0 = mu[0]\n",
    "            theta_a1 = 1.\n",
    "            theta_a2 = 1.\n",
    "            theta_a3 = 1.\n",
    "            theta_a4 = 1.\n",
    "            theta_a5 = 1.\n",
    "            theta_a6 = 1.\n",
    "            theta_a7 = 1.\n",
    "            theta_a8 = 1.\n",
    "            return (theta_a0, theta_a1, theta_a2, theta_a3, theta_a4, theta_a5, theta_a6, theta_a7, theta_a8)\n",
    "        elif term == \"f\":\n",
    "            theta_f0 = mu[1]\n",
    "            theta_f1 = 1.\n",
    "            theta_f2 = 1.\n",
    "            return (theta_f0, theta_f1, theta_f2)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for compute_theta().\")\n",
    "\n",
    "    # Return forms resulting from the discretization of the affine expansion of the problem operators.\n",
    "    def assemble_operator(self, term):\n",
    "        v = self.v\n",
    "        dx = self.dx\n",
    "        if term == \"a\":\n",
    "            u = self.u\n",
    "            a0 = self.elasticity(u, v) * dx(1)\n",
    "            a1 = self.elasticity(u, v) * dx(2)\n",
    "            a2 = self.elasticity(u, v) * dx(3)\n",
    "            a3 = self.elasticity(u, v) * dx(4)\n",
    "            a4 = self.elasticity(u, v) * dx(5)\n",
    "            a5 = self.elasticity(u, v) * dx(6)\n",
    "            a6 = self.elasticity(u, v) * dx(7)\n",
    "            a7 = self.elasticity(u, v) * dx(8)\n",
    "            a8 = self.elasticity(u, v) * dx(9)\n",
    "            return (a0, a1, a2, a3, a4, a5, a6, a7, a8)\n",
    "        elif term == \"f\":\n",
    "            ds = self.ds\n",
    "            f = self.f\n",
    "            f0 = inner(f, v) * ds(2)\n",
    "            f1 = inner(f, v) * ds(3)\n",
    "            f2 = inner(f, v) * ds(4)\n",
    "            return (f0, f1, f2)\n",
    "        elif term == \"dirichlet_bc\":\n",
    "            bc0 = [DirichletBC(self.V, Constant((0.0, 0.0)), self.boundaries, 6)]\n",
    "            return (bc0,)\n",
    "        elif term == \"inner_product\":\n",
    "            u = self.u\n",
    "            x0 = inner(u, v) * dx + inner(grad(u), grad(v)) * dx\n",
    "            return (x0,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for assemble_operator().\")\n",
    "\n",
    "    # Auxiliary function to compute the elasticity bilinear form\n",
    "    def elasticity(self, u, v):\n",
    "        lambda_1 = self.lambda_1\n",
    "        lambda_2 = self.lambda_2\n",
    "        return 2.0 * lambda_2 * inner(sym(grad(u)), sym(grad(v))) + lambda_1 * tr(sym(grad(u))) * tr(sym(grad(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main program\n",
    "### 4.1. Read the mesh for this problem\n",
    "The mesh was generated by the [data/generate_mesh.ipynb](data/generate_mesh.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = Mesh(\"data/elastic_block.xml\")\n",
    "subdomains = MeshFunction(\"size_t\", mesh, \"data/elastic_block_physical_region.xml\")\n",
    "boundaries = MeshFunction(\"size_t\", mesh, \"data/elastic_block_facet_region.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create Finite Element space (Lagrange P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = VectorFunctionSpace(mesh, \"Lagrange\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Allocate an object of the ElasticBlock class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = ElasticBlock(V, subdomains=subdomains, boundaries=boundaries)\n",
    "mu_range = [\n",
    "    (1.0, 10.0),\n",
    "    (-1.0, 1.0)\n",
    "]\n",
    "problem.set_mu_range(mu_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Prepare reduction with a POD-Galerkin method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method = PODGalerkin(problem)\n",
    "reduction_method.set_Nmax(20)\n",
    "reduction_method.set_tolerance(2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Perform the offline phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Fit Reduction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method.initialize_training_set(500)\n",
    "reduced_problem = reduction_method.offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 Train PINN\n",
    "\n",
    "Given a training set $X_{PINN} = (\\boldsymbol{\\mu}^{(1)}, \\dots, \\boldsymbol{\\mu}^{(n)})$ of parameters for the PDE, we train a Physics-Informed Neural Network (PINN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PINN}(X_{PINN}; W) = \\frac1n \\sum_{i=1}^n \\|A(\\boldsymbol{\\mu^{(i)}}) \\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\boldsymbol{f}(\\boldsymbol{\\mu}^{(i)})\\|_2^2$$\n",
    "\n",
    "over $W$, where for a given $\\boldsymbol{\\mu}$, $A(\\boldsymbol{\\mu})$ is the assembled matrix corresponding to the bilinear form $a$ and $\\boldsymbol{f}(\\boldsymbol{\\mu})$ is the assembled vector corresponding to the linear form $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pinn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pinn = Normalization.StandardNormalization()\n",
    "\n",
    "pinn_net  = NN.RONN(\"PINN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pinn_loss = Losses.PINN_Loss(pinn_net, output_normalization_pinn)\n",
    "data      = RONNData.RONNDataLoader(pinn_net, validation_proportion=0.2, \n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(pinn_net.parameters(), lr=0.001)\n",
    "\n",
    "pinn_trainer = Training.PINNTrainer(\n",
    "    pinn_net, data, pinn_loss, optimizer,\n",
    "    input_normalization_pinn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pinn_net, data, pinn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pinn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pinn_trainer, pinn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 Train PDNN\n",
    "\n",
    "Given a training set $X_{PDNN} = ((\\boldsymbol{\\mu}^{(1)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(1)})), \\dots, (\\boldsymbol{\\mu}^{(n)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(n)})))$ of parameter and high fidelity solution pairs for the PDE, we train a Projection-Driven Neural Network (PDNN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "$$L_{PDNN}(X_{PDNN}; W) = \\frac1n \\sum_{i=1}^n \\|\\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu}^{(i)})\\|_2^2,$$\n",
    "where for a given $\\boldsymbol{\\mu}$, $\\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu})$ is the projection of $\\operatorname{HF}(\\boldsymbol{\\mu})$ onto the reduced order solution space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_normalization_pdnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pdnn = Normalization.StandardNormalization()\n",
    "\n",
    "pdnn_net  = NN.RONN(\"PDNN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pdnn_loss = Losses.PDNN_Loss(pdnn_net, output_normalization_pdnn)\n",
    "data      = RONNData.RONNDataLoader(pdnn_net, validation_proportion=0.2)\n",
    "optimizer = torch.optim.Adam(pdnn_net.parameters(), lr=0.001)\n",
    "\n",
    "pdnn_trainer = Training.PDNNTrainer(\n",
    "    pdnn_net, data, pdnn_loss, optimizer,\n",
    "    input_normalization_pdnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pdnn_net, data, pdnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pdnn_trainer, pdnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Train PRNN\n",
    "\n",
    "We train a Physics-Reinforced Neural Network (PRNN) $N_W(\\boldsymbol{\\mu})$ dependnent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PRNN}(X_{PINN}, X_{PDNN}; W) = L_{PINN}(X_{PINN}; W) + \\omega L_{PDNN}(X_{PDNN}; W),$$\n",
    "\n",
    "where $\\omega$ is a scaling parameter which can be chosen freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_prnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_prnn = Normalization.StandardNormalization()\n",
    "\n",
    "omega = 1.\n",
    "prnn_net  = NN.RONN(f\"PRNN_{omega}\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "prnn_loss = Losses.PRNN_Loss(prnn_net, output_normalization_prnn, omega=omega)\n",
    "data      = RONNData.RONNDataLoader(prnn_net, validation_proportion=0.2,\n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(prnn_net.parameters(), lr=0.001)\n",
    "\n",
    "prnn_trainer = Training.PRNNTrainer(\n",
    "    prnn_net, data, prnn_loss, optimizer,\n",
    "    input_normalization_prnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    prnn_net, data, prnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(prnn_trainer, prnn_net, separate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Perform an error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 Reduction Method Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduction_method.initialize_testing_set(100)\n",
    "reduction_method.error_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 PINN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mu = torch.tensor(reduction_method.testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pinn_net, test_mu, input_normalization_pinn, output_normalization_pinn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pinn_net, (1.0, -1.0), input_normalization_pinn, output_normalization_pinn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3 PDNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pdnn_net, test_mu, input_normalization_pdnn, output_normalization_pdnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pdnn_net, (1.0, -1.0), input_normalization_pdnn, output_normalization_pdnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.4 PRNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    prnn_net, test_mu, input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    prnn_net, (1.0, -1.0), input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.5 Neural Network Error Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = dict()\n",
    "nets[\"pinn_net\"] = pinn_net\n",
    "nets[\"pdnn_net\"] = pdnn_net\n",
    "nets[\"prnn_net\"] = prnn_net\n",
    "\n",
    "input_normalizations = dict()\n",
    "input_normalizations[\"pinn_net\"] = input_normalization_pinn\n",
    "input_normalizations[\"pdnn_net\"] = input_normalization_pdnn\n",
    "input_normalizations[\"prnn_net\"] = input_normalization_prnn\n",
    "\n",
    "output_normalizations = dict()\n",
    "output_normalizations[\"pinn_net\"] = output_normalization_pinn\n",
    "output_normalizations[\"pdnn_net\"] = output_normalization_pdnn\n",
    "output_normalizations[\"prnn_net\"] = output_normalization_prnn\n",
    "\n",
    "_ = ErrorAnalysis.error_analysis_by_network(\n",
    "    nets, test_mu, input_normalizations, output_normalizations, euclidean=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Perform a speedup analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method.initialize_testing_set(100)\n",
    "reduction_method.speedup_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
