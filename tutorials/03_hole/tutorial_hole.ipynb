{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUTORIAL 03 - Geometrical parametrization\n",
    "**_Keywords: geometrical parametrization_**\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "This Tutorial introduces problems featuring a geometrical parametrization, by solving a thermal conduction problem on a parametrized computational domain $\\Omega_o(\\boldsymbol{\\mu})$.\n",
    "\n",
    "The problem is characterized by three parameters. The parameters $\\mu_0$ and $\\mu_1$ are related to the shape of the central hole and vary in the following intervals\n",
    "$$\n",
    "\\mu_0\\in[0.5,1.5] \\quad \\text{and}\\quad \\mu_1\\in[0.5,1.5].\n",
    "$$\n",
    "\n",
    "The parameter $\\mu_2$ is the Biot number, which allows to parametrize heat exchange with a surrounding exterior fluid (e.g., air) in the following interval\n",
    "$$\n",
    "\\mu_2\\in[0.01,1];\n",
    "$$\n",
    "the diffusion process on the external boundaries $\\Gamma_{o, 5} \\cup \\Gamma_{o, 6} \\cup \\Gamma_{o, 7} \\cup \\Gamma_{o, 8}$ will be affected by this parameter.\n",
    "\n",
    "The parameter vector $\\boldsymbol{\\mu}$ is thus given by \n",
    "$$\n",
    "\\boldsymbol{\\mu} = (\\mu_0, \\mu_1, \\mu_2)\n",
    "$$\n",
    "on the parameter domain\n",
    "$$\n",
    "\\mathbb{P}=[0.5,1.5]^2\\times[0.01,1].\n",
    "$$\n",
    "\n",
    "In order to obtain a faster approximation of the problem, and avoiding _any_ remeshing, we pursue a model reduction by means of a POD-Galerkin reduced order method from a fixed reference domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parametrized formulation\n",
    "\n",
    "Let $u_o(\\boldsymbol{\\mu})$ be the temperature in the domain $\\Omega_o(\\boldsymbol{\\mu})$.\n",
    "\n",
    "We will directly provide a weak formulation for this problem for a given parameter $\\boldsymbol{\\mu}\\in\\mathbb{P}$, find $u_o(\\boldsymbol{\\mu})\\in\\mathbb{V}_o(\\boldsymbol{\\mu})$ such that\n",
    "\n",
    "$$a_o\\left(u_o(\\boldsymbol{\\mu}),v_o;\\boldsymbol{\\mu}\\right)=f_o(v_o;\\boldsymbol{\\mu})\\quad \\forall v_o\\in\\mathbb{V}_o(\\boldsymbol{\\mu})$$\n",
    "\n",
    "where\n",
    "\n",
    "* the function space $\\mathbb{V}_o(\\boldsymbol{\\mu})$ is defined as\n",
    "$$\n",
    "\\mathbb{V}_o(\\boldsymbol{\\mu}) = H^1(\\Omega(\\boldsymbol{\\mu})).\n",
    "$$\n",
    "Note that, in contrast to the previous tutorials, the function space is parameter dependent due to the shape variation. \n",
    "* the parametrized bilinear form $a_o(\\cdot, \\cdot; \\boldsymbol{\\mu}): \\mathbb{V}_o(\\boldsymbol{\\mu}) \\times \\mathbb{V}_o(\\boldsymbol{\\mu}) \\to \\mathbb{R}$ is defined by\n",
    "$$a_o(u_o,v_o;\\boldsymbol{\\mu}) = \\int_{\\Omega_o(\\boldsymbol{\\mu})}\\nabla u_o\\cdot\\nabla v_o \\ d\\boldsymbol{x} + \\mu_2\\left(\\int_{\\Gamma_{o,5}}u_o\\,v_o \\ ds + \\int_{\\Gamma_{o,6}}u_o\\,v_o \\ ds + \\int_{\\Gamma_{o,7}}u_o\\,v_o \\ ds + \\int_{\\Gamma_{o,8}}u_o\\,v_o \\ ds\\right),$$\n",
    "* the parametrized linear form $f_o(\\cdot; \\boldsymbol{\\mu}): \\mathbb{V}_o(\\boldsymbol{\\mu}) \\to \\mathbb{R}$ is defined by\n",
    "$$f_o(v_o;\\boldsymbol{\\mu}) = \\int_{\\Gamma_{o,1}(\\boldsymbol{\\mu})}v_o \\ ds + \\int_{\\Gamma_{o,2}(\\boldsymbol{\\mu})}v_o \\ ds + \\int_{\\Gamma_{o,3}(\\boldsymbol{\\mu})}v_o \\ ds + \\int_{\\Gamma_{o,4}(\\boldsymbol{\\mu})}v_o \\ ds.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from mlnics import NN, Losses, Normalization, RONNData, IO, Training, ErrorAnalysis\n",
    "from dolfin import *\n",
    "from rbnics import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Affine decomposition\n",
    "\n",
    "In order to obtain an affine decomposition, we need to recast the problem on a fixed, parameter _independent_, reference domain, as follows:\n",
    "\n",
    "1. Choose $\\Omega = \\Omega_o((\\mu_0, \\mu_1) \\equiv (1, 1))$ as reference domain, which we generate through the generate_mesh notebook provided in the _data_ folder.\n",
    "2. Define a map $\\boldsymbol{T}(\\boldsymbol{\\cdot}; \\boldsymbol{\\mu}): \\Omega \\to \\Omega_o(\\boldsymbol{\\mu})$ to carry out a pull back of the problem from the parametrized domain $\\Omega_o(\\boldsymbol{\\mu})$ to the reference one $\\Omega$. Since we aim at obtaining an affine decomposition, the map $\\boldsymbol{T}(\\boldsymbol{\\cdot}; \\boldsymbol{\\mu})$ should be affine in its first argument. This is possible by taking a partition of the reference domain in several triangular subdomains.\n",
    "3. Define a reference function space $$\\mathbb{V} = H^1(\\Omega),$$ to which the pulled back solution\n",
    "$$u(\\boldsymbol{\\cdot}; \\boldsymbol{\\mu}) = u_o(\\boldsymbol{T}(\\boldsymbol{\\cdot}; \\boldsymbol{\\mu}); \\boldsymbol{\\mu})$$ belongs\n",
    "4. Pull back the bilinear form $a_o(\\cdot, \\cdot; \\boldsymbol{\\mu})$ and linear form $f_o(\\cdot; \\boldsymbol{\\mu})$ onto the reference domain $\\Omega$ by change of variables. Note that, due to the definition of the map $\\boldsymbol{T}(\\boldsymbol{\\cdot}; \\boldsymbol{\\mu})$, the pull back will be different from one subdomain to the other. Call\n",
    "$$a(\\cdot, \\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$$\n",
    "$$f(\\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\to \\mathbb{R}$$\n",
    "the resulting forms (and note that their arguments are now defined on $\\Omega$).\n",
    "5. Solve the equivalent problem: for a given parameter $\\boldsymbol{\\mu}\\in\\mathbb{P}$, find $u(\\boldsymbol{\\mu})\\in\\mathbb{V}$ such that\n",
    "\n",
    "$$a\\left(u(\\boldsymbol{\\mu}),v;\\boldsymbol{\\mu}\\right)=f(v;\\boldsymbol{\\mu})\\quad \\forall v\\in\\mathbb{V}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@PullBackFormsToReferenceDomain()\n",
    "@AffineShapeParametrization(\"data/hole_vertices_mapping.vmp\")\n",
    "class Hole(EllipticCoerciveProblem):\n",
    "\n",
    "    # Default initialization of members\n",
    "    def __init__(self, V, **kwargs):\n",
    "        # Call the standard initialization\n",
    "        EllipticCoerciveProblem.__init__(self, V, **kwargs)\n",
    "        # ... and also store FEniCS data structures for assembly\n",
    "        assert \"subdomains\" in kwargs\n",
    "        assert \"boundaries\" in kwargs\n",
    "        self.subdomains, self.boundaries = kwargs[\"subdomains\"], kwargs[\"boundaries\"]\n",
    "        self.u = TrialFunction(V)\n",
    "        self.v = TestFunction(V)\n",
    "        self.dx = Measure(\"dx\")(subdomain_data=subdomains)\n",
    "        self.ds = Measure(\"ds\")(subdomain_data=boundaries)\n",
    "        self.subdomains = subdomains\n",
    "        self.boundaries = boundaries\n",
    "\n",
    "    # Return custom problem name\n",
    "    def name(self):\n",
    "        return \"Hole\"\n",
    "\n",
    "    # Return theta multiplicative terms of the affine expansion of the problem.\n",
    "    def compute_theta(self, term):\n",
    "        mu = self.mu\n",
    "        if term == \"a\":\n",
    "            theta_a0 = 1.0\n",
    "            theta_a1 = mu[2]\n",
    "            return (theta_a0, theta_a1)\n",
    "        elif term == \"f\":\n",
    "            theta_f0 = 1.0\n",
    "            return (theta_f0, )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for compute_theta().\")\n",
    "\n",
    "    # Return forms resulting from the discretization of the affine expansion of the problem operators.\n",
    "    def assemble_operator(self, term):\n",
    "        u = self.u\n",
    "        v = self.v\n",
    "        dx = self.dx\n",
    "        ds = self.ds\n",
    "        if term == \"a\":\n",
    "            a0 = inner(grad(u), grad(v)) * dx\n",
    "            a1 = inner(u, v) * ds(5) + inner(u, v) * ds(6) + inner(u, v) * ds(7) + inner(u, v) * ds(8)\n",
    "            return (a0, a1)\n",
    "        elif term == \"f\":\n",
    "            f0 = v * ds(1) + v * ds(2) + v * ds(3) + v * ds(4)\n",
    "            return (f0, )\n",
    "        elif term == \"inner_product\":\n",
    "            x0 = u * v * dx + inner(grad(u), grad(v)) * dx\n",
    "            return (x0,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for assemble_operator().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main program\n",
    "### 4.1. Read the mesh for this problem\n",
    "The mesh was generated by the [data/generate_mesh.ipynb](data/generate_mesh.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = Mesh(\"data/hole.xml\")\n",
    "subdomains = MeshFunction(\"size_t\", mesh, \"data/hole_physical_region.xml\")\n",
    "boundaries = MeshFunction(\"size_t\", mesh, \"data/hole_facet_region.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create Finite Element space (Lagrange P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = FunctionSpace(mesh, \"Lagrange\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Allocate an object of the Hole class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = Hole(V, subdomains=subdomains, boundaries=boundaries)\n",
    "mu_range = [(0.5, 1.5), (0.5, 1.5), (0.01, 1.0)]\n",
    "problem.set_mu_range(mu_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Prepare reduction with a POD-Galerkin method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method = PODGalerkin(problem)\n",
    "reduction_method.set_Nmax(20)\n",
    "reduction_method.set_tolerance(1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Perform the offline phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Fit Reduction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method.initialize_training_set(100)\n",
    "reduced_problem = reduction_method.offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 Train PINN\n",
    "\n",
    "Given a training set $X_{PINN} = (\\boldsymbol{\\mu}^{(1)}, \\dots, \\boldsymbol{\\mu}^{(n)})$ of parameters for the PDE, we train a Physics-Informed Neural Network (PINN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PINN}(X_{PINN}; W) = \\frac1n \\sum_{i=1}^n \\|A(\\boldsymbol{\\mu^{(i)}}) \\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\boldsymbol{f}(\\boldsymbol{\\mu}^{(i)})\\|_2^2,$$\n",
    "\n",
    "over $W$, where for a given $\\boldsymbol{\\mu}$, $A(\\boldsymbol{\\mu})$ is the assembled matrix corresponding to the pull back of $a_o$ and $\\boldsymbol{f}(\\boldsymbol{\\mu})$ is the assembled vector corresponding to the pull back of $f_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pinn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pinn = Normalization.StandardNormalization()\n",
    "\n",
    "pinn_net  = NN.RONN(\"PINN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pinn_loss = Losses.PINN_Loss(pinn_net, output_normalization_pinn)\n",
    "data      = RONNData.RONNDataLoader(pinn_net, validation_proportion=0.2, \n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(pinn_net.parameters(), lr=0.001)\n",
    "\n",
    "pinn_trainer = Training.PINNTrainer(\n",
    "    pinn_net, data, pinn_loss, optimizer,\n",
    "    input_normalization_pinn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pinn_net, data, pinn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pinn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pinn_trainer, pinn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### 4.5.3 Train PDNN\n",
    "\n",
    "Given a training set $X_{PDNN} = ((\\boldsymbol{\\mu}^{(1)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(1)})), \\dots, (\\boldsymbol{\\mu}^{(n)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(n)})))$ of parameter and high fidelity solution pairs for the PDE, we train a Projection-Driven Neural Network (PDNN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "$$L_{PDNN}(X_{PDNN}; W) = \\frac1n \\sum_{i=1}^n \\|\\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu}^{(i)})\\|_2^2,$$\n",
    "where for a given $\\boldsymbol{\\mu}$, $\\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu})$ is the projection of $\\operatorname{HF}(\\boldsymbol{\\mu})$ onto the reduced order solution space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pdnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pdnn = Normalization.StandardNormalization()\n",
    "\n",
    "pdnn_net  = NN.RONN(\"PDNN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pdnn_loss = Losses.PDNN_Loss(pdnn_net, output_normalization_pdnn)\n",
    "data      = RONNData.RONNDataLoader(pdnn_net, validation_proportion=0.2)\n",
    "optimizer = torch.optim.Adam(pdnn_net.parameters(), lr=0.001)\n",
    "\n",
    "pdnn_trainer = Training.PDNNTrainer(\n",
    "    pdnn_net, data, pdnn_loss, optimizer,\n",
    "    input_normalization_pdnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pdnn_net, data, pdnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pdnn_trainer, pdnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Train PRNN\n",
    "\n",
    "We train a Physics-Reinforced Neural Network (PRNN) $N_W(\\boldsymbol{\\mu})$ dependnent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PRNN}(X_{PINN}, X_{PDNN}; W) = L_{PINN}(X_{PINN}; W) + \\omega L_{PDNN}(X_{PDNN}; W),$$\n",
    "\n",
    "where $\\omega$ is a scaling parameter which can be chosen freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_prnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_prnn = Normalization.StandardNormalization()\n",
    "\n",
    "omega = 1.\n",
    "prnn_net  = NN.RONN(f\"PRNN_{omega}\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "prnn_loss = Losses.PRNN_Loss(prnn_net, output_normalization_prnn, omega=omega)\n",
    "data      = RONNData.RONNDataLoader(prnn_net, validation_proportion=0.2,\n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(prnn_net.parameters(), lr=0.001)\n",
    "\n",
    "prnn_trainer = Training.PRNNTrainer(\n",
    "    prnn_net, data, prnn_loss, optimizer,\n",
    "    input_normalization_prnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    prnn_net, data, prnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(prnn_trainer, prnn_net, separate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Perform an error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 Reduction Method Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduction_method.initialize_testing_set(100)\n",
    "reduction_method.error_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 PINN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mu = torch.tensor(reduction_method.testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pinn_net, test_mu, input_normalization_pinn, output_normalization_pinn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pinn_net, (0.5, 0.5, 0.01), input_normalization_pinn, output_normalization_pinn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3 PDNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pdnn_net, test_mu, input_normalization_pdnn, output_normalization_pdnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pdnn_net, (0.5, 0.5, 0.01), input_normalization_pdnn, output_normalization_pdnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.4 PRNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    prnn_net, test_mu, input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    prnn_net, (0.5, 0.5, 0.01), input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.5 Neural Network Error Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = dict()\n",
    "nets[\"pinn_net\"] = pinn_net\n",
    "nets[\"pdnn_net\"] = pdnn_net\n",
    "nets[\"prnn_net\"] = prnn_net\n",
    "\n",
    "input_normalizations = dict()\n",
    "input_normalizations[\"pinn_net\"] = input_normalization_pinn\n",
    "input_normalizations[\"pdnn_net\"] = input_normalization_pdnn\n",
    "input_normalizations[\"prnn_net\"] = input_normalization_prnn\n",
    "\n",
    "output_normalizations = dict()\n",
    "output_normalizations[\"pinn_net\"] = output_normalization_pinn\n",
    "output_normalizations[\"pdnn_net\"] = output_normalization_pdnn\n",
    "output_normalizations[\"prnn_net\"] = output_normalization_prnn\n",
    "\n",
    "_ = ErrorAnalysis.error_analysis_by_network(\n",
    "    nets, test_mu, input_normalizations, output_normalizations, euclidean=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Perform a speedup analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method.initialize_testing_set(100)\n",
    "reduction_method.speedup_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
