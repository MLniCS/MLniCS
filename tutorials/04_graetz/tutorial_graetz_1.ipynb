{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUTORIAL 04 - Graetz problem 1\n",
    "**_Keywords: successive constraints method_**\n",
    "\n",
    "### 1. Introduction\n",
    "This Tutorial addresses geometrical parametrization and the successive constraints method (SCM). In particular, we will solve the Graetz problem, which deals with forced heat convection in a channel $\\Omega_o(\\mu_0)$ divided into two parts $\\Omega_o^1$ and $\\Omega_o^2(\\mu_0)$.\n",
    "\n",
    "Boundaries $\\Gamma_{o, 1} \\cup \\Gamma_{o, 5} \\cup \\Gamma_{o, 6}$ are kept at low temperature (say, zero), while boundaries $\\Gamma_{o, 2}(\\mu_0) \\cup \\Gamma_{o, 4}(\\mu_0)$ are kept at high temperature (say, one). The convection is characterized by the velocity $\\boldsymbol{\\beta} = (x_1(1-x_1), 0)$, being $\\boldsymbol{x}_o = (x_{o, 0}, x_1)$ the coordinate vector on the parametrized domain $\\Omega_o(\\mu_0)$.\n",
    "\n",
    "The problem is characterized by two parameters. The first parameter $\\mu_0$ controls the shape of deformable subdomain $\\Omega_2(\\mu_0)$. The heat transfer between the domains can be taken into account by means of the PÃ©clet number, which will be labeled as the parameter $\\mu_1$. The ranges of the two parameters are the following:\n",
    "$$\\mu_0 \\in [0.1,10.0] \\quad \\text{and} \\quad \\mu_1 \\in [0.01,10.0].$$\n",
    "\n",
    "The parameter vector $\\boldsymbol{\\mu}$ is thus given by \n",
    "$$\n",
    "\\boldsymbol{\\mu} = (\\mu_0, \\mu_1)\n",
    "$$\n",
    "on the parameter domain\n",
    "$$\n",
    "\\mathbb{P}=[0.1,10.0]\\times[0.01,10.0].\n",
    "$$\n",
    "\n",
    "In order to obtain a faster (yet, provably accurate) approximation of the problem, and avoiding _any_ remeshing, we pursue a model reduction by means of a certified reduced basis reduced order method from a fixed reference domain.\n",
    "The successive constraints method will be used to evaluate the stability factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parametrized formulation\n",
    "\n",
    "Let $u_o(\\boldsymbol{\\mu})$ be the temperature in the domain $\\Omega_o(\\mu_0)$.\n",
    "\n",
    "We will directly provide a weak formulation for this problem: for a given parameter $\\boldsymbol{\\mu}\\in\\mathbb{P}$, find $u_o(\\boldsymbol{\\mu})\\in\\mathbb{V}_o(\\boldsymbol{\\mu})$ such that\n",
    "\n",
    "$$a_o\\left(u_o(\\boldsymbol{\\mu}),v_o;\\boldsymbol{\\mu}\\right)=f_o(v_o;\\boldsymbol{\\mu})\\quad \\forall v_o\\in\\mathbb{V}_o(\\boldsymbol{\\mu})$$\n",
    "\n",
    "where\n",
    "\n",
    "* the function space $\\mathbb{V}_o(\\boldsymbol{\\mu})$ is defined as\n",
    "$$\n",
    "\\mathbb{V}_o(\\mu_0) = \\left\\{ v \\in H^1(\\Omega_o(\\mu_0)): v|_{\\Gamma_{o,1} \\cup \\Gamma_{o,5} \\cup \\Gamma_{o,6}} = 0, v|_{\\Gamma_{o,2}(\\mu_0) \\cup \\Gamma_{o,2}(\\mu_0)} = 1 \\right\\}\n",
    "$$\n",
    "Note that, as in the previous tutorial, the function space is parameter dependent due to the shape variation. \n",
    "* the parametrized bilinear form $a_o(\\cdot, \\cdot; \\boldsymbol{\\mu}): \\mathbb{V}_o(\\boldsymbol{\\mu}) \\times \\mathbb{V}_o(\\boldsymbol{\\mu}) \\to \\mathbb{R}$ is defined by\n",
    "$$a_o(u_o,v_o;\\boldsymbol{\\mu}) = \\mu_1\\int_{\\Omega_o(\\mu_0)} \\nabla u_o \\cdot \\nabla v_o \\ d\\boldsymbol{x} + \\int_{\\Omega_o(\\mu_0)} x_1(1-x_1) \\partial_{x} u_o\\ v_o \\ d\\boldsymbol{x},$$\n",
    "* the parametrized linear form $f_o(\\cdot; \\boldsymbol{\\mu}): \\mathbb{V}_o(\\boldsymbol{\\mu}) \\to \\mathbb{R}$ is defined by\n",
    "$$f_o(v_o;\\boldsymbol{\\mu}) = 0.$$\n",
    "\n",
    "The successive constraints method will be used to compute the stability factor of the bilinear form $a_o(\\cdot, \\cdot; \\boldsymbol{\\mu})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from mlnics import NN, Losses, Normalization, RONNData, IO, Training, ErrorAnalysis\n",
    "from dolfin import *\n",
    "from rbnics import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Affine decomposition\n",
    "\n",
    "In order to obtain an affine decomposition, we proceed as in the previous tutorial and recast the problem on a fixed, parameter _independent_, reference domain $\\Omega$. As reference domain which choose the one characterized by $\\mu_0 = 1$ which we generate through the generate_mesh notebook provided in the _data_ folder.\n",
    "As in the previous tutorial, we pull back the problem to the reference domain $\\Omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@SCM()\n",
    "@PullBackFormsToReferenceDomain()\n",
    "@ShapeParametrization(\n",
    "    (\"x[0]\", \"x[1]\"),  # subdomain 1\n",
    "    (\"mu[0]*(x[0] - 1) + 1\", \"x[1]\"),  # subdomain 2\n",
    ")\n",
    "class Graetz(EllipticCoerciveProblem):\n",
    "\n",
    "    # Default initialization of members\n",
    "    @generate_function_space_for_stability_factor\n",
    "    def __init__(self, V, **kwargs):\n",
    "        # Call the standard initialization\n",
    "        EllipticCoerciveProblem.__init__(self, V, **kwargs)\n",
    "        # ... and also store FEniCS data structures for assembly\n",
    "        assert \"subdomains\" in kwargs\n",
    "        assert \"boundaries\" in kwargs\n",
    "        self.subdomains, self.boundaries = kwargs[\"subdomains\"], kwargs[\"boundaries\"]\n",
    "        self.u = TrialFunction(V)\n",
    "        self.v = TestFunction(V)\n",
    "        self.dx = Measure(\"dx\")(subdomain_data=subdomains)\n",
    "        self.ds = Measure(\"ds\")(subdomain_data=boundaries)\n",
    "        # Store the velocity expression\n",
    "        self.vel = Expression(\"x[1]*(1-x[1])\", element=self.V.ufl_element())\n",
    "        # Customize eigen solver parameters\n",
    "        self._eigen_solver_parameters.update({\n",
    "            \"bounding_box_minimum\": {\n",
    "                \"problem_type\": \"gen_hermitian\", \"spectral_transform\": \"shift-and-invert\",\n",
    "                \"spectral_shift\": 1.e-5, \"linear_solver\": \"mumps\"\n",
    "            },\n",
    "            \"bounding_box_maximum\": {\n",
    "                \"problem_type\": \"gen_hermitian\", \"spectral_transform\": \"shift-and-invert\",\n",
    "                \"spectral_shift\": 1.e5, \"linear_solver\": \"mumps\"\n",
    "            },\n",
    "            \"stability_factor\": {\n",
    "                \"problem_type\": \"gen_hermitian\", \"spectral_transform\": \"shift-and-invert\",\n",
    "                \"spectral_shift\": 1.e-5, \"linear_solver\": \"mumps\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Return custom problem name\n",
    "    def name(self):\n",
    "        return \"Graetz1\"\n",
    "\n",
    "    # Return theta multiplicative terms of the affine expansion of the problem.\n",
    "    @compute_theta_for_stability_factor\n",
    "    def compute_theta(self, term):\n",
    "        mu = self.mu\n",
    "        if term == \"a\":\n",
    "            theta_a0 = mu[1]\n",
    "            theta_a1 = 1.0\n",
    "            return (theta_a0, theta_a1)\n",
    "        elif term == \"f\":\n",
    "            theta_f0 = 1.0\n",
    "            return (theta_f0,)\n",
    "        elif term == \"dirichlet_bc\":\n",
    "            theta_bc0 = 1.0\n",
    "            return (theta_bc0,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for compute_theta().\")\n",
    "\n",
    "    # Return forms resulting from the discretization of the affine expansion of the problem operators.\n",
    "    @assemble_operator_for_stability_factor\n",
    "    def assemble_operator(self, term):\n",
    "        v = self.v\n",
    "        dx = self.dx\n",
    "        if term == \"a\":\n",
    "            u = self.u\n",
    "            vel = self.vel\n",
    "            a0 = inner(grad(u), grad(v)) * dx\n",
    "            a1 = vel * u.dx(0) * v * dx\n",
    "            return (a0, a1)\n",
    "        elif term == \"f\":\n",
    "            f0 = Constant(0.0) * v * dx\n",
    "            return (f0,)\n",
    "        elif term == \"dirichlet_bc\":\n",
    "            bc0 = [DirichletBC(self.V, Constant(0.0), self.boundaries, 1),\n",
    "                   DirichletBC(self.V, Constant(1.0), self.boundaries, 2),\n",
    "                   DirichletBC(self.V, Constant(1.0), self.boundaries, 4),\n",
    "                   DirichletBC(self.V, Constant(0.0), self.boundaries, 5),\n",
    "                   DirichletBC(self.V, Constant(0.0), self.boundaries, 6)]\n",
    "            return (bc0,)\n",
    "        elif term == \"inner_product\":\n",
    "            u = self.u\n",
    "            x0 = inner(grad(u), grad(v)) * dx\n",
    "            return (x0,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for assemble_operator().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main program\n",
    "### 4.1. Read the mesh for this problem\n",
    "The mesh was generated by the [data/generate_mesh_1.ipynb](data/generate_mesh_1.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = Mesh(\"data/graetz_1.xml\")\n",
    "subdomains = MeshFunction(\"size_t\", mesh, \"data/graetz_physical_region_1.xml\")\n",
    "boundaries = MeshFunction(\"size_t\", mesh, \"data/graetz_facet_region_1.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create Finite Element space (Lagrange P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = FunctionSpace(mesh, \"Lagrange\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Allocate an object of the Graetz class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = Graetz(V, subdomains=subdomains, boundaries=boundaries)\n",
    "mu_range = [(0.1, 10.0), (0.01, 10.0)]\n",
    "problem.set_mu_range(mu_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Prepare reduction with a reduced basis method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method = ReducedBasis(problem)\n",
    "reduction_method.set_Nmax(30, SCM=20)\n",
    "reduction_method.set_tolerance(1e-5, SCM=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Perform the offline phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Fit Reduction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lifting_mu = (1.0, 1.0)\n",
    "problem.set_mu(lifting_mu)\n",
    "reduction_method.initialize_training_set(200, SCM=250)\n",
    "reduced_problem = reduction_method.offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 Train PINN\n",
    "\n",
    "Given a training set $X_{PINN} = (\\boldsymbol{\\mu}^{(1)}, \\dots, \\boldsymbol{\\mu}^{(n)})$ of parameters for the PDE, we train a Physics-Informed Neural Network (PINN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PINN}(X_{PINN}; W) = \\frac1n \\sum_{i=1}^n \\|A(\\boldsymbol{\\mu^{(i)}}) \\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\boldsymbol{f}(\\boldsymbol{\\mu}^{(i)})\\|_2^2 + \\left|\\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)})_1 - 1\\right|^2$$\n",
    "\n",
    "over $W$, where for a given $\\boldsymbol{\\mu}$, $A(\\boldsymbol{\\mu})$ is the assembled matrix corresponding to the pull back of $a_o$ and $\\boldsymbol{f}(\\boldsymbol{\\mu})$ is the assembled vector corresponding to the pull back of $f_o$. \n",
    "\n",
    "The second term in the sum enforces boundary conditions by training the first basis function coefficient to be one, as our first basis function contains the boundary condition while all other basis functions are zero on the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pinn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pinn = Normalization.StandardNormalization()\n",
    "\n",
    "pinn_net  = NN.RONN(\"PINN\", problem, reduction_method, n_hidden=2, n_neurons=20)\n",
    "pinn_loss = Losses.PINN_Loss(pinn_net, output_normalization_pinn)\n",
    "data      = RONNData.RONNDataLoader(pinn_net, validation_proportion=0.2, \n",
    "                                    num_without_snapshots=10)\n",
    "optimizer = torch.optim.Adam(pinn_net.parameters(), lr=0.001)\n",
    "\n",
    "pinn_trainer = Training.PINNTrainer(\n",
    "    pinn_net, data, pinn_loss, optimizer,\n",
    "    input_normalization_pinn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pinn_net, data, pinn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pinn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pinn_trainer, pinn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### 4.5.3 Train PDNN\n",
    "\n",
    "Given a training set $X_{PDNN} = ((\\boldsymbol{\\mu}^{(1)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(1)})), \\dots, (\\boldsymbol{\\mu}^{(n)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(n)})))$ of parameter and high fidelity solution pairs for the PDE, we train a Projection-Driven Neural Network (PDNN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "$$L_{PDNN}(X_{PDNN}; W) = \\frac1n \\sum_{i=1}^n \\|\\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu}^{(i)})\\|_2^2,$$\n",
    "where for a given $\\boldsymbol{\\mu}$, $\\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu})$ is the projection of $\\operatorname{HF}(\\boldsymbol{\\mu})$ onto the reduced order solution space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pdnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pdnn = Normalization.StandardNormalization()\n",
    "\n",
    "pdnn_net  = NN.RONN(\"PDNN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pdnn_loss = Losses.PDNN_Loss(pdnn_net, output_normalization_pdnn)\n",
    "data      = RONNData.RONNDataLoader(pdnn_net, validation_proportion=0.2)\n",
    "optimizer = torch.optim.Adam(pdnn_net.parameters(), lr=0.001)\n",
    "\n",
    "pdnn_trainer = Training.PDNNTrainer(\n",
    "    pdnn_net, data, pdnn_loss, optimizer,\n",
    "    input_normalization_pdnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pdnn_net, data, pdnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pdnn_trainer, pdnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Train PRNN\n",
    "\n",
    "We train a Physics-Reinforced Neural Network (PRNN) $N_W(\\boldsymbol{\\mu})$ dependnent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PRNN}(X_{PINN}, X_{PDNN}; W) = L_{PINN}(X_{PINN}; W) + \\omega L_{PDNN}(X_{PDNN}; W)$$\n",
    "\n",
    "where $\\omega$ is a scaling parameter which can be chosen freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_prnn = Normalization.IdentityNormalization(input_normalization=True)\n",
    "output_normalization_prnn = Normalization.IdentityNormalization()\n",
    "\n",
    "omega = 100.\n",
    "prnn_net  = NN.RONN(f\"PRNN_{omega}\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "prnn_loss = Losses.PRNN_Loss(prnn_net, output_normalization_prnn, omega=omega)\n",
    "data      = RONNData.RONNDataLoader(prnn_net, validation_proportion=0.2,\n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(prnn_net.parameters(), lr=0.001)\n",
    "\n",
    "prnn_trainer = Training.PRNNTrainer(\n",
    "    prnn_net, data, prnn_loss, optimizer,\n",
    "    input_normalization_prnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    prnn_net, data, prnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(prnn_trainer, prnn_net, separate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Perform an error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 Reduction Method Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "=                          SCM error analysis begins                           =\n",
      "================================================================================\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 1 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 2 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 3 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 4 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 5 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 6 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 7 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 8 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 9 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 10 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 11 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 12 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 13 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 14 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 15 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 16 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 17 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 18 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 19 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 20 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 21 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 22 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 23 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 24 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 25 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 26 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 27 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 28 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 29 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 30 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 31 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 32 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 33 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 34 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 35 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 36 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 37 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 38 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 39 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 40 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 41 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 42 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 43 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 44 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 45 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 46 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 47 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 48 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 49 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 50 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 51 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 52 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 54 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 55 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 56 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 57 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 58 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 59 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 60 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 61 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 62 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 63 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 64 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 65 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 66 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 67 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 68 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 69 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 70 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 71 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 72 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 73 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 74 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 75 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 76 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 77 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 78 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 79 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 80 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 81 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 82 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 83 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 84 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 85 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 86 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 87 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 88 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 89 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 90 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 91 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 92 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 93 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 94 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 95 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 96 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 97 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 98 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SCM 99 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "N \tmin(normalized_error) \tgmean(normalized_error)\tmax(normalized_error)\n",
      "1 \t0.003022070682074032  \t0.13925944078915956    \t0.9256335134488062   \n",
      "2 \t0.003022070682073669  \t0.15739055418412204    \t0.9094592913121833   \n",
      "3 \t0.0003934361198156536 \t0.029647202318832562   \t0.20000622675123497  \n",
      "4 \t0.00039343611981179085\t0.02914177939748637    \t0.1991104909285588   \n",
      "5 \t0.00039343611981579156\t0.02446899573917232    \t0.19173443154000386  \n",
      "6 \t1.5503969793298845e-05\t0.002660131832441949   \t0.03149148993861569  \n",
      "7 \t1.107402074043885e-05 \t0.0005104203845785066  \t0.024746945691205918 \n",
      "8 \t8.437572038872452e-08 \t6.705419875337893e-05  \t0.014298591847858868 \n",
      "9 \t8.437571950365566e-08 \t6.310334205110583e-05  \t0.014298591847858868 \n",
      "10\t8.437571879560059e-08 \t6.257895054429984e-05  \t0.014298591847858715 \n",
      "11\t8.437572091976583e-08 \t6.226984053946452e-05  \t0.014271667688134308 \n",
      "12\t8.43757210967796e-08  \t5.748975133538068e-05  \t0.014271667688134614 \n",
      "13\t8.437572003469697e-08 \t5.7381764431159206e-05 \t0.01367932312268175  \n",
      "14\t8.437571844157306e-08 \t5.2768451337004215e-05 \t0.013679323122681901 \n",
      "15\t8.437571861858682e-08 \t3.201856045907626e-05  \t0.013679323122682054 \n",
      "16\t8.437571879560059e-08 \t2.011082357523138e-05  \t0.013679323122682054 \n",
      "17\t8.437571914962813e-08 \t1.9586157248682458e-05 \t0.012051140886247709 \n",
      "18\t8.437571861858682e-08 \t1.9586157248420264e-05 \t0.012051140886247251 \n",
      "19\t8.437571950365566e-08 \t1.9586157251271678e-05 \t0.012051140886247709 \n",
      "20\t8.385336564615032e-09 \t1.5823240060485444e-05 \t0.0120511408862471   \n",
      "\n",
      "================================================================================\n",
      "=                           SCM error analysis ends                            =\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "=                       Graetz1 RB error analysis begins                       =\n",
      "================================================================================\n",
      "\n",
      "###################################### 0 #######################################\n",
      "###################################### 1 #######################################\n",
      "###################################### 2 #######################################\n",
      "###################################### 3 #######################################\n",
      "###################################### 4 #######################################\n",
      "###################################### 5 #######################################\n",
      "###################################### 6 #######################################\n",
      "###################################### 7 #######################################\n",
      "###################################### 8 #######################################\n",
      "###################################### 9 #######################################\n",
      "###################################### 10 ######################################\n",
      "###################################### 11 ######################################\n",
      "###################################### 12 ######################################\n",
      "###################################### 13 ######################################\n",
      "###################################### 14 ######################################\n",
      "###################################### 15 ######################################\n",
      "###################################### 16 ######################################\n",
      "###################################### 17 ######################################\n",
      "###################################### 18 ######################################\n",
      "###################################### 19 ######################################\n",
      "###################################### 20 ######################################\n",
      "###################################### 21 ######################################\n",
      "###################################### 22 ######################################\n",
      "###################################### 23 ######################################\n",
      "###################################### 24 ######################################\n",
      "###################################### 25 ######################################\n",
      "###################################### 26 ######################################\n",
      "###################################### 27 ######################################\n",
      "###################################### 28 ######################################\n",
      "###################################### 29 ######################################\n",
      "###################################### 30 ######################################\n",
      "###################################### 31 ######################################\n",
      "###################################### 32 ######################################\n",
      "###################################### 33 ######################################\n",
      "###################################### 34 ######################################\n",
      "###################################### 35 ######################################\n",
      "###################################### 36 ######################################\n",
      "###################################### 37 ######################################\n",
      "###################################### 38 ######################################\n",
      "###################################### 39 ######################################\n",
      "###################################### 40 ######################################\n",
      "###################################### 41 ######################################\n",
      "###################################### 42 ######################################\n",
      "###################################### 43 ######################################\n",
      "###################################### 44 ######################################\n",
      "###################################### 45 ######################################\n",
      "###################################### 46 ######################################\n",
      "###################################### 47 ######################################\n",
      "###################################### 48 ######################################\n",
      "###################################### 49 ######################################\n",
      "###################################### 50 ######################################\n",
      "###################################### 51 ######################################\n",
      "###################################### 52 ######################################\n",
      "###################################### 53 ######################################\n",
      "###################################### 54 ######################################\n",
      "###################################### 55 ######################################\n",
      "###################################### 56 ######################################\n",
      "###################################### 57 ######################################\n",
      "###################################### 58 ######################################\n",
      "###################################### 59 ######################################\n",
      "###################################### 60 ######################################\n",
      "###################################### 61 ######################################\n",
      "###################################### 62 ######################################\n",
      "###################################### 63 ######################################\n",
      "###################################### 64 ######################################\n",
      "###################################### 65 ######################################\n",
      "###################################### 66 ######################################\n",
      "###################################### 67 ######################################\n",
      "###################################### 68 ######################################\n",
      "###################################### 69 ######################################\n",
      "###################################### 70 ######################################\n",
      "###################################### 71 ######################################\n",
      "###################################### 72 ######################################\n",
      "###################################### 73 ######################################\n",
      "###################################### 74 ######################################\n",
      "###################################### 75 ######################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################### 76 ######################################\n",
      "###################################### 77 ######################################\n",
      "###################################### 78 ######################################\n",
      "###################################### 79 ######################################\n",
      "###################################### 80 ######################################\n",
      "###################################### 81 ######################################\n",
      "###################################### 82 ######################################\n",
      "###################################### 83 ######################################\n",
      "###################################### 84 ######################################\n",
      "###################################### 85 ######################################\n",
      "###################################### 86 ######################################\n",
      "###################################### 87 ######################################\n",
      "###################################### 88 ######################################\n",
      "###################################### 89 ######################################\n",
      "###################################### 90 ######################################\n",
      "###################################### 91 ######################################\n",
      "###################################### 92 ######################################\n",
      "###################################### 93 ######################################\n",
      "###################################### 94 ######################################\n",
      "###################################### 95 ######################################\n",
      "###################################### 96 ######################################\n",
      "###################################### 97 ######################################\n",
      "###################################### 98 ######################################\n",
      "###################################### 99 ######################################\n",
      "\n",
      "N \tgmean(error_u)        \tmax(error_u)          \tgmean(error_estimator_u)\tmax(error_estimator_u)\tmin(effectivity_u)\tgmean(effectivity_u)\tmax(effectivity_u)\n",
      "1 \t0.21969640870111432   \t1.4384618695224072    \t0.7584136175241245      \t7.856529717311552     \t1.1231136226182286\t3.452098384347773   \t7.450838200327362 \n",
      "2 \t0.1236346248874072    \t0.6582054572717189    \t0.45934042608414943     \t4.887378444718097     \t1.101694806996043 \t3.7153056961386506  \t14433923.788764488\n",
      "3 \t0.08035315162049832   \t0.33755230632299793   \t0.35324937505921966     \t1.0417003722538807    \t1.09297138212902  \t4.39621057712321    \t36172270.596028425\n",
      "4 \t0.011934058623443553  \t0.13738094457993957   \t0.06435998682046518     \t0.3282928786289891    \t1.0930224362153966\t5.39296720849309    \t29199428.877561923\n",
      "5 \t0.009342452764861927  \t0.10704935373527967   \t0.04758011772579365     \t0.21063360106066775   \t1.081378006453272 \t5.092893581945427   \t22486608.0649249  \n",
      "6 \t0.008348466070191882  \t0.10398140772105027   \t0.041259785138688054    \t0.20701410217398616   \t1.0822752437207082\t4.942199536032819   \t17969776.218952414\n",
      "7 \t0.005256803448244549  \t0.05522193526310549   \t0.03162751286608097     \t0.11454411918283336   \t1.0776575844089384\t6.016491424392648   \t19599757.715695284\n",
      "8 \t0.0008980070325853019 \t0.03446604928206235   \t0.005946222264783525    \t0.05987304576211136   \t1.0817165771582467\t6.621576501093492   \t13595202.18532713 \n",
      "9 \t0.0008118790323284065 \t0.027954607805248027  \t0.0052899044356715524   \t0.04864831834630143   \t1.065578015899929 \t6.515631301008611   \t15591904.567860665\n",
      "10\t0.0006399048880730129 \t0.02210313953028449   \t0.004263510588215188    \t0.038717710324750375  \t1.0621314211172868\t6.662725457613192   \t10271687.29235134 \n",
      "11\t0.0002432474351373886 \t0.013001535266429262  \t0.0017278744956274819   \t0.018758244288631733  \t1.058142131961619 \t7.103361622915216   \t10919791.596175687\n",
      "12\t0.00020346923008656345\t0.003587439412029335  \t0.0014596841142419082   \t0.007597002621378897  \t1.0601216161439957\t7.1739796411521475  \t10448204.122696022\n",
      "13\t0.00016407790926523764\t0.0029399431976765986 \t0.001117411935129844    \t0.006312053025656049  \t1.0589412147686927\t6.810252154807203   \t6892981.521685145 \n",
      "14\t9.445804383949088e-05 \t0.0025947677648430717 \t0.0006274849749037969   \t0.0035021203695400345 \t1.0588942139658295\t6.643002008066784   \t4928483.122560333 \n",
      "15\t4.602386662754094e-05 \t0.00195077287755201   \t0.0003860329062945628   \t0.0027997804021600834 \t1.0843856938398149\t8.38766784674191    \t5443633.589288018 \n",
      "16\t3.4159913966131784e-05\t0.0004936784361652955 \t0.0003368755426137916   \t0.0026816687918106326 \t1.0963492714062604\t9.861721049642872   \t4274544.324935188 \n",
      "17\t1.0779811974402799e-05\t0.0004936623573692421 \t0.0001071479945317558   \t0.001178170573733629  \t1.0964001826086391\t9.939690486827052   \t4108362.5981580773\n",
      "18\t8.687352540673662e-06 \t0.0003994773063952484 \t8.184458762371898e-05   \t0.000502395314640322  \t1.089556722548535 \t9.421119638063216   \t2708137.5714799184\n",
      "19\t7.952487020382867e-06 \t0.00037618683569002026\t7.345852330143418e-05   \t0.00045099311238381694\t1.0926179083622056\t9.237176132844265   \t1292417.549566696 \n",
      "20\t5.96944896839683e-06  \t0.0003282016740520203 \t5.342031236948137e-05   \t0.0003818543507358483 \t1.0889569862682102\t8.948952014213791   \t665041.5893760731 \n",
      "21\t5.042843818514874e-06 \t0.0001742311429446645 \t4.442925882869013e-05   \t0.00024741294106879496\t1.0813681719737214\t8.810357890832844   \t506254.77116770076\n",
      "\n",
      "N \tgmean(relative_error_u)\tmax(relative_error_u) \n",
      "1 \t0.09338628143393744    \t0.5902923551080426    \n",
      "2 \t0.05255333003837195    \t0.24005441454489074   \n",
      "3 \t0.034155688186712004   \t0.154223675715073     \n",
      "4 \t0.005072806441611876   \t0.06923154038085373   \n",
      "5 \t0.003971193376991411   \t0.05394628548034089   \n",
      "6 \t0.0035486797739751287  \t0.052400229518796596  \n",
      "7 \t0.0022345077425845426  \t0.028106458619837398  \n",
      "8 \t0.0003817155590775059  \t0.01754227886654265   \n",
      "9 \t0.0003451051578475295  \t0.014228132783983164  \n",
      "10\t0.0002720041639362289  \t0.011249895057399276  \n",
      "11\t0.00010339710862878746 \t0.006799390091468124  \n",
      "12\t8.648859986537413e-05  \t0.0018755934231240484 \n",
      "13\t6.97445438563412e-05   \t0.0015463614926532728 \n",
      "14\t4.015125016310361e-05  \t0.0013648049244972891 \n",
      "15\t1.9563350111037705e-05 \t0.001027347346992044  \n",
      "16\t1.4520343588047475e-05 \t0.00026251876584182143\n",
      "17\t4.5821711915921905e-06 \t0.0002625102157707895 \n",
      "18\t3.6927301364442623e-06 \t0.0002112179823286784 \n",
      "19\t3.380361087265422e-06  \t0.00019890347496846257\n",
      "20\t2.537431743486698e-06  \t0.0001735319986401782 \n",
      "21\t2.143559991933691e-06  \t9.26492657291776e-05  \n",
      "\n",
      "================================================================================\n",
      "=                        Graetz1 RB error analysis ends                        =\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reduction_method.initialize_testing_set(100, SCM=100)\n",
    "reduction_method.error_analysis(filename=\"error_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 PINN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mu = torch.tensor(reduction_method.testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pinn_net, test_mu, input_normalization_pinn, output_normalization_pinn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pinn_net, (10.0, 0.01), input_normalization_pinn, output_normalization_pinn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3 PDNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pdnn_net, test_mu, input_normalization_pdnn, output_normalization_pdnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pdnn_net, (10.0, 0.01), input_normalization_pdnn, output_normalization_pdnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.4 PRNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    prnn_net, test_mu, input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    prnn_net, (10.0, 0.01), input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.5 Neural Network Error Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = dict()\n",
    "nets[\"pinn_net\"] = pinn_net\n",
    "nets[\"pdnn_net\"] = pdnn_net\n",
    "nets[\"prnn_net\"] = prnn_net\n",
    "\n",
    "input_normalizations = dict()\n",
    "input_normalizations[\"pinn_net\"] = input_normalization_pinn\n",
    "input_normalizations[\"pdnn_net\"] = input_normalization_pdnn\n",
    "input_normalizations[\"prnn_net\"] = input_normalization_prnn\n",
    "\n",
    "output_normalizations = dict()\n",
    "output_normalizations[\"pinn_net\"] = output_normalization_pinn\n",
    "output_normalizations[\"pdnn_net\"] = output_normalization_pdnn\n",
    "output_normalizations[\"prnn_net\"] = output_normalization_prnn\n",
    "\n",
    "_ = ErrorAnalysis.error_analysis_by_network(\n",
    "    nets, test_mu, input_normalizations, output_normalizations, euclidean=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Perform a speedup analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method.speedup_analysis(filename=\"speedup_analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
