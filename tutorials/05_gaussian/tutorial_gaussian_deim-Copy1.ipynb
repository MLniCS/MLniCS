{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUTORIAL 05 - Discrete Empirical Interpolation Method for non-affine elliptic problems\n",
    "**_Keywords: discrete empirical interpolation method_**\n",
    "\n",
    "### 1. Introduction\n",
    "In this Tutorial, we consider steady heat conduction in a two-dimensional square domain $\\Omega = (-1, 1)^2$.\n",
    "The boundary $\\partial\\Omega$ is kept at a reference temperature (say, zero). The conductivity coefficient is fixed to 1, while the heat source is characterized by the following expression\n",
    "$$\n",
    "g(\\boldsymbol{x}; \\boldsymbol{\\mu}) = \\exp\\{ -2 (x_0-\\mu_0)^2 - 2 (x_1 - \\mu_1)^2\\} \\quad \\forall \\boldsymbol{x} = (x_0, x_1) \\in \\Omega.\n",
    "$$\n",
    "\n",
    "The parameter vector $\\boldsymbol{\\mu}$, given by \n",
    "$$\n",
    "\\boldsymbol{\\mu} = (\\mu_0,\\mu_1)\n",
    "$$\n",
    "affects the center of the Gaussian source $g(\\boldsymbol{x}; \\boldsymbol{\\mu})$, which could be located at any point $\\Omega$. Thus, the parameter domain is\n",
    "$$\n",
    "\\mathbb{P}=[-1,1]^2.\n",
    "$$\n",
    "\n",
    "In order to obtain a faster evaluation (yet, provably accurate) of the problem we propose to use a certified reduced basis approximation for the problem. In order to preserve the affinity assumption (for the sake of performance) the discrete empirical interpolation method will be used on the forcing term $g(\\boldsymbol{x}; \\boldsymbol{\\mu})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parametrized formulation\n",
    "\n",
    "Let $u(\\boldsymbol{\\mu})$ be the temperature in the domain $\\Omega$.\n",
    "\n",
    "We will directly provide a weak formulation for this problem: for a given parameter $\\boldsymbol{\\mu}\\in\\mathbb{P}$, find $u(\\boldsymbol{\\mu})\\in\\mathbb{V}$ such that\n",
    "\n",
    "$$a\\left(u(\\boldsymbol{\\mu}),v;\\boldsymbol{\\mu}\\right)=f(v;\\boldsymbol{\\mu})\\quad \\forall v\\in\\mathbb{V}$$\n",
    "\n",
    "where\n",
    "\n",
    "* the function space $\\mathbb{V}$ is defined as\n",
    "$$\n",
    "\\mathbb{V} = \\left\\{ v \\in H^1(\\Omega(\\mu_0)): v|_{\\partial\\Omega} = 0\\right\\}\n",
    "$$\n",
    "Note that, as in the previous tutorial, the function space is parameter dependent due to the shape variation. \n",
    "* the parametrized bilinear form $a(\\cdot, \\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$a(u,v;\\boldsymbol{\\mu}) = \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\ d\\boldsymbol{x}$$\n",
    "* the parametrized linear form $f(\\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$f(v;\\boldsymbol{\\mu}) = \\int_\\Omega g(\\boldsymbol{\\mu}) v  \\ d\\boldsymbol{x}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from mlnics import NN, Losses, Normalization, RONNData, IO, Training, ErrorAnalysis\n",
    "from dolfin import *\n",
    "from rbnics import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Affine decomposition\n",
    "\n",
    "The parametrized bilinear form $a(\\cdot, \\cdot; \\boldsymbol{\\mu})$ is trivially affine.\n",
    "The discrete empirical interpolation method will be used on the forcing term $g(\\boldsymbol{x}; \\boldsymbol{\\mu})$ to obtain an efficient (approximately affine) expansion of $f(\\cdot; \\boldsymbol{\\mu})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DEIM(\"online\", basis_generation=\"Greedy\")\n",
    "@ExactParametrizedFunctions(\"offline\")\n",
    "class Gaussian(EllipticCoerciveProblem):\n",
    "\n",
    "    # Default initialization of members\n",
    "    def __init__(self, V, **kwargs):\n",
    "        # Call the standard initialization\n",
    "        EllipticCoerciveProblem.__init__(self, V, **kwargs)\n",
    "        # ... and also store FEniCS data structures for assembly\n",
    "        assert \"subdomains\" in kwargs\n",
    "        assert \"boundaries\" in kwargs\n",
    "        self.subdomains, self.boundaries = kwargs[\"subdomains\"], kwargs[\"boundaries\"]\n",
    "        self.u = TrialFunction(V)\n",
    "        self.v = TestFunction(V)\n",
    "        self.dx = Measure(\"dx\")(subdomain_data=subdomains)\n",
    "        self.f = ParametrizedExpression(\n",
    "            self, \"exp(- 2 * pow(x[0] - mu[0], 2) - 2 * pow(x[1] - mu[1], 2))\", mu=(0., 0.),\n",
    "            element=V.ufl_element())\n",
    "        # note that we cannot use self.mu in the initialization of self.f, because self.mu has not been initialized yet\n",
    "\n",
    "    # Return custom problem name\n",
    "    def name(self):\n",
    "        return \"GaussianDEIM\"\n",
    "\n",
    "    # Return the alpha_lower bound.\n",
    "    def get_stability_factor_lower_bound(self):\n",
    "        return 1.\n",
    "\n",
    "    # Return theta multiplicative terms of the affine expansion of the problem.\n",
    "    def compute_theta(self, term):\n",
    "        if term == \"a\":\n",
    "            return (1.,)\n",
    "        elif term == \"f\":\n",
    "            return (1.,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for compute_theta().\")\n",
    "\n",
    "    # Return forms resulting from the discretization of the affine expansion of the problem operators.\n",
    "    def assemble_operator(self, term):\n",
    "        v = self.v\n",
    "        dx = self.dx\n",
    "        if term == \"a\":\n",
    "            u = self.u\n",
    "            a0 = inner(grad(u), grad(v)) * dx\n",
    "            return (a0,)\n",
    "        elif term == \"f\":\n",
    "            f = self.f\n",
    "            f0 = f * v * dx\n",
    "            return (f0,)\n",
    "        elif term == \"dirichlet_bc\":\n",
    "            bc0 = [DirichletBC(self.V, Constant(0.0), self.boundaries, 1),\n",
    "                   DirichletBC(self.V, Constant(0.0), self.boundaries, 2),\n",
    "                   DirichletBC(self.V, Constant(0.0), self.boundaries, 3)]\n",
    "            return (bc0,)\n",
    "        elif term == \"inner_product\":\n",
    "            u = self.u\n",
    "            x0 = inner(grad(u), grad(v)) * dx\n",
    "            return (x0,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for assemble_operator().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main program\n",
    "### 4.1. Read the mesh for this problem\n",
    "The mesh was generated by the [data/generate_mesh.ipynb](data/generate_mesh.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = Mesh(\"data/gaussian.xml\")\n",
    "subdomains = MeshFunction(\"size_t\", mesh, \"data/gaussian_physical_region.xml\")\n",
    "boundaries = MeshFunction(\"size_t\", mesh, \"data/gaussian_facet_region.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create Finite Element space (Lagrange P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = FunctionSpace(mesh, \"Lagrange\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Allocate an object of the Gaussian class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = Gaussian(V, subdomains=subdomains, boundaries=boundaries)\n",
    "mu_range = [(-1.0, 1.0), (-1.0, 1.0)]\n",
    "problem.set_mu_range(mu_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Prepare reduction with a reduced basis method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method = ReducedBasis(problem)\n",
    "reduction_method.set_Nmax(20, DEIM=21)\n",
    "reduction_method.set_tolerance(1e-4, DEIM=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Perform the offline phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Fit Reduction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method.initialize_training_set(100, DEIM=60)\n",
    "reduced_problem = reduction_method.offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_size, 60)\n",
    "        self.fc2 = nn.Linear(60, 60)\n",
    "        self.fc3 = nn.Linear(60, out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Get nonlinear_terms, parameters, N0\n",
    "nonlinear_terms = np.zeros((len(reduction_method.training_set), len(reduction_method.training_set), reduced_problem.N))\n",
    "params = np.array(reduction_method.training_set)\n",
    "N0 = lambda sol, param_idx: 0 # N0 identically 0\n",
    "\n",
    "solutions = []\n",
    "Basis_Matrix = np.array([v.vector()[:] for v in reduced_problem.basis_functions])\n",
    "\n",
    "for mu in reduction_method.training_set:\n",
    "    problem.set_mu(mu)\n",
    "    solution = problem.solve()\n",
    "    solutions.append(np.array(problem._solution.vector()[:]))\n",
    "\n",
    "for i, mu in enumerate(reduction_method.training_set):\n",
    "    problem.set_mu(mu)\n",
    "    operator_form = problem.assemble_operator('f')[0]\n",
    "    theta = problem.compute_theta('f')\n",
    "    \n",
    "    for j, solution in enumerate(solutions):\n",
    "        problem._solution.vector()[:] = solution\n",
    "        nonlinear_terms[i, j] = (theta * Basis_Matrix @ np.array(assemble(operator_form)[:]).reshape(-1, 1)).reshape(-1)\n",
    "        \n",
    "solutions_ = []\n",
    "for sol in solutions:\n",
    "    F = Function(V)\n",
    "    F.vector()[:] = sol\n",
    "    solutions_.append(np.array(reduced_problem.project(F).vector()[:]))\n",
    "solutions = solutions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_parameter_indices = []\n",
    "fixed_mu_networks = []\n",
    "\n",
    "# Step 2\n",
    "# 2a. Set mu_1\n",
    "errors = np.zeros(params.shape[0])\n",
    "for i, mu_i in enumerate(params):\n",
    "    # compute error\n",
    "    s = 0\n",
    "    for j, mu_j in enumerate(params):\n",
    "        s += np.sum((nonlinear_terms[i, j] - N0(solutions[j], i))**2)\n",
    "    errors[i] = s / params.shape[0]\n",
    "\n",
    "mu_1_idx = np.argmax(errors)\n",
    "mu_1 = params[mu_1_idx]\n",
    "chosen_parameter_indices.append(mu_1_idx)\n",
    "print(\"max error:\", np.max(errors))\n",
    "print(\"mu_1 index:\", mu_1_idx)\n",
    "\n",
    "# 2b. Train Network_{mu_1}(u) to approximate Nonlinearity(u; mu_1)\n",
    "print(\"\\nTraining network to approximate nonlinearity...\")\n",
    "Network_mu_1 = Net(reduced_problem.N, reduced_problem.N)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(Network_mu_1.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9999)\n",
    "\n",
    "x_normalization = Normalization.MinMaxNormalization(input_normalization=True)\n",
    "y_normalization = Normalization.MinMaxNormalization()\n",
    "\n",
    "x_data = x_normalization(torch.tensor(np.array(solutions)).float())\n",
    "y_data = y_normalization(torch.tensor(nonlinear_terms[mu_1_idx] / np.linalg.norm(nonlinear_terms[mu_1_idx])).float().T).T\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(30000):\n",
    "    optimizer.zero_grad()\n",
    "    output = Network_mu_1(x_data)\n",
    "    loss = criterion(output, y_data)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "Network_mu_1.eval()\n",
    "fixed_mu_networks.append(Network_mu_1)\n",
    "\n",
    "# 2c. Find theta_1_1(mu)\n",
    "print(\"\\nFinding theta...\")\n",
    "thetas = np.zeros((params.shape[0], 1))\n",
    "for i, mu_i in enumerate(params):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for j, mu_j in enumerate(params):\n",
    "        net_u_mu = y_normalization(Network_mu_1(x_data[j].view(1, -1)).T, normalize=False).T.detach().numpy().reshape(-1)\n",
    "        numerator += np.dot(nonlinear_terms[i, j], net_u_mu)\n",
    "        denominator += np.dot(net_u_mu, net_u_mu)\n",
    "        \n",
    "    theta_1_1_i = numerator / denominator\n",
    "    thetas[i] = theta_1_1_i\n",
    "    print(thetas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = []\n",
    "maximum_error = np.max(errors)\n",
    "y_normalization_list = [y_normalization]\n",
    "mean_errors = 1\n",
    "\n",
    "mean_errors_list = []\n",
    "mean_errors_list_all = []\n",
    "\n",
    "#while np.mean(mean_errors) > 0.0005:\n",
    "for iteration in range(7):\n",
    "    # Step 3\n",
    "    # 3a. Set mu_2\n",
    "    N1 = lambda sol, param_idx: sum([\n",
    "        thetas[param_idx][i] * y_normalization_list[i](\n",
    "            net(x_normalization(torch.tensor(sol).float().view(1, -1))).T, normalize=False\n",
    "        ).T.detach().numpy().reshape(-1)\\\n",
    "        for i, net in enumerate(fixed_mu_networks)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "\n",
    "    errors = np.zeros(params.shape[0])\n",
    "    for i, mu_i in enumerate(params):\n",
    "        # compute error\n",
    "        s = 0\n",
    "        for j, mu_j in enumerate(params):\n",
    "            s += np.sum((nonlinear_terms[i, j] - N1(solutions[j], i))**2)\n",
    "        errors[i] = s / params.shape[0]\n",
    "        \n",
    "    mean_errors = np.zeros(params.shape[0])\n",
    "    for i, mu_i in enumerate(params):\n",
    "        mean_errors[i] = np.linalg.norm(nonlinear_terms[i, i] - N1(solutions[i], i)) / np.linalg.norm(nonlinear_terms[i, i])\n",
    "    \n",
    "    mean_errors_all = np.zeros(params.shape[0])\n",
    "    for i, mu_i in enumerate(params):\n",
    "        for j, mu_j in enumerate(params):\n",
    "            mean_errors_all[i] += np.linalg.norm(nonlinear_terms[i, j] - N1(solutions[j], i)) / np.linalg.norm(nonlinear_terms[i, j])\n",
    "        mean_errors_all[i] /= params.shape[0]\n",
    "        \n",
    "    print(\"max error:\", np.max(errors))\n",
    "    errors[np.array(chosen_parameter_indices)] = -1 # don't choose already chosen parameters again\n",
    "    mu_2_idx = np.argmax(errors)\n",
    "    mu_2 = params[mu_1_idx]\n",
    "    chosen_parameter_indices.append(mu_2_idx)\n",
    "    \n",
    "    maximum_error = np.max(errors)\n",
    "    print(\"mean error:\", np.mean(mean_errors))\n",
    "    mean_errors_list.append(np.mean(mean_errors))\n",
    "    print(\"mean error all:\", np.mean(mean_errors_all))\n",
    "    mean_errors_list_all.append(np.mean(mean_errors_all))\n",
    "    print(\"mu_2 index:\", mu_2_idx)\n",
    "\n",
    "    # 3b. Train Network_{mu_2}(u) to approximate Nonlinearity(u; mu_2)\n",
    "    print(\"\\nTraining network to approximate nonlinearity...\")\n",
    "    Network_mu_2 = Net(reduced_problem.N, reduced_problem.N)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(Network_mu_2.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9999)\n",
    "    \n",
    "    \n",
    "    # need to do Gram-Schmidt on this matrix\n",
    "    y_data = nonlinear_terms[mu_2_idx]\n",
    "    \n",
    "    for net in fixed_mu_networks:\n",
    "        # form matrix of evaluations for this network\n",
    "        previous_net_matrix = np.zeros((params.shape[0], nonlinear_terms.shape[2]))\n",
    "        for i, mu_i in enumerate(params):\n",
    "            previous_net_matrix[i] = y_normalization(net(x_data[i].view(1, -1)).T, normalize=False).T.detach().numpy().reshape(-1)\n",
    "        \n",
    "        # subtract out projection of y_data onto previous_net_matrix from y_data\n",
    "        y_data -= np.sum(y_data * previous_net_matrix) / np.linalg.norm(previous_net_matrix) * previous_net_matrix\n",
    "    \n",
    "    y_data = torch.tensor(y_data / np.linalg.norm(y_data)).float()\n",
    "    y_normalization = Normalization.MinMaxNormalization()\n",
    "    y_data = y_normalization(y_data.T).T\n",
    "    \n",
    "    \n",
    "\n",
    "    for epoch in range(30000):\n",
    "        optimizer.zero_grad()\n",
    "        output = Network_mu_2(x_data)\n",
    "        loss = criterion(output, y_data)\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    Network_mu_2.eval()\n",
    "    fixed_mu_networks.append(Network_mu_2)\n",
    "    y_normalization_list.append(y_normalization)\n",
    "\n",
    "    # 3c. Find theta_1_2(mu), theta_2_2(mu)\n",
    "    print(\"\\nFinding theta...\")\n",
    "    num_nets = len(fixed_mu_networks)\n",
    "    thetas = np.zeros((params.shape[0], num_nets))\n",
    "    for i, mu_i in enumerate(params):\n",
    "        LHS_numerator = np.zeros((num_nets, num_nets))\n",
    "        LHS_denominator = np.zeros((num_nets, num_nets))\n",
    "        RHS_numerator = np.zeros((num_nets, 1))\n",
    "        RHS_denominator = np.zeros((num_nets, 1))\n",
    "\n",
    "        for j, mu_j in enumerate(params):\n",
    "            nets_u_mu = [y_normalization_list[i_net](net(x_data[j].view(1, -1)).T, normalize=False).T.detach().numpy().reshape(-1) for i_net, net in enumerate(fixed_mu_networks)]\n",
    "\n",
    "            for k1 in range(num_nets):\n",
    "                RHS_numerator[k1] += np.dot(nonlinear_terms[i, j], nets_u_mu[k1])\n",
    "                RHS_denominator[k1] += np.dot(nets_u_mu[k1], nets_u_mu[k1])\n",
    "                for k2 in range(num_nets):\n",
    "                    \n",
    "                    LHS_numerator[k1, k2] += np.dot(nets_u_mu[k1], nets_u_mu[k2])\n",
    "                    LHS_denominator[k1, k2] += np.dot(nets_u_mu[k1], nets_u_mu[k1])\n",
    "\n",
    "\n",
    "\n",
    "        LHS = LHS_numerator / LHS_denominator\n",
    "        RHS = RHS_numerator / RHS_denominator\n",
    "        matrices.append(LHS)\n",
    "        thetas[i] = np.linalg.solve(LHS, RHS).reshape(-1)\n",
    "        #print(thetas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NonlinearityApprox(u, mu):\n",
    "    #thetas_ = theta_normalization(theta_net(torch.tensor(mu).float().view(1, -1)).detach().T, normalize=False).T.numpy().reshape(-1, 1)\n",
    "    thetas_ = interpolate.griddata(params, thetas, mu, method='cubic').reshape(-1, 1)\n",
    "    if True in np.isnan(thetas_):\n",
    "        thetas_ = interpolate.griddata(params, thetas, mu, method='nearest').reshape(-1, 1)\n",
    "    net_evals = np.array([y_normalization_list[i](net(x_normalization(torch.tensor(u).float().view(1, -1))).T, normalize=False).T.detach().numpy().reshape(-1) for i, net in enumerate(fixed_mu_networks)])\n",
    "    return np.sum(thetas_ * net_evals, axis=0)\n",
    "\n",
    "def NonlinearityApprox2(u, mu):\n",
    "    #thetas_ = theta_normalization(theta_net(torch.tensor(mu).float().view(1, -1)).T, normalize=False).T.view(-1)\n",
    "    thetas_ = interpolate.griddata(params, thetas, mu, method='cubic').reshape(-1)\n",
    "    if True in np.isnan(thetas_):\n",
    "        thetas_ = interpolate.griddata(params, thetas, mu, method='nearest').reshape(-1)\n",
    "    s = 0\n",
    "    for i, net in enumerate(fixed_mu_networks):\n",
    "        s += thetas_[i] * y_normalization_list[i](net(x_normalization(u.view(1, -1))).T, normalize=False).T.view(-1)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rbnics.backends.basic.wrapping.delayed_transpose import DelayedTranspose\n",
    "from rbnics.backends.online import OnlineFunction, OnlineVector\n",
    "from rbnics.backends.common.time_series import TimeSeries\n",
    "from rbnics.backends.dolfin.parametrized_tensor_factory import ParametrizedTensorFactory\n",
    "from rbnics.backends.dolfin.evaluate import evaluate\n",
    "\n",
    "class Approx_PINN_Loss(Losses.RONN_Loss_Base):\n",
    "    \"\"\"\n",
    "    PINN_Loss\n",
    "\n",
    "    ronn: object of type RONN\n",
    "\n",
    "    RETURNS: loss function loss_fn(parameters, reduced order coefficients)\n",
    "    \"\"\"\n",
    "    def __init__(self, ronn, normalization=None, beta=1., mu=None):\n",
    "        super(Approx_PINN_Loss, self).__init__(ronn, mu)\n",
    "        self.operators = None\n",
    "        self.proj_snapshots = None\n",
    "        self.T0_idx = None\n",
    "        self.normalization = normalization\n",
    "        if self.normalization is None:\n",
    "            self.normalization = IdentityNormalization()\n",
    "\n",
    "        self.beta = beta\n",
    "\n",
    "        # if time dependent, we need the neural net to compute time derivative\n",
    "        self.time_dependent = False\n",
    "\n",
    "    def name(self):\n",
    "        return \"Approx_PINN\"\n",
    "\n",
    "    def _compute_operators(self):\n",
    "        self.operators_initialized = True\n",
    "\n",
    "        #self.operators = self.ronn.get_operator_matrices(self.mu)\n",
    "        self.operators = self.ronn.get_reduced_operator_matrices(self.mu)\n",
    "\n",
    "        if not self.normalization.initialized:\n",
    "            self.normalization(self.ronn.get_projected_snapshots())\n",
    "\n",
    "    def set_mu(self, mu):\n",
    "        self.mu = mu\n",
    "        self.operators_initialized = False\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        pred = kwargs[\"prediction_no_snap\"]\n",
    "        if not self.operators_initialized:\n",
    "            self._compute_operators()\n",
    "\n",
    "        pred = self.normalization(pred.T, normalize=False).T\n",
    "\n",
    "        ##### 1st equation in system #####\n",
    "        res1 = 0.0\n",
    "\n",
    "        # these two could be combined when both not None\n",
    "        if 'f' in self.operators:\n",
    "            res1 -= self.operators['f']\n",
    "        if 'c' in self.operators:\n",
    "            self.operators['c'] = torch.zeros_like(self.operators['c'])\n",
    "            for i, mu in enumerate(kwargs[\"input_normalization\"](kwargs[\"normalized_mu\"], normalize=False)):                \n",
    "                mu = np.array(mu)\n",
    "                sol = pred[i].float()\n",
    "                C = NonlinearityApprox2(sol, mu).view(-1, 1).double()#.detach()\n",
    "                self.operators['c'][i] = C[None, :, :]\n",
    "\n",
    "            res1 += self.operators['c']\n",
    "        \n",
    "        if 'a' in self.operators:\n",
    "            res1 += torch.matmul(self.operators['a'], pred[:, :, None].double())\n",
    "\n",
    "        loss1 = torch.mean(torch.sum(res1**2, dim=1)) if type(res1) is not float else res1\n",
    "        if self.ronn.problem.dirichlet_bc_are_homogeneous:\n",
    "            boundary_condition_loss = 0\n",
    "        else:\n",
    "            boundary_condition_loss = torch.mean((pred[:, 0] - 1.)**2)\n",
    "\n",
    "        self.value = loss1 + self.beta*boundary_condition_loss\n",
    "\n",
    "        return self.value\n",
    "\n",
    "    def reinitialize(self, mu):\n",
    "        normalization = self.normalization\n",
    "        beta = self.beta\n",
    "        return Approx_PINN_Loss(self.ronn, normalization, beta, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 Train PINN\n",
    "\n",
    "Given a training set $X_{PINN} = (\\boldsymbol{\\mu}^{(1)}, \\dots, \\boldsymbol{\\mu}^{(n)})$ of parameters for the PDE, we train a Physics-Informed Neural Network (PINN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PINN}(X_{PINN}; W) = \\frac1n \\sum_{i=1}^n \\|A(\\boldsymbol{\\mu^{(i)}}) \\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\boldsymbol{f}(\\boldsymbol{\\mu}^{(i)})\\|_2^2$$\n",
    "\n",
    "over $W$, where for a given $\\boldsymbol{\\mu}$, $A(\\boldsymbol{\\mu})$ is the assembled matrix corresponding to the pull back of $a_o$ and $\\boldsymbol{f}(\\boldsymbol{\\mu})$ is the assembled vector corresponding to the pull back of $f_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build matrices for computing DEIM\n",
    "selected_indices = sorted([idx[0] for idx in problem.DEIM_approximations['f'][0].interpolation_locations.get_dofs_list()])\n",
    "U = np.array(problem._assemble_operator_DEIM('f')).T\n",
    "P = []\n",
    "for idx in selected_indices:\n",
    "    new_column = np.zeros(U.shape[0])\n",
    "    new_column[idx] = 1\n",
    "    P.append(new_column)\n",
    "P = np.array(P).T\n",
    "PtUinv = np.linalg.inv(P.T @ U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this is f, not c! Code in MLniCS probably needs to change...\n",
    "selected_indices = sorted([idx[0] for idx in problem.DEIM_approximations['f'][0].interpolation_locations.get_dofs_list()])\n",
    "dofs_locs = torch.tensor(V.tabulate_dof_coordinates()[selected_indices])\n",
    "dx_test = torch.tensor(np.array(assemble(problem.v*dx))[selected_indices].reshape(-1, 1))\n",
    "\n",
    "def DEIM_nonlinearity(mu):\n",
    "    \"\"\"\n",
    "    mu: torch.tensor with shape (number of samples, number of parameters)\n",
    "    \"\"\"\n",
    "    res = torch.exp(-2 * ((dofs_locs[:, 0].reshape(-1, 1) - mu[:, 0].view(1, -1))**2 + (dofs_locs[:, 1].reshape(-1, 1) - mu[:, 1].view(1, -1))**2))\n",
    "    return torch.matmul(torch.tensor(PtUinv[:, :]), dx_test * res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pinn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pinn = Normalization.StandardNormalization()\n",
    "\n",
    "pinn_net  = NN.RONN(\"PINN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pinn_loss = Losses.PINN_Loss(pinn_net, output_normalization_pinn, DEIM_func_f=DEIM_nonlinearity)\n",
    "data      = RONNData.RONNDataLoader(pinn_net, validation_proportion=0.2, \n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(pinn_net.parameters(), lr=0.001)\n",
    "scheduler = None\n",
    "\n",
    "pinn_trainer = Training.PINNTrainer(\n",
    "    pinn_net, data, pinn_loss, optimizer, scheduler,\n",
    "    input_normalization_pinn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pinn_net, data, pinn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pinn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pinn_trainer, pinn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 Train PDNN\n",
    "\n",
    "Given a training set $X_{PDNN} = ((\\boldsymbol{\\mu}^{(1)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(1)})), \\dots, (\\boldsymbol{\\mu}^{(n)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(n)})))$ of parameter and high fidelity solution pairs for the PDE, we train a Projection-Driven Neural Network (PDNN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "$$L_{PDNN}(X_{PDNN}; W) = \\frac1n \\sum_{i=1}^n \\|\\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu}^{(i)})\\|_2^2,$$\n",
    "where for a given $\\boldsymbol{\\mu}$, $\\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu})$ is the projection of $\\operatorname{HF}(\\boldsymbol{\\mu})$ onto the reduced order solution space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pdnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pdnn = Normalization.StandardNormalization()\n",
    "\n",
    "pdnn_net  = NN.RONN(\"PDNN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pdnn_loss = Losses.PDNN_Loss(pdnn_net, output_normalization_pdnn)\n",
    "data      = RONNData.RONNDataLoader(pdnn_net, validation_proportion=0.2)\n",
    "optimizer = torch.optim.Adam(pdnn_net.parameters(), lr=0.001)\n",
    "\n",
    "pdnn_trainer = Training.PDNNTrainer(\n",
    "    pdnn_net, data, pdnn_loss, optimizer,\n",
    "    input_normalization_pdnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pdnn_net, data, pdnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pdnn_trainer, pdnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Train PRNN\n",
    "\n",
    "We train a Physics-Reinforced Neural Network (PRNN) $N_W(\\boldsymbol{\\mu})$ dependnent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PRNN}(X_{PINN}, X_{PDNN}; W) = L_{PINN}(X_{PINN}; W) + \\omega L_{PDNN}(X_{PDNN}; W)$$\n",
    "\n",
    "where $\\omega$ is a scaling parameter which can be chosen freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_prnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_prnn = Normalization.StandardNormalization()\n",
    "\n",
    "omega = 1.\n",
    "prnn_net  = NN.RONN(f\"PRNN_{omega}\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "prnn_loss = Losses.PRNN_Loss(prnn_net, output_normalization_prnn, omega=omega)\n",
    "data      = RONNData.RONNDataLoader(prnn_net, validation_proportion=0.2,\n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(prnn_net.parameters(), lr=0.001)\n",
    "\n",
    "prnn_trainer = Training.PRNNTrainer(\n",
    "    prnn_net, data, prnn_loss, optimizer,\n",
    "    input_normalization_prnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    prnn_net, data, prnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(prnn_trainer, prnn_net, separate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Perform an error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 Reduction Method Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduction_method.initialize_testing_set(50, DEIM=60)\n",
    "#reduction_method.error_analysis(filename=\"error_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 PINN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mu = torch.tensor(reduction_method.testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pinn_net, test_mu, input_normalization_pinn, output_normalization_pinn, relative=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pinn_net, (0.3, -1.0), input_normalization_pinn, output_normalization_pinn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3 PDNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pdnn_net, test_mu, input_normalization_pdnn, output_normalization_pdnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pdnn_net, (0.3, -1.0), input_normalization_pdnn, output_normalization_pdnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.4 PRNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    prnn_net, test_mu, input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    prnn_net, (0.3, -1.0), input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.5 Neural Network Error Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = dict()\n",
    "nets[\"pinn_net\"] = pinn_net\n",
    "nets[\"pdnn_net\"] = pdnn_net\n",
    "nets[\"prnn_net\"] = prnn_net\n",
    "\n",
    "input_normalizations = dict()\n",
    "input_normalizations[\"pinn_net\"] = input_normalization_pinn\n",
    "input_normalizations[\"pdnn_net\"] = input_normalization_pdnn\n",
    "input_normalizations[\"prnn_net\"] = input_normalization_prnn\n",
    "\n",
    "output_normalizations = dict()\n",
    "output_normalizations[\"pinn_net\"] = output_normalization_pinn\n",
    "output_normalizations[\"pdnn_net\"] = output_normalization_pdnn\n",
    "output_normalizations[\"prnn_net\"] = output_normalization_prnn\n",
    "\n",
    "_ = ErrorAnalysis.error_analysis_by_network(\n",
    "    nets, test_mu, input_normalizations, output_normalizations, euclidean=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
