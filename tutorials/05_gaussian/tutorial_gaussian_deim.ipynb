{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUTORIAL 05 - Discrete Empirical Interpolation Method for non-affine elliptic problems\n",
    "**_Keywords: discrete empirical interpolation method_**\n",
    "\n",
    "### 1. Introduction\n",
    "In this Tutorial, we consider steady heat conduction in a two-dimensional square domain $\\Omega = (-1, 1)^2$.\n",
    "The boundary $\\partial\\Omega$ is kept at a reference temperature (say, zero). The conductivity coefficient is fixed to 1, while the heat source is characterized by the following expression\n",
    "$$\n",
    "g(\\boldsymbol{x}; \\boldsymbol{\\mu}) = \\exp\\{ -2 (x_0-\\mu_0)^2 - 2 (x_1 - \\mu_1)^2\\} \\quad \\forall \\boldsymbol{x} = (x_0, x_1) \\in \\Omega.\n",
    "$$\n",
    "\n",
    "The parameter vector $\\boldsymbol{\\mu}$, given by \n",
    "$$\n",
    "\\boldsymbol{\\mu} = (\\mu_0,\\mu_1)\n",
    "$$\n",
    "affects the center of the Gaussian source $g(\\boldsymbol{x}; \\boldsymbol{\\mu})$, which could be located at any point $\\Omega$. Thus, the parameter domain is\n",
    "$$\n",
    "\\mathbb{P}=[-1,1]^2.\n",
    "$$\n",
    "\n",
    "In order to obtain a faster evaluation (yet, provably accurate) of the problem we propose to use a certified reduced basis approximation for the problem. In order to preserve the affinity assumption (for the sake of performance) the discrete empirical interpolation method will be used on the forcing term $g(\\boldsymbol{x}; \\boldsymbol{\\mu})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parametrized formulation\n",
    "\n",
    "Let $u(\\boldsymbol{\\mu})$ be the temperature in the domain $\\Omega$.\n",
    "\n",
    "We will directly provide a weak formulation for this problem: for a given parameter $\\boldsymbol{\\mu}\\in\\mathbb{P}$, find $u(\\boldsymbol{\\mu})\\in\\mathbb{V}$ such that\n",
    "\n",
    "$$a\\left(u(\\boldsymbol{\\mu}),v;\\boldsymbol{\\mu}\\right)=f(v;\\boldsymbol{\\mu})\\quad \\forall v\\in\\mathbb{V}$$\n",
    "\n",
    "where\n",
    "\n",
    "* the function space $\\mathbb{V}$ is defined as\n",
    "$$\n",
    "\\mathbb{V} = \\left\\{ v \\in H^1(\\Omega(\\mu_0)): v|_{\\partial\\Omega} = 0\\right\\}\n",
    "$$\n",
    "Note that, as in the previous tutorial, the function space is parameter dependent due to the shape variation. \n",
    "* the parametrized bilinear form $a(\\cdot, \\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$a(u,v;\\boldsymbol{\\mu}) = \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\ d\\boldsymbol{x}$$\n",
    "* the parametrized linear form $f(\\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$f(v;\\boldsymbol{\\mu}) = \\int_\\Omega g(\\boldsymbol{\\mu}) v  \\ d\\boldsymbol{x}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from mlnics import NN, Losses, Normalization, RONNData, IO, Training, ErrorAnalysis\n",
    "from dolfin import *\n",
    "from rbnics import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Affine decomposition\n",
    "\n",
    "The parametrized bilinear form $a(\\cdot, \\cdot; \\boldsymbol{\\mu})$ is trivially affine.\n",
    "The discrete empirical interpolation method will be used on the forcing term $g(\\boldsymbol{x}; \\boldsymbol{\\mu})$ to obtain an efficient (approximately affine) expansion of $f(\\cdot; \\boldsymbol{\\mu})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DEIM()\n",
    "class Gaussian(EllipticCoerciveProblem):\n",
    "\n",
    "    # Default initialization of members\n",
    "    def __init__(self, V, **kwargs):\n",
    "        # Call the standard initialization\n",
    "        EllipticCoerciveProblem.__init__(self, V, **kwargs)\n",
    "        # ... and also store FEniCS data structures for assembly\n",
    "        assert \"subdomains\" in kwargs\n",
    "        assert \"boundaries\" in kwargs\n",
    "        self.subdomains, self.boundaries = kwargs[\"subdomains\"], kwargs[\"boundaries\"]\n",
    "        self.u = TrialFunction(V)\n",
    "        self.v = TestFunction(V)\n",
    "        self.dx = Measure(\"dx\")(subdomain_data=subdomains)\n",
    "        self.f = ParametrizedExpression(\n",
    "            self, \"exp(- 2 * pow(x[0] - mu[0], 2) - 2 * pow(x[1] - mu[1], 2))\", mu=(0., 0.),\n",
    "            element=V.ufl_element())\n",
    "        # note that we cannot use self.mu in the initialization of self.f, because self.mu has not been initialized yet\n",
    "\n",
    "    # Return custom problem name\n",
    "    def name(self):\n",
    "        return \"GaussianDEIM\"\n",
    "\n",
    "    # Return the alpha_lower bound.\n",
    "    def get_stability_factor_lower_bound(self):\n",
    "        return 1.\n",
    "\n",
    "    # Return theta multiplicative terms of the affine expansion of the problem.\n",
    "    def compute_theta(self, term):\n",
    "        if term == \"a\":\n",
    "            return (1.,)\n",
    "        elif term == \"f\":\n",
    "            return (1.,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for compute_theta().\")\n",
    "\n",
    "    # Return forms resulting from the discretization of the affine expansion of the problem operators.\n",
    "    def assemble_operator(self, term):\n",
    "        v = self.v\n",
    "        dx = self.dx\n",
    "        if term == \"a\":\n",
    "            u = self.u\n",
    "            a0 = inner(grad(u), grad(v)) * dx\n",
    "            return (a0,)\n",
    "        elif term == \"f\":\n",
    "            f = self.f\n",
    "            f0 = f * v * dx\n",
    "            return (f0,)\n",
    "        elif term == \"dirichlet_bc\":\n",
    "            bc0 = [DirichletBC(self.V, Constant(0.0), self.boundaries, 1),\n",
    "                   DirichletBC(self.V, Constant(0.0), self.boundaries, 2),\n",
    "                   DirichletBC(self.V, Constant(0.0), self.boundaries, 3)]\n",
    "            return (bc0,)\n",
    "        elif term == \"inner_product\":\n",
    "            u = self.u\n",
    "            x0 = inner(grad(u), grad(v)) * dx\n",
    "            return (x0,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for assemble_operator().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main program\n",
    "### 4.1. Read the mesh for this problem\n",
    "The mesh was generated by the [data/generate_mesh.ipynb](data/generate_mesh.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = Mesh(\"data/gaussian.xml\")\n",
    "subdomains = MeshFunction(\"size_t\", mesh, \"data/gaussian_physical_region.xml\")\n",
    "boundaries = MeshFunction(\"size_t\", mesh, \"data/gaussian_facet_region.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create Finite Element space (Lagrange P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = FunctionSpace(mesh, \"Lagrange\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Allocate an object of the Gaussian class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = Gaussian(V, subdomains=subdomains, boundaries=boundaries)\n",
    "mu_range = [(-1.0, 1.0), (-1.0, 1.0)]\n",
    "problem.set_mu_range(mu_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Prepare reduction with a reduced basis method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method = ReducedBasis(problem)\n",
    "reduction_method.set_Nmax(20, DEIM=21)\n",
    "reduction_method.set_tolerance(1e-4, DEIM=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Perform the offline phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Fit Reduction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method.initialize_training_set(100, DEIM=60)\n",
    "reduced_problem = reduction_method.offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 Train PINN\n",
    "\n",
    "Given a training set $X_{PINN} = (\\boldsymbol{\\mu}^{(1)}, \\dots, \\boldsymbol{\\mu}^{(n)})$ of parameters for the PDE, we train a Physics-Informed Neural Network (PINN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PINN}(X_{PINN}; W) = \\frac1n \\sum_{i=1}^n \\|A(\\boldsymbol{\\mu^{(i)}}) \\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\boldsymbol{f}(\\boldsymbol{\\mu}^{(i)})\\|_2^2$$\n",
    "\n",
    "over $W$, where for a given $\\boldsymbol{\\mu}$, $A(\\boldsymbol{\\mu})$ is the assembled matrix corresponding to the pull back of $a_o$ and $\\boldsymbol{f}(\\boldsymbol{\\mu})$ is the assembled vector corresponding to the pull back of $f_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pinn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pinn = Normalization.StandardNormalization()\n",
    "\n",
    "pinn_net  = NN.RONN(\"PINN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pinn_loss = Losses.PINN_Loss(pinn_net, output_normalization_pinn)\n",
    "data      = RONNData.RONNDataLoader(pinn_net, validation_proportion=0.2, \n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(pinn_net.parameters(), lr=0.001)\n",
    "\n",
    "pinn_trainer = Training.PINNTrainer(\n",
    "    pinn_net, data, pinn_loss, optimizer,\n",
    "    input_normalization_pinn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pinn_net, data, pinn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pinn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pinn_trainer, pinn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 Train PDNN\n",
    "\n",
    "Given a training set $X_{PDNN} = ((\\boldsymbol{\\mu}^{(1)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(1)})), \\dots, (\\boldsymbol{\\mu}^{(n)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(n)})))$ of parameter and high fidelity solution pairs for the PDE, we train a Projection-Driven Neural Network (PDNN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "$$L_{PDNN}(X_{PDNN}; W) = \\frac1n \\sum_{i=1}^n \\|\\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu}^{(i)})\\|_2^2,$$\n",
    "where for a given $\\boldsymbol{\\mu}$, $\\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu})$ is the projection of $\\operatorname{HF}(\\boldsymbol{\\mu})$ onto the reduced order solution space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pdnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pdnn = Normalization.StandardNormalization()\n",
    "\n",
    "pdnn_net  = NN.RONN(\"PDNN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pdnn_loss = Losses.PDNN_Loss(pdnn_net, output_normalization_pdnn)\n",
    "data      = RONNData.RONNDataLoader(pdnn_net, validation_proportion=0.2)\n",
    "optimizer = torch.optim.Adam(pdnn_net.parameters(), lr=0.001)\n",
    "\n",
    "pdnn_trainer = Training.PDNNTrainer(\n",
    "    pdnn_net, data, pdnn_loss, optimizer,\n",
    "    input_normalization_pdnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pdnn_net, data, pdnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pdnn_trainer, pdnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Train PRNN\n",
    "\n",
    "We train a Physics-Reinforced Neural Network (PRNN) $N_W(\\boldsymbol{\\mu})$ dependnent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PRNN}(X_{PINN}, X_{PDNN}; W) = L_{PINN}(X_{PINN}; W) + \\omega L_{PDNN}(X_{PDNN}; W)$$\n",
    "\n",
    "where $\\omega$ is a scaling parameter which can be chosen freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_prnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_prnn = Normalization.StandardNormalization()\n",
    "\n",
    "omega = 1.\n",
    "prnn_net  = NN.RONN(f\"PRNN_{omega}\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "prnn_loss = Losses.PRNN_Loss(prnn_net, output_normalization_prnn, omega=omega)\n",
    "data      = RONNData.RONNDataLoader(prnn_net, validation_proportion=0.2,\n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(prnn_net.parameters(), lr=0.001)\n",
    "\n",
    "prnn_trainer = Training.PRNNTrainer(\n",
    "    prnn_net, data, prnn_loss, optimizer,\n",
    "    input_normalization_prnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    prnn_net, data, prnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(prnn_trainer, prnn_net, separate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Perform an error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 Reduction Method Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduction_method.initialize_testing_set(50, DEIM=60)\n",
    "reduction_method.error_analysis(filename=\"error_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 PINN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mu = torch.tensor(reduction_method.testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pinn_net, test_mu, input_normalization_pinn, output_normalization_pinn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pinn_net, (0.3, -1.0), input_normalization_pinn, output_normalization_pinn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3 PDNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pdnn_net, test_mu, input_normalization_pdnn, output_normalization_pdnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pdnn_net, (0.3, -1.0), input_normalization_pdnn, output_normalization_pdnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.4 PRNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    prnn_net, test_mu, input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    prnn_net, (0.3, -1.0), input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.5 Neural Network Error Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = dict()\n",
    "nets[\"pinn_net\"] = pinn_net\n",
    "nets[\"pdnn_net\"] = pdnn_net\n",
    "nets[\"prnn_net\"] = prnn_net\n",
    "\n",
    "input_normalizations = dict()\n",
    "input_normalizations[\"pinn_net\"] = input_normalization_pinn\n",
    "input_normalizations[\"pdnn_net\"] = input_normalization_pdnn\n",
    "input_normalizations[\"prnn_net\"] = input_normalization_prnn\n",
    "\n",
    "output_normalizations = dict()\n",
    "output_normalizations[\"pinn_net\"] = output_normalization_pinn\n",
    "output_normalizations[\"pdnn_net\"] = output_normalization_pdnn\n",
    "output_normalizations[\"prnn_net\"] = output_normalization_prnn\n",
    "\n",
    "_ = ErrorAnalysis.error_analysis_by_network(\n",
    "    nets, test_mu, input_normalizations, output_normalizations, euclidean=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
