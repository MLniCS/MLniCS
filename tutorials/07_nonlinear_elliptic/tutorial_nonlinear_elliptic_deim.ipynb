{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 07 - Non linear Elliptic problem\n",
    "**_Keywords: DEIM, POD-Galerkin_**\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "In this tutorial, we consider a non linear elliptic problem in a two-dimensional spatial domain $\\Omega=(0,1)^2$. We impose a homogeneous Dirichlet condition on the boundary $\\partial\\Omega$. The source term is characterized by the following expression\n",
    "$$\n",
    "g(\\boldsymbol{x}; \\boldsymbol{\\mu}) = 100\\sin(2\\pi x_0)cos(2\\pi x_1) \\quad \\forall \\boldsymbol{x} = (x_0, x_1) \\in \\Omega.\n",
    "$$\n",
    "\n",
    "This problem is characterized by two parameters. The first parameter $\\mu_0$ controls the strength of the sink term and the second parameter $\\mu_1$ the strength of the nonlinearity. The range of the two parameters is the following:\n",
    "$$\n",
    "\\mu_0,\\mu_1\\in[0.01,10.0]\n",
    "$$\n",
    "The parameter vector $\\boldsymbol{\\mu}$ is thus given by\n",
    "$$\n",
    "\\boldsymbol{\\mu} = (\\mu_0,\\mu_1)\n",
    "$$\n",
    "on the parameter domain\n",
    "$$\n",
    "\\mathbb{P}=[0.01,10]^2.\n",
    "$$\n",
    "\n",
    "\n",
    "In order to obtain a faster approximation of the problem, we pursue a model reduction by means of a POD-Galerkin reduced order method. In order to preserve the affinity assumption the discrete empirical interpolation method will be used on the forcing term $g(\\boldsymbol{x}; \\boldsymbol{\\mu})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parametrized formulation\n",
    "\n",
    "Let $u(\\boldsymbol{\\mu})$ be the solution in the domain $\\Omega$.\n",
    "\n",
    "The strong formulation of the parametrized problem is given by: for a given parameter $\\boldsymbol{\\mu}\\in\\mathbb{P}$, find $u(\\boldsymbol{\\mu})$ such that\n",
    "\n",
    "$$ -\\nabla^2u(\\boldsymbol{\\mu})+\\frac{\\mu_0}{\\mu_1}(\\exp\\{\\mu_1u(\\boldsymbol{\\mu})\\}-1)=g(\\boldsymbol{x}; \\boldsymbol{\\mu})$$\n",
    "<br>\n",
    "    \n",
    "The corresponding weak formulation reads: for a given parameter $\\boldsymbol{\\mu}\\in\\mathbb{P}$, find $u(\\boldsymbol{\\mu})\\in\\mathbb{V}$ such that\n",
    "\n",
    "$$a\\left(u(\\boldsymbol{\\mu}),v;\\boldsymbol{\\mu}\\right)+c\\left(u(\\boldsymbol{\\mu}),v;\\boldsymbol{\\mu}\\right)=f(v;\\boldsymbol{\\mu})\\quad \\forall v\\in\\mathbb{V}$$\n",
    "\n",
    "where\n",
    "\n",
    "* the function space $\\mathbb{V}$ is defined as\n",
    "$$\n",
    "\\mathbb{V} = \\{v\\in H_1(\\Omega) : v|_{\\partial\\Omega}=0\\}\n",
    "$$\n",
    "* the parametrized bilinear form $a(\\cdot, \\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$a(u, v;\\boldsymbol{\\mu})=\\int_{\\Omega} \\nabla u\\cdot \\nabla v \\ d\\boldsymbol{x},$$\n",
    "* the parametrized bilinear form $c(\\cdot, \\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$c(u, v;\\boldsymbol{\\mu})=\\mu_0\\int_{\\Omega} \\frac{1}{\\mu_1}\\big(\\exp\\{\\mu_1u\\} - 1\\big)v \\ d\\boldsymbol{x},$$\n",
    "* the parametrized linear form $f(\\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$f(v; \\boldsymbol{\\mu})= \\int_{\\Omega}g(\\boldsymbol{x}; \\boldsymbol{\\mu})v \\ d\\boldsymbol{x}.$$\n",
    "\n",
    "The output of interest $s(\\boldsymbol{\\mu})$ is given by\n",
    "$$s(\\boldsymbol{\\mu}) = \\int_{\\Omega} v \\ d\\boldsymbol{x}$$\n",
    "is computed for each $\\boldsymbol{\\mu}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from mlnics import NN, Losses, Normalization, RONNData, IO, Training, ErrorAnalysis\n",
    "from dolfin import *\n",
    "from rbnics import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Affine Decomposition \n",
    "\n",
    "For this problem the affine decomposition is straightforward:\n",
    "$$a(u,v;\\boldsymbol{\\mu})=\\underbrace{1}_{\\Theta^{a}_0(\\boldsymbol{\\mu})}\\underbrace{\\int_{\\Omega}\\nabla u \\cdot \\nabla v \\ d\\boldsymbol{x}}_{a_0(u,v)},$$\n",
    "$$c(u,v;\\boldsymbol{\\mu})=\\underbrace{\\mu_0}_{\\Theta^{c}_0(\\boldsymbol{\\mu})}\\underbrace{\\int_{\\Omega}\\frac{1}{\\mu_1}\\big(\\exp\\{\\mu_1u\\} - 1\\big)v \\ d\\boldsymbol{x}}_{c_0(u,v)},$$\n",
    "$$f(v; \\boldsymbol{\\mu}) = \\underbrace{100}_{\\Theta^{f}_0(\\boldsymbol{\\mu})} \\underbrace{\\int_{\\Omega}\\sin(2\\pi x_0)cos(2\\pi x_1)v \\ d\\boldsymbol{x}}_{f_0(v)}.$$\n",
    "We will implement the numerical discretization of the problem in the class\n",
    "```\n",
    "class NonlinearElliptic(NonlinearEllipticProblem):\n",
    "```\n",
    "by specifying the coefficients $\\Theta^{a}_*(\\boldsymbol{\\mu})$, $\\Theta^{c}_*(\\boldsymbol{\\mu})$ and $\\Theta^{f}_*(\\boldsymbol{\\mu})$ in the method\n",
    "```\n",
    "    def compute_theta(self, term):\n",
    "```\n",
    "and the bilinear forms $a_*(u, v)$, $c_*(u, v)$ and linear forms $f_*(v)$ in\n",
    "```\n",
    "    def assemble_operator(self, term):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DEIM(\"online\", basis_generation=\"Greedy\")\n",
    "@ExactParametrizedFunctions(\"offline\")\n",
    "class NonlinearElliptic(NonlinearEllipticProblem):\n",
    "\n",
    "    # Default initialization of members\n",
    "    def __init__(self, V, **kwargs):\n",
    "        # Call the standard initialization\n",
    "        NonlinearEllipticProblem.__init__(self, V, **kwargs)\n",
    "        # ... and also store FEniCS data structures for assembly\n",
    "        assert \"subdomains\" in kwargs\n",
    "        assert \"boundaries\" in kwargs\n",
    "        self.subdomains, self.boundaries = kwargs[\"subdomains\"], kwargs[\"boundaries\"]\n",
    "        self.du = TrialFunction(V)\n",
    "        self.u = self._solution\n",
    "        self.v = TestFunction(V)\n",
    "        self.dx = Measure(\"dx\")(subdomain_data=self.subdomains)\n",
    "        self.ds = Measure(\"ds\")(subdomain_data=self.boundaries)\n",
    "        # Store the forcing term expression\n",
    "        self.f = Expression(\"sin(2*pi*x[0])*sin(2*pi*x[1])\", element=self.V.ufl_element())\n",
    "        # Customize nonlinear solver parameters\n",
    "        self._nonlinear_solver_parameters.update({\n",
    "            \"linear_solver\": \"mumps\",\n",
    "            \"maximum_iterations\": 20,\n",
    "            \"report\": True\n",
    "        })\n",
    "\n",
    "    # Return custom problem name\n",
    "    def name(self):\n",
    "        return \"NonlinearEllipticDEIM\"\n",
    "\n",
    "    # Return theta multiplicative terms of the affine expansion of the problem.\n",
    "    @compute_theta_for_derivatives\n",
    "    def compute_theta(self, term):\n",
    "        mu = self.mu\n",
    "        if term == \"a\":\n",
    "            theta_a0 = 1.#mu[0]\n",
    "            return (theta_a0,)\n",
    "        elif term == \"c\":\n",
    "            theta_c0 = mu[0]\n",
    "            return (theta_c0,)\n",
    "        elif term == \"f\":\n",
    "            theta_f0 = 100.\n",
    "            return (theta_f0,)\n",
    "        elif term == \"s\":\n",
    "            theta_s0 = 1.0\n",
    "            return (theta_s0,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for compute_theta().\")\n",
    "\n",
    "    # Return forms resulting from the discretization of the affine expansion of the problem operators.\n",
    "    @assemble_operator_for_derivatives\n",
    "    def assemble_operator(self, term):\n",
    "        v = self.v\n",
    "        dx = self.dx\n",
    "        if term == \"a\":\n",
    "            du = self.du\n",
    "            a0 = inner(grad(du), grad(v)) * dx\n",
    "            return (a0,)\n",
    "        elif term == \"c\":\n",
    "            u = self.u\n",
    "            mu = self.mu\n",
    "            c0 = (exp(mu[1] * u) - 1) / mu[1] * v * dx\n",
    "            return (c0,)\n",
    "        elif term == \"f\":\n",
    "            f = self.f\n",
    "            f0 = f * v * dx\n",
    "            return (f0,)\n",
    "        elif term == \"s\":\n",
    "            s0 = v * dx\n",
    "            return (s0,)\n",
    "        elif term == \"dirichlet_bc\":\n",
    "            bc0 = [DirichletBC(self.V, Constant(0.0), self.boundaries, 1)]\n",
    "            return (bc0,)\n",
    "        elif term == \"inner_product\":\n",
    "            du = self.du\n",
    "            x0 = inner(grad(du), grad(v)) * dx\n",
    "            return (x0,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for assemble_operator().\")\n",
    "\n",
    "\n",
    "# Customize the resulting reduced problem\n",
    "@CustomizeReducedProblemFor(NonlinearEllipticProblem)\n",
    "def CustomizeReducedNonlinearElliptic(ReducedNonlinearElliptic_Base):\n",
    "    class ReducedNonlinearElliptic(ReducedNonlinearElliptic_Base):\n",
    "        def __init__(self, truth_problem, **kwargs):\n",
    "            ReducedNonlinearElliptic_Base.__init__(self, truth_problem, **kwargs)\n",
    "            self._nonlinear_solver_parameters.update({\n",
    "                \"report\": True,\n",
    "                \"line_search\": \"wolfe\"\n",
    "            })\n",
    "\n",
    "    return ReducedNonlinearElliptic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main program\n",
    "\n",
    "### 4.1. Read the mesh for this problem\n",
    "The mesh was generated by the [data/generate_mesh.ipynb](data/generate_mesh.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = Mesh(\"data/square.xml\")\n",
    "subdomains = MeshFunction(\"size_t\", mesh, \"data/square_physical_region.xml\")\n",
    "boundaries = MeshFunction(\"size_t\", mesh, \"data/square_facet_region.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create Finite Element space (Lagrange P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = FunctionSpace(mesh, \"Lagrange\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Allocate an object of the NonlinearElliptic class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = NonlinearElliptic(V, subdomains=subdomains, boundaries=boundaries)\n",
    "mu_range = [(0.1, 10.0), (0.1, 10.0)]\n",
    "problem.set_mu_range(mu_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Prepare reduction with a POD-Galerkin method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method = PODGalerkin(problem)\n",
    "reduction_method.set_Nmax(8, DEIM=8)#21\n",
    "reduction_method.set_tolerance(0, DEIM=0)#1e-8, DEIM=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Perform the offline phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Fit Reduction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduction_method.initialize_training_set(100, DEIM=100)\n",
    "reduced_problem = reduction_method.offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 Train PINN\n",
    "\n",
    "Given a training set $X_{PINN} = (\\boldsymbol{\\mu}^{(1)}, \\dots, \\boldsymbol{\\mu}^{(n)})$ of parameters for the PDE, we train a Physics-Informed Neural Network (PINN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PINN}(X_{PINN}; W) = \\frac1n \\sum_{i=1}^n \\left\\|A(\\boldsymbol{\\mu^{(i)}}) \\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\boldsymbol{f}(\\boldsymbol{\\mu}^{(i)}) + \\boldsymbol{c}(\\boldsymbol{\\mu}^{(i)})\\right\\|_2^2$$\n",
    "\n",
    "over $W$, where for a given $\\boldsymbol{\\mu}$, $A(\\boldsymbol{\\mu})$ is the assembled matrix corresponding to the bilinear form $a$, $\\boldsymbol{f}(\\boldsymbol{\\mu})$ is the assembled vector corresponding to the linear form $f$, and $\\boldsymbol{c}(\\boldsymbol{\\mu})$ is a vector corresponding to the nonlinear form $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build matrices for computing DEIM\n",
    "selected_indices = sorted([idx[0] for idx in problem.DEIM_approximations['c'][0].interpolation_locations.get_dofs_list()])\n",
    "U = np.array(problem._assemble_operator_DEIM('c')).T\n",
    "P = []\n",
    "for idx in selected_indices:\n",
    "    new_column = np.zeros(U.shape[0])\n",
    "    new_column[idx] = 1\n",
    "    P.append(new_column)\n",
    "P = np.array(P).T\n",
    "PtUinv = np.linalg.inv(P.T @ U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = sorted([idx[0] for idx in problem.DEIM_approximations['c'][0].interpolation_locations.get_dofs_list()])\n",
    "dx_test = torch.tensor(np.array(assemble(problem.v*dx))[selected_indices].reshape(-1, 1))\n",
    "ones_tensor = torch.ones_like(dx_test)\n",
    "\n",
    "def DEIM_nonlinearity(u, mu):\n",
    "    \"\"\"\n",
    "    u: torch.tensor with shape (reduced order dimension, number of samples)\n",
    "    mu: torch.tensor with shape (number of samples, number of parameters)\n",
    "    \"\"\"\n",
    "    res = mu[:, 0].view(1, -1) / mu[:, 1].view(1, -1) * (torch.exp(mu[:, 1].view(1, -1) * torch.minimum(u, 20*ones_tensor)) - 1)\n",
    "    return torch.matmul(torch.tensor(PtUinv[:, :]), dx_test * res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pinn = Normalization.MinMaxNormalization(input_normalization=True)\n",
    "output_normalization_pinn = Normalization.MinMaxNormalization()\n",
    "\n",
    "pinn_net  = NN.RONN(\"PINN\", problem, reduction_method, n_hidden=2, n_neurons=60)\n",
    "pinn_loss = Losses.PINN_Loss(pinn_net, output_normalization_pinn, DEIM_func_c=DEIM_nonlinearity)\n",
    "data      = RONNData.RONNDataLoader(pinn_net, validation_proportion=0.2, \n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(pinn_net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.99999)\n",
    "\n",
    "pinn_trainer = Training.PINNTrainer(\n",
    "    pinn_net, data, pinn_loss, optimizer, scheduler,\n",
    "    input_normalization_pinn, num_epochs=60000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pinn_net, data, pinn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  |F(x)| = 1.24136; step 1\n",
      "1:  |F(x)| = 0.180216; step 1\n",
      "2:  |F(x)| = 0.00690687; step 1\n",
      "3:  |F(x)| = 0.000166227; step 1\n",
      "4:  |F(x)| = 6.11039e-06; step 1\n",
      "5:  |F(x)| = 2.14636e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.58173; step 0.521607\n",
      "1:  |F(x)| = 1.40884; step 0.431387\n",
      "2:  |F(x)| = 0.169933; step 1\n",
      "3:  |F(x)| = 0.00810881; step 1\n",
      "4:  |F(x)| = 1.85772e-05; step 1\n",
      "5:  |F(x)| = 7.41561e-08; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 4.79955; step 0.128162\n",
      "1:  |F(x)| = 4.07274; step 0.150932\n",
      "2:  |F(x)| = 3.21324; step 0.208541\n",
      "3:  |F(x)| = 1.95445; step 0.37659\n",
      "4:  |F(x)| = 0.386544; step 1\n",
      "5:  |F(x)| = 0.0330789; step 1\n",
      "6:  |F(x)| = 0.000294287; step 1\n",
      "7:  |F(x)| = 8.13406e-07; step 1\n",
      "scipy solver converged in 9 iterations.\n",
      "0:  |F(x)| = 3.64002; step 0.327254\n",
      "1:  |F(x)| = 2.36138; step 0.337255\n",
      "2:  |F(x)| = 0.423577; step 1\n",
      "3:  |F(x)| = 0.0325127; step 1\n",
      "4:  |F(x)| = 0.000328133; step 1\n",
      "5:  |F(x)| = 4.29405e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.397699; step 1\n",
      "1:  |F(x)| = 0.0299637; step 1\n",
      "2:  |F(x)| = 0.0036772; step 1\n",
      "3:  |F(x)| = 0.000500911; step 1\n",
      "4:  |F(x)| = 6.8684e-05; step 1\n",
      "5:  |F(x)| = 9.42631e-06; step 1\n",
      "6:  |F(x)| = 1.29385e-06; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 0.0990187; step 1\n",
      "1:  |F(x)| = 0.00666277; step 1\n",
      "2:  |F(x)| = 0.000977914; step 1\n",
      "3:  |F(x)| = 0.000147297; step 1\n",
      "4:  |F(x)| = 2.2128e-05; step 1\n",
      "5:  |F(x)| = 3.32335e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.48346; step 0.362965\n",
      "1:  |F(x)| = 0.964755; step 1\n",
      "2:  |F(x)| = 0.126965; step 1\n",
      "3:  |F(x)| = 0.0039309; step 1\n",
      "4:  |F(x)| = 8.2756e-05; step 1\n",
      "5:  |F(x)| = 2.79386e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.55325; step 0.523805\n",
      "1:  |F(x)| = 0.326915; step 1\n",
      "2:  |F(x)| = 0.0185628; step 1\n",
      "3:  |F(x)| = 0.000350757; step 1\n",
      "4:  |F(x)| = 1.41628e-05; step 1\n",
      "5:  |F(x)| = 5.10622e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.0179891; step 1\n",
      "1:  |F(x)| = 0.000363411; step 1\n",
      "2:  |F(x)| = 1.61039e-05; step 1\n",
      "3:  |F(x)| = 7.14262e-07; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 3.68514; step 0.313391\n",
      "1:  |F(x)| = 1.4875; step 0.523246\n",
      "2:  |F(x)| = 0.0314132; step 1\n",
      "3:  |F(x)| = 0.000557735; step 1\n",
      "4:  |F(x)| = 4.62661e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 0.538969; step 1\n",
      "1:  |F(x)| = 0.0331336; step 1\n",
      "2:  |F(x)| = 0.00101972; step 1\n",
      "3:  |F(x)| = 4.76988e-05; step 1\n",
      "4:  |F(x)| = 2.24379e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 0.0375408; step 1\n",
      "1:  |F(x)| = 0.000365563; step 1\n",
      "2:  |F(x)| = 6.62423e-06; step 1\n",
      "3:  |F(x)| = 1.23972e-07; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 0.110713; step 1\n",
      "1:  |F(x)| = 0.00258581; step 1\n",
      "2:  |F(x)| = 0.000105697; step 1\n",
      "3:  |F(x)| = 4.56435e-06; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 0.0822896; step 1\n",
      "1:  |F(x)| = 0.00226895; step 1\n",
      "2:  |F(x)| = 0.000118054; step 1\n",
      "3:  |F(x)| = 6.38081e-06; step 1\n",
      "4:  |F(x)| = 3.45959e-07; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.7013; step 0.324038\n",
      "1:  |F(x)| = 2.3215; step 0.347367\n",
      "2:  |F(x)| = 0.821645; step 1\n",
      "3:  |F(x)| = 0.124689; step 1\n",
      "4:  |F(x)| = 0.00405408; step 1\n",
      "5:  |F(x)| = 8.95131e-06; step 1\n",
      "6:  |F(x)| = 6.56935e-08; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 4.60351; step 0.163631\n",
      "1:  |F(x)| = 3.60196; step 0.215765\n",
      "2:  |F(x)| = 2.22565; step 0.369271\n",
      "3:  |F(x)| = 0.485445; step 1\n",
      "4:  |F(x)| = 0.0475155; step 1\n",
      "5:  |F(x)| = 0.00059875; step 1\n",
      "6:  |F(x)| = 2.76579e-06; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 2.37748; step 0.535922\n",
      "1:  |F(x)| = 0.261708; step 1\n",
      "2:  |F(x)| = 0.0128979; step 1\n",
      "3:  |F(x)| = 0.000265659; step 1\n",
      "4:  |F(x)| = 9.81583e-06; step 1\n",
      "5:  |F(x)| = 3.25154e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.204917; step 1\n",
      "1:  |F(x)| = 0.00439028; step 1\n",
      "2:  |F(x)| = 8.43597e-05; step 1\n",
      "3:  |F(x)| = 2.13951e-06; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 2.7456; step 0.480303\n",
      "1:  |F(x)| = 0.5858; step 1\n",
      "2:  |F(x)| = 0.0580481; step 1\n",
      "3:  |F(x)| = 0.000920391; step 1\n",
      "4:  |F(x)| = 1.13337e-05; step 1\n",
      "5:  |F(x)| = 2.30206e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.71453; step 0.314853\n",
      "1:  |F(x)| = 1.7937; step 0.461171\n",
      "2:  |F(x)| = 0.118326; step 1\n",
      "3:  |F(x)| = 0.00341837; step 1\n",
      "4:  |F(x)| = 1.43947e-05; step 1\n",
      "5:  |F(x)| = 3.81928e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.6108; step 0.314368\n",
      "1:  |F(x)| = 2.29222; step 0.346502\n",
      "2:  |F(x)| = 0.310475; step 1\n",
      "3:  |F(x)| = 0.0192163; step 1\n",
      "4:  |F(x)| = 9.24688e-05; step 1\n",
      "5:  |F(x)| = 4.19239e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.67514; step 0.330084\n",
      "1:  |F(x)| = 2.35679; step 0.338874\n",
      "2:  |F(x)| = 1.21841; step 0.451405\n",
      "3:  |F(x)| = 0.0957352; step 1\n",
      "4:  |F(x)| = 0.0026067; step 1\n",
      "5:  |F(x)| = 1.25597e-05; step 1\n",
      "6:  |F(x)| = 1.15959e-07; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 3.68633; step 0.314485\n",
      "1:  |F(x)| = 1.74534; step 0.464919\n",
      "2:  |F(x)| = 0.0864558; step 1\n",
      "3:  |F(x)| = 0.00202258; step 1\n",
      "4:  |F(x)| = 1.89227e-05; step 1\n",
      "5:  |F(x)| = 4.98388e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.15891; step 0.447053\n",
      "1:  |F(x)| = 0.153495; step 1\n",
      "2:  |F(x)| = 0.0067609; step 1\n",
      "3:  |F(x)| = 0.000272632; step 1\n",
      "4:  |F(x)| = 8.83119e-06; step 1\n",
      "5:  |F(x)| = 3.72156e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.1693; step 0.417327\n",
      "1:  |F(x)| = 0.656286; step 1\n",
      "2:  |F(x)| = 0.0611277; step 1\n",
      "3:  |F(x)| = 0.00165954; step 1\n",
      "4:  |F(x)| = 7.31832e-05; step 1\n",
      "5:  |F(x)| = 3.42906e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.73375; step 0.493037\n",
      "1:  |F(x)| = 0.41558; step 1\n",
      "2:  |F(x)| = 0.0285638; step 1\n",
      "3:  |F(x)| = 0.000505273; step 1\n",
      "4:  |F(x)| = 1.79594e-05; step 1\n",
      "5:  |F(x)| = 6.07077e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.713509; step 1\n",
      "1:  |F(x)| = 0.0596649; step 1\n",
      "2:  |F(x)| = 0.0015735; step 1\n",
      "3:  |F(x)| = 6.03595e-05; step 1\n",
      "4:  |F(x)| = 2.4638e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.67789; step 0.321711\n",
      "1:  |F(x)| = 2.85492; step 0.217689\n",
      "2:  |F(x)| = 1.06043; step 1\n",
      "3:  |F(x)| = 0.162081; step 1\n",
      "4:  |F(x)| = 0.0055967; step 1\n",
      "5:  |F(x)| = 1.724e-05; step 1\n",
      "6:  |F(x)| = 3.84626e-07; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 4.70749; step 0.144755\n",
      "1:  |F(x)| = 3.7244; step 0.207177\n",
      "2:  |F(x)| = 2.2428; step 0.383259\n",
      "3:  |F(x)| = 0.428302; step 1\n",
      "4:  |F(x)| = 0.0355926; step 1\n",
      "5:  |F(x)| = 0.000377401; step 1\n",
      "6:  |F(x)| = 4.03528e-06; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 0.114719; step 1\n",
      "1:  |F(x)| = 0.00157936; step 1\n",
      "2:  |F(x)| = 3.09559e-05; step 1\n",
      "3:  |F(x)| = 6.80554e-07; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 3.73052; step 0.31859\n",
      "1:  |F(x)| = 2.4566; step 0.315744\n",
      "2:  |F(x)| = 1.1712; step 0.485128\n",
      "3:  |F(x)| = 0.0629748; step 1\n",
      "4:  |F(x)| = 0.00103642; step 1\n",
      "5:  |F(x)| = 1.17554e-05; step 1\n",
      "6:  |F(x)| = 7.93674e-08; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 0.0153811; step 1\n",
      "1:  |F(x)| = 2.12411e-05; step 1\n",
      "2:  |F(x)| = 5.24836e-08; step 1\n",
      "scipy solver converged in 4 iterations.\n",
      "0:  |F(x)| = 0.21317; step 1\n",
      "1:  |F(x)| = 0.0115175; step 1\n",
      "2:  |F(x)| = 0.00111945; step 1\n",
      "3:  |F(x)| = 0.000115813; step 1\n",
      "4:  |F(x)| = 1.20361e-05; step 1\n",
      "5:  |F(x)| = 1.25156e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.70239; step 0.311834\n",
      "1:  |F(x)| = 2.5419; step 0.30225\n",
      "2:  |F(x)| = 0.521213; step 1\n",
      "3:  |F(x)| = 0.0462241; step 1\n",
      "4:  |F(x)| = 0.000554; step 1\n",
      "5:  |F(x)| = 4.93725e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.12418; step 0.413878\n",
      "1:  |F(x)| = 0.829903; step 1\n",
      "2:  |F(x)| = 0.0933516; step 1\n",
      "3:  |F(x)| = 0.00237922; step 1\n",
      "4:  |F(x)| = 5.79995e-05; step 1\n",
      "5:  |F(x)| = 1.98307e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.63167; step 0.50125\n",
      "1:  |F(x)| = 0.561754; step 1\n",
      "2:  |F(x)| = 0.0568661; step 1\n",
      "3:  |F(x)| = 0.000844211; step 1\n",
      "4:  |F(x)| = 6.61186e-06; step 1\n",
      "5:  |F(x)| = 1.03018e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  |F(x)| = 3.21327; step 0.399259\n",
      "1:  |F(x)| = 0.964359; step 1\n",
      "2:  |F(x)| = 0.121878; step 1\n",
      "3:  |F(x)| = 0.00356914; step 1\n",
      "4:  |F(x)| = 6.93291e-05; step 1\n",
      "5:  |F(x)| = 2.30336e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.70611; step 0.315112\n",
      "1:  |F(x)| = 2.05651; step 0.398652\n",
      "2:  |F(x)| = 0.245194; step 1\n",
      "3:  |F(x)| = 0.0130037; step 1\n",
      "4:  |F(x)| = 6.9512e-05; step 1\n",
      "5:  |F(x)| = 1.56443e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.0100609; step 1\n",
      "1:  |F(x)| = 0.000103006; step 1\n",
      "2:  |F(x)| = 7.96551e-06; step 1\n",
      "3:  |F(x)| = 6.44323e-07; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 0.806876; step 1\n",
      "1:  |F(x)| = 0.0843658; step 1\n",
      "2:  |F(x)| = 0.0018333; step 1\n",
      "3:  |F(x)| = 3.70748e-05; step 1\n",
      "4:  |F(x)| = 9.50366e-07; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 2.50394; step 0.515175\n",
      "1:  |F(x)| = 0.319422; step 1\n",
      "2:  |F(x)| = 0.0179707; step 1\n",
      "3:  |F(x)| = 0.000374245; step 1\n",
      "4:  |F(x)| = 1.49817e-05; step 1\n",
      "5:  |F(x)| = 5.2618e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 1.50132; step 1\n",
      "1:  |F(x)| = 0.251562; step 1\n",
      "2:  |F(x)| = 0.0122324; step 1\n",
      "3:  |F(x)| = 0.000264202; step 1\n",
      "4:  |F(x)| = 1.00591e-05; step 1\n",
      "5:  |F(x)| = 3.58125e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.21565; step 0.444767\n",
      "1:  |F(x)| = 0.159002; step 1\n",
      "2:  |F(x)| = 0.00767667; step 1\n",
      "3:  |F(x)| = 0.000308035; step 1\n",
      "4:  |F(x)| = 9.69576e-06; step 1\n",
      "5:  |F(x)| = 4.01753e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.78212; step 0.30994\n",
      "1:  |F(x)| = 1.23888; step 1\n",
      "2:  |F(x)| = 0.175682; step 1\n",
      "3:  |F(x)| = 0.00754009; step 1\n",
      "4:  |F(x)| = 0.000286084; step 1\n",
      "5:  |F(x)| = 1.56606e-05; step 1\n",
      "6:  |F(x)| = 8.12652e-07; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 3.67679; step 0.310916\n",
      "1:  |F(x)| = 2.38482; step 0.336699\n",
      "2:  |F(x)| = 0.369613; step 1\n",
      "3:  |F(x)| = 0.0255376; step 1\n",
      "4:  |F(x)| = 0.000195336; step 1\n",
      "5:  |F(x)| = 2.70436e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 1.03271; step 1\n",
      "1:  |F(x)| = 0.11564; step 1\n",
      "2:  |F(x)| = 0.00519914; step 1\n",
      "3:  |F(x)| = 0.000314733; step 1\n",
      "4:  |F(x)| = 2.12758e-05; step 1\n",
      "5:  |F(x)| = 1.42348e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.208944; step 1\n",
      "1:  |F(x)| = 0.0053024; step 1\n",
      "2:  |F(x)| = 0.000159714; step 1\n",
      "3:  |F(x)| = 5.73356e-06; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 1.85751; step 1\n",
      "1:  |F(x)| = 0.323054; step 1\n",
      "2:  |F(x)| = 0.020676; step 1\n",
      "3:  |F(x)| = 0.000980215; step 1\n",
      "4:  |F(x)| = 7.00005e-05; step 1\n",
      "5:  |F(x)| = 4.83073e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.32647; step 0.547498\n",
      "1:  |F(x)| = 0.271596; step 1\n",
      "2:  |F(x)| = 0.0142308; step 1\n",
      "3:  |F(x)| = 0.000129212; step 1\n",
      "4:  |F(x)| = 3.3214e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.66019; step 0.312466\n",
      "1:  |F(x)| = 2.55015; step 0.291008\n",
      "2:  |F(x)| = 0.521696; step 1\n",
      "3:  |F(x)| = 0.0473042; step 1\n",
      "4:  |F(x)| = 0.000506598; step 1\n",
      "5:  |F(x)| = 4.60009e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 1.1969; step 1\n",
      "1:  |F(x)| = 0.206949; step 1\n",
      "2:  |F(x)| = 0.00898867; step 1\n",
      "3:  |F(x)| = 5.31239e-05; step 1\n",
      "4:  |F(x)| = 7.63598e-07; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.6699; step 0.311753\n",
      "1:  |F(x)| = 2.50734; step 0.303989\n",
      "2:  |F(x)| = 0.47655; step 1\n",
      "3:  |F(x)| = 0.0401513; step 1\n",
      "4:  |F(x)| = 0.000389296; step 1\n",
      "5:  |F(x)| = 4.24403e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.0985818; step 1\n",
      "1:  |F(x)| = 0.00118413; step 1\n",
      "2:  |F(x)| = 2.1199e-05; step 1\n",
      "3:  |F(x)| = 4.19322e-07; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 0.107016; step 1\n",
      "1:  |F(x)| = 0.00330333; step 1\n",
      "2:  |F(x)| = 0.000188331; step 1\n",
      "3:  |F(x)| = 1.12214e-05; step 1\n",
      "4:  |F(x)| = 6.70909e-07; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 0.214587; step 1\n",
      "1:  |F(x)| = 0.00877025; step 1\n",
      "2:  |F(x)| = 0.000586091; step 1\n",
      "3:  |F(x)| = 4.2576e-05; step 1\n",
      "4:  |F(x)| = 3.10784e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 0.915109; step 1\n",
      "1:  |F(x)| = 0.0932609; step 1\n",
      "2:  |F(x)| = 0.00363819; step 1\n",
      "3:  |F(x)| = 0.000196867; step 1\n",
      "4:  |F(x)| = 1.16779e-05; step 1\n",
      "5:  |F(x)| = 6.85751e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.317889; step 1\n",
      "1:  |F(x)| = 0.0172644; step 1\n",
      "2:  |F(x)| = 0.00142063; step 1\n",
      "3:  |F(x)| = 0.000130996; step 1\n",
      "4:  |F(x)| = 1.2155e-05; step 1\n",
      "5:  |F(x)| = 1.12877e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.22477; step 0.443863\n",
      "1:  |F(x)| = 0.166071; step 1\n",
      "2:  |F(x)| = 0.00843247; step 1\n",
      "3:  |F(x)| = 0.000341418; step 1\n",
      "4:  |F(x)| = 1.07242e-05; step 1\n",
      "5:  |F(x)| = 4.38865e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.653996; step 1\n",
      "1:  |F(x)| = 0.0491979; step 1\n",
      "2:  |F(x)| = 0.0014365; step 1\n",
      "3:  |F(x)| = 6.38106e-05; step 1\n",
      "4:  |F(x)| = 2.92171e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.88983; step 0.291786\n",
      "1:  |F(x)| = 1.90733; step 0.488812\n",
      "2:  |F(x)| = 0.220336; step 1\n",
      "3:  |F(x)| = 0.0103774; step 1\n",
      "4:  |F(x)| = 6.2453e-05; step 1\n",
      "5:  |F(x)| = 1.16473e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 4.05454; step 0.263061\n",
      "1:  |F(x)| = 2.82188; step 0.300209\n",
      "2:  |F(x)| = 1.3448; step 0.498601\n",
      "3:  |F(x)| = 0.136983; step 1\n",
      "4:  |F(x)| = 0.00514275; step 1\n",
      "5:  |F(x)| = 6.09618e-06; step 1\n",
      "6:  |F(x)| = 2.45844e-08; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 1.94157; step 1\n",
      "1:  |F(x)| = 0.396512; step 1\n",
      "2:  |F(x)| = 0.0278911; step 1\n",
      "3:  |F(x)| = 0.000439392; step 1\n",
      "4:  |F(x)| = 1.38526e-05; step 1\n",
      "5:  |F(x)| = 4.08164e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.70328; step 0.312888\n",
      "1:  |F(x)| = 2.61129; step 0.284833\n",
      "2:  |F(x)| = 0.605069; step 1\n",
      "3:  |F(x)| = 0.0602265; step 1\n",
      "4:  |F(x)| = 0.000875435; step 1\n",
      "5:  |F(x)| = 5.85003e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.22829; step 0.406885\n",
      "1:  |F(x)| = 0.690313; step 1\n",
      "2:  |F(x)| = 0.065091; step 1\n",
      "3:  |F(x)| = 0.00215078; step 1\n",
      "4:  |F(x)| = 0.000119559; step 1\n",
      "5:  |F(x)| = 6.81785e-06; step 1\n",
      "6:  |F(x)| = 3.78081e-07; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 3.68945; step 0.311003\n",
      "1:  |F(x)| = 2.51341; step 0.306632\n",
      "2:  |F(x)| = 0.482896; step 1\n",
      "3:  |F(x)| = 0.040694; step 1\n",
      "4:  |F(x)| = 0.00042354; step 1\n",
      "5:  |F(x)| = 4.64441e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.44242; step 0.364822\n",
      "1:  |F(x)| = 2.19072; step 0.347668\n",
      "2:  |F(x)| = 0.398809; step 1\n",
      "3:  |F(x)| = 0.0311545; step 1\n",
      "4:  |F(x)| = 0.000267415; step 1\n",
      "5:  |F(x)| = 2.19016e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.70362; step 0.314944\n",
      "1:  |F(x)| = 2.00556; step 0.409638\n",
      "2:  |F(x)| = 0.209159; step 1\n",
      "3:  |F(x)| = 0.00966963; step 1\n",
      "4:  |F(x)| = 5.40164e-05; step 1\n",
      "5:  |F(x)| = 1.29668e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.173279; step 1\n",
      "1:  |F(x)| = 0.00292132; step 1\n",
      "2:  |F(x)| = 4.22212e-05; step 1\n",
      "3:  |F(x)| = 8.05052e-07; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 1.23859; step 1\n",
      "1:  |F(x)| = 0.209827; step 1\n",
      "2:  |F(x)| = 0.00895484; step 1\n",
      "3:  |F(x)| = 7.25625e-05; step 1\n",
      "4:  |F(x)| = 1.29663e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.15158; step 0.410537\n",
      "1:  |F(x)| = 0.934975; step 1\n",
      "2:  |F(x)| = 0.120248; step 1\n",
      "3:  |F(x)| = 0.00339492; step 1\n",
      "4:  |F(x)| = 5.11116e-05; step 1\n",
      "5:  |F(x)| = 1.50604e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.0131789; step 1\n",
      "1:  |F(x)| = 8.38924e-05; step 1\n",
      "2:  |F(x)| = 7.31545e-06; step 1\n",
      "3:  |F(x)| = 7.4189e-07; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 0.119857; step 1\n",
      "1:  |F(x)| = 0.00488134; step 1\n",
      "2:  |F(x)| = 0.000376944; step 1\n",
      "3:  |F(x)| = 3.03874e-05; step 1\n",
      "4:  |F(x)| = 2.45832e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.70132; step 0.31616\n",
      "1:  |F(x)| = 2.20023; step 0.364327\n",
      "2:  |F(x)| = 0.365174; step 1\n",
      "3:  |F(x)| = 0.0272036; step 1\n",
      "4:  |F(x)| = 0.000142519; step 1\n",
      "5:  |F(x)| = 3.51186e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.71773; step 0.31551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  |F(x)| = 2.15583; step 0.37944\n",
      "2:  |F(x)| = 0.347239; step 1\n",
      "3:  |F(x)| = 0.0250339; step 1\n",
      "4:  |F(x)| = 0.000126924; step 1\n",
      "5:  |F(x)| = 1.86236e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.595159; step 1\n",
      "1:  |F(x)| = 0.0452929; step 1\n",
      "2:  |F(x)| = 0.00360481; step 1\n",
      "3:  |F(x)| = 0.000362209; step 1\n",
      "4:  |F(x)| = 3.69789e-05; step 1\n",
      "5:  |F(x)| = 3.78208e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.315798; step 1\n",
      "1:  |F(x)| = 0.0173115; step 1\n",
      "2:  |F(x)| = 0.00145338; step 1\n",
      "3:  |F(x)| = 0.000136315; step 1\n",
      "4:  |F(x)| = 1.28644e-05; step 1\n",
      "5:  |F(x)| = 1.21502e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.64798; step 0.325623\n",
      "1:  |F(x)| = 2.36741; step 0.337124\n",
      "2:  |F(x)| = 0.422841; step 1\n",
      "3:  |F(x)| = 0.0323169; step 1\n",
      "4:  |F(x)| = 0.000327134; step 1\n",
      "5:  |F(x)| = 4.35141e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.71878; step 0.480933\n",
      "1:  |F(x)| = 0.45662; step 1\n",
      "2:  |F(x)| = 0.0335238; step 1\n",
      "3:  |F(x)| = 0.000571144; step 1\n",
      "4:  |F(x)| = 1.96872e-05; step 1\n",
      "5:  |F(x)| = 6.33586e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.173912; step 1\n",
      "1:  |F(x)| = 0.0123555; step 1\n",
      "2:  |F(x)| = 0.00176382; step 1\n",
      "3:  |F(x)| = 0.000264995; step 1\n",
      "4:  |F(x)| = 3.98789e-05; step 1\n",
      "5:  |F(x)| = 5.99866e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.67505; step 0.324541\n",
      "1:  |F(x)| = 1.62091; step 0.499535\n",
      "2:  |F(x)| = 0.0821216; step 1\n",
      "3:  |F(x)| = 0.00175468; step 1\n",
      "4:  |F(x)| = 4.70295e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "Operator 'dc' not implemented. Continuing without operator 'dc'...\n",
      "Operator 's' not implemented. Continuing without operator 's'...\n",
      "0:  |F(x)| = 2.26084; step 0.444771\n",
      "1:  |F(x)| = 0.149802; step 1\n",
      "2:  |F(x)| = 0.00695692; step 1\n",
      "3:  |F(x)| = 0.000261272; step 1\n",
      "4:  |F(x)| = 7.91229e-06; step 1\n",
      "5:  |F(x)| = 3.30818e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 4.18082; step 0.239158\n",
      "1:  |F(x)| = 1.96498; step 0.510429\n",
      "2:  |F(x)| = 0.18399; step 1\n",
      "3:  |F(x)| = 0.00800991; step 1\n",
      "4:  |F(x)| = 0.000294146; step 1\n",
      "5:  |F(x)| = 1.46928e-05; step 1\n",
      "6:  |F(x)| = 6.91873e-07; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 0.291153; step 1\n",
      "1:  |F(x)| = 0.0116607; step 1\n",
      "2:  |F(x)| = 0.000589582; step 1\n",
      "3:  |F(x)| = 3.50918e-05; step 1\n",
      "4:  |F(x)| = 2.1044e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.6706; step 0.321603\n",
      "1:  |F(x)| = 2.53862; step 0.297412\n",
      "2:  |F(x)| = 0.572981; step 1\n",
      "3:  |F(x)| = 0.0554673; step 1\n",
      "4:  |F(x)| = 0.000778081; step 1\n",
      "5:  |F(x)| = 6.39487e-06; step 1\n",
      "6:  |F(x)| = 1.1547e-07; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 0.342161; step 1\n",
      "1:  |F(x)| = 0.0238373; step 1\n",
      "2:  |F(x)| = 0.00279042; step 1\n",
      "3:  |F(x)| = 0.000357872; step 1\n",
      "4:  |F(x)| = 4.61703e-05; step 1\n",
      "5:  |F(x)| = 5.96133e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.34926; step 0.379281\n",
      "1:  |F(x)| = 1.58192; step 0.496521\n",
      "2:  |F(x)| = 0.10262; step 1\n",
      "3:  |F(x)| = 0.00269796; step 1\n",
      "4:  |F(x)| = 2.02944e-05; step 1\n",
      "5:  |F(x)| = 3.29012e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.236387; step 1\n",
      "1:  |F(x)| = 0.00675202; step 1\n",
      "2:  |F(x)| = 0.000216495; step 1\n",
      "3:  |F(x)| = 8.43488e-06; step 1\n",
      "4:  |F(x)| = 3.31657e-07; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 0.197407; step 1\n",
      "1:  |F(x)| = 0.0135098; step 1\n",
      "2:  |F(x)| = 0.00180125; step 1\n",
      "3:  |F(x)| = 0.000252907; step 1\n",
      "4:  |F(x)| = 3.56664e-05; step 1\n",
      "5:  |F(x)| = 5.03241e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.25599; step 1\n",
      "1:  |F(x)| = 0.0114765; step 1\n",
      "2:  |F(x)| = 0.00080008; step 1\n",
      "3:  |F(x)| = 6.17009e-05; step 1\n",
      "4:  |F(x)| = 4.7844e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.6245; step 0.316236\n",
      "1:  |F(x)| = 1.35567; step 0.54398\n",
      "2:  |F(x)| = 0.0174063; step 1\n",
      "3:  |F(x)| = 0.00030114; step 1\n",
      "4:  |F(x)| = 3.1548e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.6904; step 0.314018\n",
      "1:  |F(x)| = 1.68433; step 0.479512\n",
      "2:  |F(x)| = 0.0701329; step 1\n",
      "3:  |F(x)| = 0.0014586; step 1\n",
      "4:  |F(x)| = 1.28996e-05; step 1\n",
      "5:  |F(x)| = 4.12455e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.289785; step 1\n",
      "1:  |F(x)| = 0.00909696; step 1\n",
      "2:  |F(x)| = 9.39766e-05; step 1\n",
      "3:  |F(x)| = 1.64198e-06; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 2.88087; step 0.468199\n",
      "1:  |F(x)| = 0.519168; step 1\n",
      "2:  |F(x)| = 0.0440863; step 1\n",
      "3:  |F(x)| = 0.000730895; step 1\n",
      "4:  |F(x)| = 1.77952e-05; step 1\n",
      "5:  |F(x)| = 4.87609e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.17897; step 1\n",
      "1:  |F(x)| = 0.443099; step 1\n",
      "2:  |F(x)| = 0.0330895; step 1\n",
      "3:  |F(x)| = 0.00082341; step 1\n",
      "4:  |F(x)| = 4.09654e-05; step 1\n",
      "5:  |F(x)| = 1.88423e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.64905; step 0.325512\n",
      "1:  |F(x)| = 2.39089; step 0.3313\n",
      "2:  |F(x)| = 0.442823; step 1\n",
      "3:  |F(x)| = 0.0351274; step 1\n",
      "4:  |F(x)| = 0.000370532; step 1\n",
      "5:  |F(x)| = 4.5945e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.59163; step 0.344266\n",
      "1:  |F(x)| = 1.13072; step 1\n",
      "2:  |F(x)| = 0.174616; step 1\n",
      "3:  |F(x)| = 0.00662455; step 1\n",
      "4:  |F(x)| = 8.40353e-05; step 1\n",
      "5:  |F(x)| = 2.27982e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.181721; step 1\n",
      "1:  |F(x)| = 0.00982611; step 1\n",
      "2:  |F(x)| = 0.000991724; step 1\n",
      "3:  |F(x)| = 0.000105547; step 1\n",
      "4:  |F(x)| = 1.12804e-05; step 1\n",
      "5:  |F(x)| = 1.20619e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.67689; step 0.319464\n",
      "1:  |F(x)| = 2.40293; step 0.333218\n",
      "2:  |F(x)| = 0.429872; step 1\n",
      "3:  |F(x)| = 0.0329547; step 1\n",
      "4:  |F(x)| = 0.000342143; step 1\n",
      "5:  |F(x)| = 4.51004e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.21985; step 0.399291\n",
      "1:  |F(x)| = 1.05389; step 1\n",
      "2:  |F(x)| = 0.148028; step 1\n",
      "3:  |F(x)| = 0.00484617; step 1\n",
      "4:  |F(x)| = 6.12792e-05; step 1\n",
      "5:  |F(x)| = 1.80323e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.818545; step 1\n",
      "1:  |F(x)| = 0.104996; step 1\n",
      "2:  |F(x)| = 0.00238547; step 1\n",
      "3:  |F(x)| = 1.54869e-05; step 1\n",
      "4:  |F(x)| = 1.77697e-07; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "Operator 'dc' not implemented. Continuing without operator 'dc'...\n",
      "Operator 's' not implemented. Continuing without operator 's'...\n",
      "0 42.617582254414216 \tLoss(validation) = 24.87151865168212\n",
      "100 0.15044624425263797 \tLoss(validation) = 0.13606947701899702\n",
      "200 0.10821228986090428 \tLoss(validation) = 0.0898456865676605\n",
      "300 0.09821363053262208 \tLoss(validation) = 0.08077244736939697\n",
      "400 0.08940111615759096 \tLoss(validation) = 0.07250549693598667\n",
      "500 0.0814959047856353 \tLoss(validation) = 0.06505464226263626\n",
      "600 0.07440481319627294 \tLoss(validation) = 0.05847600323903327\n",
      "700 0.0679336819979692 \tLoss(validation) = 0.052622283258022096\n",
      "800 0.06188630378158187 \tLoss(validation) = 0.0473321108171784\n",
      "900 0.05614339069288717 \tLoss(validation) = 0.042519865374871534\n",
      "1000 0.05068365163521853 \tLoss(validation) = 0.03818431586283325\n",
      "1100 0.04555850565872982 \tLoss(validation) = 0.03436509196453179\n",
      "1200 0.04084002692102455 \tLoss(validation) = 0.03108003621853862\n",
      "1300 0.03656790424355366 \tLoss(validation) = 0.028279294383326437\n",
      "1400 0.03271633614647345 \tLoss(validation) = 0.02582918267732206\n",
      "1500 0.029197371224081798 \tLoss(validation) = 0.02353204673641702\n",
      "1600 0.025915648205774973 \tLoss(validation) = 0.02122135055462172\n",
      "1700 0.02284303524389343 \tLoss(validation) = 0.018864342516022925\n",
      "1800 0.020017177572204246 \tLoss(validation) = 0.01652509604715832\n",
      "1900 0.017469835653581435 \tLoss(validation) = 0.014286141622503506\n",
      "2000 0.01520658466194747 \tLoss(validation) = 0.012218638646384263\n",
      "2100 0.013219454296548323 \tLoss(validation) = 0.010371403643858561\n",
      "2200 0.01149525634427446 \tLoss(validation) = 0.008768283810998105\n",
      "2300 0.010018824089672012 \tLoss(validation) = 0.007411227132677762\n",
      "2400 0.008772919366422171 \tLoss(validation) = 0.006286310963151307\n",
      "2500 0.007736743562576134 \tLoss(validation) = 0.0053702535170635545\n",
      "2600 0.006885180974748531 \tLoss(validation) = 0.004635950348598627\n",
      "2700 0.006190201424866279 \tLoss(validation) = 0.004055880018637945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800 0.005623662705323408 \tLoss(validation) = 0.003603628723207036\n",
      "2900 0.005159639868018028 \tLoss(validation) = 0.003254444721717895\n",
      "3000 0.004775743074072366 \tLoss(validation) = 0.0029855608302874403\n",
      "3100 0.004453464982260074 \tLoss(validation) = 0.002776774137744427\n",
      "3200 0.004178086697646967 \tLoss(validation) = 0.002611122959940526\n",
      "3300 0.003938306013057581 \tLoss(validation) = 0.0024753064900518485\n",
      "3400 0.0037257024893573216 \tLoss(validation) = 0.002359615191168581\n",
      "3500 0.0035341132002055845 \tLoss(validation) = 0.0022574677913017585\n",
      "3600 0.003359084032186133 \tLoss(validation) = 0.0021646857154701176\n",
      "3700 0.003197362177374247 \tLoss(validation) = 0.00207880402791586\n",
      "3800 0.003046549871241742 \tLoss(validation) = 0.001998428198169275\n",
      "3900 0.0029048505957419467 \tLoss(validation) = 0.0019228649053299207\n",
      "4000 0.002770890620247391 \tLoss(validation) = 0.0018517321533333784\n",
      "4100 0.0026436037132552175 \tLoss(validation) = 0.001784845164081375\n",
      "4200 0.002522142083554891 \tLoss(validation) = 0.001722051421313036\n",
      "4300 0.0024057948511011 \tLoss(validation) = 0.0016631881737668892\n",
      "4400 0.0022939448928782622 \tLoss(validation) = 0.0016080249341065196\n",
      "4500 0.0021860121983944014 \tLoss(validation) = 0.001556234499881108\n",
      "4600 0.0020814390301833307 \tLoss(validation) = 0.0015074102085962885\n",
      "4700 0.001979689753388373 \tLoss(validation) = 0.0014610356189434955\n",
      "4800 0.0018802723434709905 \tLoss(validation) = 0.0014165043141944837\n",
      "4900 0.001782811277522994 \tLoss(validation) = 0.001373135736671302\n",
      "5000 0.0016871288333222157 \tLoss(validation) = 0.0013302096738823845\n",
      "5100 0.0015933334280459927 \tLoss(validation) = 0.0012870664035674827\n",
      "5200 0.0015018591447723787 \tLoss(validation) = 0.0012432161420777553\n",
      "5300 0.0014133981774963667 \tLoss(validation) = 0.001198471293319898\n",
      "5400 0.0013287169968330581 \tLoss(validation) = 0.0011529871828087272\n",
      "5500 0.0012484666395145634 \tLoss(validation) = 0.0011071501185971055\n",
      "5600 0.0011730614090142831 \tLoss(validation) = 0.0010615398928670856\n",
      "5700 0.00110266143387608 \tLoss(validation) = 0.0010167434601308118\n",
      "5800 0.0010372350057355983 \tLoss(validation) = 0.0009733254096467803\n",
      "5900 0.0009766079709966268 \tLoss(validation) = 0.0009317644429823475\n",
      "6000 0.0009205173120207949 \tLoss(validation) = 0.0008924220656752345\n",
      "6100 0.0008686284711982862 \tLoss(validation) = 0.0008554866951758135\n",
      "6200 0.0008205562449108868 \tLoss(validation) = 0.0008209815654526811\n",
      "6300 0.0007758858430975296 \tLoss(validation) = 0.0007887655677983405\n",
      "6400 0.0007342038100391178 \tLoss(validation) = 0.0007585638192995143\n",
      "6500 0.0006951184114408071 \tLoss(validation) = 0.0007300525880068426\n",
      "6600 0.0006582901465709978 \tLoss(validation) = 0.0007028838266322585\n",
      "6700 0.0006234363517806405 \tLoss(validation) = 0.0006767703686320325\n",
      "6800 0.0005903400229463139 \tLoss(validation) = 0.0006514667741561848\n",
      "6900 0.000558835700426858 \tLoss(validation) = 0.0006268243195710809\n",
      "7000 0.0005288057797314232 \tLoss(validation) = 0.0006027348826487979\n",
      "7100 0.0005001586369170151 \tLoss(validation) = 0.0005791482390714069\n",
      "7200 0.0004728295457980026 \tLoss(validation) = 0.0005560543149850025\n",
      "7300 0.0004467606553889139 \tLoss(validation) = 0.0005334677693300937\n",
      "7400 0.0004219077587160923 \tLoss(validation) = 0.0005114078129462843\n",
      "7500 0.0003982248759207562 \tLoss(validation) = 0.0004899143242323348\n",
      "7600 0.00037566218645614937 \tLoss(validation) = 0.00046902207456557906\n",
      "7700 0.00035417247980134577 \tLoss(validation) = 0.00044875346874024213\n",
      "7800 0.0003337016097597779 \tLoss(validation) = 0.000429140528254896\n",
      "7900 0.00031420119555462697 \tLoss(validation) = 0.00041018492119904995\n",
      "8000 0.00029561903785689845 \tLoss(validation) = 0.00039189351852619424\n",
      "8100 0.000277911572935163 \tLoss(validation) = 0.00037425339267814506\n",
      "8200 0.000261041887358613 \tLoss(validation) = 0.00035724267409417904\n",
      "8300 0.0002449784437975939 \tLoss(validation) = 0.00034083913239366895\n",
      "8400 0.00022970162036617617 \tLoss(validation) = 0.0003250066117417521\n",
      "8500 0.0002155086404552327 \tLoss(validation) = 0.0003098295380433323\n",
      "8600 0.00020202872770439282 \tLoss(validation) = 0.0002962008829662741\n",
      "8700 0.0001895611713029533 \tLoss(validation) = 0.000281839822852379\n",
      "8800 0.0001781678811773928 \tLoss(validation) = 0.0002703424708094557\n",
      "8900 0.00017063058801198634 \tLoss(validation) = 0.00025529588176726977\n",
      "9000 0.0001727357071077216 \tLoss(validation) = 0.00028109175382575873\n",
      "9100 0.00014881381133239916 \tLoss(validation) = 0.00023732960422534125\n",
      "9200 0.0001446677172902693 \tLoss(validation) = 0.00022543634069473157\n",
      "9300 0.00013298341504546527 \tLoss(validation) = 0.00022053840471353785\n",
      "9400 0.00012649180926611073 \tLoss(validation) = 0.00021259487029940552\n",
      "9500 0.00011965976637841802 \tLoss(validation) = 0.00020511794720584771\n",
      "9600 0.00011477966747481597 \tLoss(validation) = 0.00020439226441757948\n",
      "9700 0.00010842039317124171 \tLoss(validation) = 0.00019156116199496833\n",
      "9800 0.00010579422210291185 \tLoss(validation) = 0.0001932169440725608\n",
      "9900 9.89907903859583e-05 \tLoss(validation) = 0.00017905329028222047\n",
      "10000 0.00020584702816312965 \tLoss(validation) = 0.00021184419124096234\n",
      "10100 9.09344700483409e-05 \tLoss(validation) = 0.00016736720732636912\n",
      "10200 8.707791943788608e-05 \tLoss(validation) = 0.0001619788748286381\n",
      "10300 8.404229231868954e-05 \tLoss(validation) = 0.000154499262679591\n",
      "10400 8.060605651698736e-05 \tLoss(validation) = 0.00015168397469552373\n",
      "10500 0.00013654746569860361 \tLoss(validation) = 0.00030804370668036817\n",
      "10600 7.492872706719094e-05 \tLoss(validation) = 0.0001422720016141398\n",
      "10700 7.206598884135023e-05 \tLoss(validation) = 0.00013769679068248707\n",
      "10800 0.00015686690545375917 \tLoss(validation) = 0.0001611366911616442\n",
      "10900 6.728470523557617e-05 \tLoss(validation) = 0.00012983762229341168\n",
      "11000 6.483764814579513e-05 \tLoss(validation) = 0.00012542981583863052\n",
      "11100 6.635883667194278e-05 \tLoss(validation) = 0.0001331280595285661\n",
      "11200 6.0820453404186555e-05 \tLoss(validation) = 0.00011875662548708303\n",
      "11300 5.895142988111052e-05 \tLoss(validation) = 0.00011759316206033025\n",
      "11400 5.7198194718848396e-05 \tLoss(validation) = 0.00011211388147889269\n",
      "11500 5.5266472008369275e-05 \tLoss(validation) = 0.00010933339443538154\n",
      "11600 0.0011459681197677984 \tLoss(validation) = 0.00032329846236952284\n",
      "11700 5.220811901313798e-05 \tLoss(validation) = 0.00010492019671372488\n",
      "11800 5.052175134631525e-05 \tLoss(validation) = 0.00010130750049876407\n",
      "11900 0.0007201577012427409 \tLoss(validation) = 0.0014942555550145596\n",
      "12000 4.802014386332604e-05 \tLoss(validation) = 9.713138004858886e-05\n",
      "12100 4.6563037370231145e-05 \tLoss(validation) = 9.476184849687626e-05\n",
      "12200 4.515712088243597e-05 \tLoss(validation) = 9.231997597663302e-05\n",
      "12300 4.9809892417722216e-05 \tLoss(validation) = 9.164806281358081e-05\n",
      "12400 4.3343462986283875e-05 \tLoss(validation) = 8.96073433725579e-05\n",
      "12500 4.2136251962417365e-05 \tLoss(validation) = 8.758395587065518e-05\n",
      "12600 4.09675715134168e-05 \tLoss(validation) = 8.56219122054751e-05\n",
      "12700 4.793475712972501e-05 \tLoss(validation) = 8.347663525599906e-05\n",
      "12800 3.9297053350757805e-05 \tLoss(validation) = 8.309700870114783e-05\n",
      "12900 3.8273263599121315e-05 \tLoss(validation) = 8.140872271204915e-05\n",
      "13000 3.784896439842034e-05 \tLoss(validation) = 7.933830905096751e-05\n",
      "13100 3.685665119239486e-05 \tLoss(validation) = 7.941731274273517e-05\n",
      "13200 3.595610208776472e-05 \tLoss(validation) = 7.795960376414884e-05\n",
      "13300 3.597487873492313e-05 \tLoss(validation) = 7.561928455648902e-05\n",
      "13400 3.482497585642206e-05 \tLoss(validation) = 7.650150447311677e-05\n",
      "13500 3.402953030784342e-05 \tLoss(validation) = 7.53554425007792e-05\n",
      "13600 3.468349301392662e-05 \tLoss(validation) = 7.311206462519087e-05\n",
      "13700 3.303252475086433e-05 \tLoss(validation) = 7.405114281356889e-05\n",
      "13800 3.231912869928414e-05 \tLoss(validation) = 7.30697329688627e-05\n",
      "13900 3.212860356934754e-05 \tLoss(validation) = 7.301634527470923e-05\n",
      "14000 3.1404180985354266e-05 \tLoss(validation) = 7.18088391488484e-05\n",
      "14100 3.075748057016297e-05 \tLoss(validation) = 7.106511559169713e-05\n",
      "14200 3.413242245412488e-05 \tLoss(validation) = 6.897891544274537e-05\n",
      "14300 3.0022909349022333e-05 \tLoss(validation) = 7.016383125638036e-05\n",
      "14400 2.94299503543053e-05 \tLoss(validation) = 6.947559343213891e-05\n",
      "14500 3.7351042088196726e-05 \tLoss(validation) = 8.182400086444036e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14600 2.8698856379794774e-05 \tLoss(validation) = 6.85588489230701e-05\n",
      "14700 2.8163567129035812e-05 \tLoss(validation) = 6.754861548344473e-05\n",
      "14800 2.81531249122005e-05 \tLoss(validation) = 6.99547489643471e-05\n",
      "14900 2.7537624521389604e-05 \tLoss(validation) = 6.725081752631046e-05\n",
      "15000 2.7021325365371016e-05 \tLoss(validation) = 6.698370453685355e-05\n",
      "15100 2.7053516105267806e-05 \tLoss(validation) = 6.878016216771345e-05\n",
      "15200 2.6443382431579305e-05 \tLoss(validation) = 6.600686914065008e-05\n",
      "15300 2.5994268551214553e-05 \tLoss(validation) = 6.4806600561785e-05\n",
      "15400 2.626607442613163e-05 \tLoss(validation) = 6.439214963531845e-05\n",
      "15500 2.5388918614801862e-05 \tLoss(validation) = 6.486763882256717e-05\n",
      "15600 0.00023065701438056783 \tLoss(validation) = 0.00037329398654629357\n",
      "15700 2.492109553661464e-05 \tLoss(validation) = 6.45734848222418e-05\n",
      "15800 2.4375588447148267e-05 \tLoss(validation) = 6.378550266689108e-05\n",
      "15900 2.462845972429226e-05 \tLoss(validation) = 0.000175140859471425\n",
      "16000 2.3853385607788884e-05 \tLoss(validation) = 6.28093955865484e-05\n",
      "16100 2.3379945702392144e-05 \tLoss(validation) = 6.273602875457068e-05\n",
      "16200 4.990660477953523e-05 \tLoss(validation) = 7.317602095447331e-05\n",
      "16300 2.2940421644915815e-05 \tLoss(validation) = 6.207601353867243e-05\n",
      "16400 2.2486272743528264e-05 \tLoss(validation) = 6.176791656868405e-05\n",
      "16500 0.0004774262612032884 \tLoss(validation) = 0.0004334875520404676\n",
      "16600 2.2031507791063812e-05 \tLoss(validation) = 6.117261493654913e-05\n",
      "16700 2.1586539404189175e-05 \tLoss(validation) = 6.078728665818718e-05\n",
      "16800 0.0014659269623254552 \tLoss(validation) = 0.0007655863857675687\n",
      "16900 2.1147116390411997e-05 \tLoss(validation) = 6.067872466208044e-05\n",
      "17000 2.0707904717030655e-05 \tLoss(validation) = 5.984808175858224e-05\n",
      "17100 2.094867275336152e-05 \tLoss(validation) = 5.698480398525469e-05\n",
      "17200 2.055109146769455e-05 \tLoss(validation) = 5.790102079380345e-05\n",
      "17300 1.9910419896442693e-05 \tLoss(validation) = 5.895019045015781e-05\n",
      "17400 1.9517529166888235e-05 \tLoss(validation) = 5.861830334138775e-05\n",
      "17500 2.8737810776921183e-05 \tLoss(validation) = 8.22608504126292e-05\n",
      "17600 1.91723155390207e-05 \tLoss(validation) = 5.815233650712263e-05\n",
      "17700 1.880524398144833e-05 \tLoss(validation) = 5.7741103278420634e-05\n",
      "17800 1.8450431172409874e-05 \tLoss(validation) = 5.747086643270571e-05\n",
      "17900 1.9235403186840005e-05 \tLoss(validation) = 5.827113900056961e-05\n",
      "18000 1.8104186613122123e-05 \tLoss(validation) = 5.679807702136854e-05\n",
      "18100 1.7776506281139703e-05 \tLoss(validation) = 5.6427425935346015e-05\n",
      "18200 2.5057947166411652e-05 \tLoss(validation) = 8.53068577644493e-05\n",
      "18300 1.741804008655412e-05 \tLoss(validation) = 5.564538491833043e-05\n",
      "18400 1.7114931903310358e-05 \tLoss(validation) = 5.535994067787229e-05\n",
      "18500 2.4791676007751242e-05 \tLoss(validation) = 5.217806608176221e-05\n",
      "18600 1.6779950408686067e-05 \tLoss(validation) = 5.462687900456651e-05\n",
      "18700 1.650109797228159e-05 \tLoss(validation) = 5.426174302640064e-05\n",
      "18800 1.7732303931646983e-05 \tLoss(validation) = 5.208473011752609e-05\n",
      "18900 1.6179179064168243e-05 \tLoss(validation) = 5.3446199090922164e-05\n",
      "19000 1.6407910275268276e-05 \tLoss(validation) = 5.7027437425441736e-05\n",
      "19100 1.6086896969139895e-05 \tLoss(validation) = 5.350171096748672e-05\n",
      "19200 1.5650477481709486e-05 \tLoss(validation) = 5.218859711778017e-05\n",
      "19300 1.5411509805401723e-05 \tLoss(validation) = 5.1710440449631057e-05\n",
      "19400 1.6306878482749828e-05 \tLoss(validation) = 5.210399568063575e-05\n",
      "19500 1.5192823059086737e-05 \tLoss(validation) = 5.091714796968043e-05\n",
      "19600 1.4969838433437671e-05 \tLoss(validation) = 5.0439423798055435e-05\n",
      "19700 9.202087312898605e-05 \tLoss(validation) = 0.0001667686958222718\n",
      "19800 1.4711263862483948e-05 \tLoss(validation) = 4.938954654452361e-05\n",
      "19900 1.4497640618457052e-05 \tLoss(validation) = 4.9095330375324445e-05\n",
      "20000 2.1451659912083496e-05 \tLoss(validation) = 4.700279702413696e-05\n",
      "20100 1.4265096319324407e-05 \tLoss(validation) = 4.8310275863321623e-05\n",
      "20200 1.4065868759320288e-05 \tLoss(validation) = 4.777689840505531e-05\n",
      "20300 1.4685830604866713e-05 \tLoss(validation) = 5.47599105815005e-05\n",
      "20400 1.3867651667821823e-05 \tLoss(validation) = 4.707828067182759e-05\n",
      "20500 1.3680062917809571e-05 \tLoss(validation) = 4.6535412163392155e-05\n",
      "20600 5.598903454312642e-05 \tLoss(validation) = 6.94464516709072e-05\n",
      "20700 1.3469870131544622e-05 \tLoss(validation) = 4.5882699849466016e-05\n",
      "20800 1.329117570156904e-05 \tLoss(validation) = 4.529363689361738e-05\n",
      "20900 1.6778926239403136e-05 \tLoss(validation) = 4.8580535797513744e-05\n",
      "21000 1.3089547869998781e-05 \tLoss(validation) = 4.460983350417044e-05\n",
      "21100 1.2926387455913275e-05 \tLoss(validation) = 4.377169529772945e-05\n",
      "21200 1.2909008365228838e-05 \tLoss(validation) = 4.2833677175440615e-05\n",
      "21300 1.2735557592271225e-05 \tLoss(validation) = 4.3404429634513004e-05\n",
      "21400 1.7682857573088176e-05 \tLoss(validation) = 6.104019964206403e-05\n",
      "21500 1.263930495544923e-05 \tLoss(validation) = 4.344996817158573e-05\n",
      "21600 1.2398567666740314e-05 \tLoss(validation) = 4.227894928215938e-05\n",
      "21700 1.974919674816356e-05 \tLoss(validation) = 0.00014978232611040465\n",
      "21800 1.2251594738475999e-05 \tLoss(validation) = 4.152609378000459e-05\n",
      "21900 1.2090534693068594e-05 \tLoss(validation) = 4.1232901378367374e-05\n",
      "22000 5.8381323069682025e-05 \tLoss(validation) = 6.580912260312408e-05\n",
      "22100 1.1929063087844382e-05 \tLoss(validation) = 4.0883050661086174e-05\n",
      "22200 1.1775977824621996e-05 \tLoss(validation) = 4.018385134791638e-05\n",
      "22300 1.235294488281598e-05 \tLoss(validation) = 4.5740581667322224e-05\n",
      "22400 1.1618003916638545e-05 \tLoss(validation) = 3.9628311353531675e-05\n",
      "22500 4.375660495735726e-05 \tLoss(validation) = 0.00011685198312126161\n",
      "22600 1.1534272406007719e-05 \tLoss(validation) = 3.965642899951931e-05\n",
      "22700 1.1330949585141513e-05 \tLoss(validation) = 3.860489353610319e-05\n",
      "22800 5.7898174435367105e-05 \tLoss(validation) = 6.91438295734521e-05\n",
      "22900 1.119865759272399e-05 \tLoss(validation) = 3.8020968577080857e-05\n",
      "23000 1.106502459772269e-05 \tLoss(validation) = 3.773603866710457e-05\n",
      "23100 1.5438633164609396e-05 \tLoss(validation) = 3.6155682944966416e-05\n",
      "23200 1.0944599535592837e-05 \tLoss(validation) = 3.73053126763482e-05\n",
      "23300 1.08176204914047e-05 \tLoss(validation) = 3.685371240905846e-05\n",
      "23400 1.286188100571134e-05 \tLoss(validation) = 3.68276672450276e-05\n",
      "23500 1.0700047474935534e-05 \tLoss(validation) = 3.637921790812931e-05\n",
      "23600 1.0577137554531105e-05 \tLoss(validation) = 3.6055738654546457e-05\n",
      "23700 1.1484851607647611e-05 \tLoss(validation) = 3.396773453610415e-05\n",
      "23800 1.0447962316938619e-05 \tLoss(validation) = 3.5549130130596045e-05\n",
      "23900 0.00040943909441324545 \tLoss(validation) = 0.00010346440490298452\n",
      "24000 1.0342457347996694e-05 \tLoss(validation) = 3.536922256356344e-05\n",
      "24100 1.0204828119732931e-05 \tLoss(validation) = 3.473316635337785e-05\n",
      "24200 1.0640178557871119e-05 \tLoss(validation) = 4.163048429421053e-05\n",
      "24300 1.0092080866579022e-05 \tLoss(validation) = 3.431143061120925e-05\n",
      "24400 9.976774740987615e-06 \tLoss(validation) = 3.395917928908192e-05\n",
      "24500 1.0141358970540379e-05 \tLoss(validation) = 3.471031527639518e-05\n",
      "24600 9.874444338628634e-06 \tLoss(validation) = 3.363348751203594e-05\n",
      "24700 9.773180996307737e-06 \tLoss(validation) = 3.358583539479421e-05\n",
      "24800 1.004316827410648e-05 \tLoss(validation) = 3.18803939124175e-05\n",
      "24900 9.672678099983501e-06 \tLoss(validation) = 3.2911530654804885e-05\n",
      "25000 9.565335361155649e-06 \tLoss(validation) = 3.2501475547622994e-05\n",
      "25100 9.662779212194926e-06 \tLoss(validation) = 3.343496229935956e-05\n",
      "25200 9.472259825399089e-06 \tLoss(validation) = 3.224357766577455e-05\n",
      "25300 9.369668495114189e-06 \tLoss(validation) = 3.2072566389813225e-05\n",
      "25400 9.403459848243415e-06 \tLoss(validation) = 3.1265240491295884e-05\n",
      "25500 9.28625997167194e-06 \tLoss(validation) = 3.1605158614935876e-05\n",
      "25600 9.18340400720611e-06 \tLoss(validation) = 3.1237752856191043e-05\n",
      "25700 9.390342128897232e-06 \tLoss(validation) = 2.9848300682684344e-05\n",
      "25800 9.092253438053173e-06 \tLoss(validation) = 3.0937385884241673e-05\n",
      "25900 1.1690791969953081e-05 \tLoss(validation) = 3.010960233803105e-05\n",
      "26000 9.093727910668965e-06 \tLoss(validation) = 3.0069250095080258e-05\n",
      "26100 8.906205135091472e-06 \tLoss(validation) = 3.0332869054338127e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26200 1.017836039258843e-05 \tLoss(validation) = 2.872480324669389e-05\n",
      "26300 8.890035909770418e-06 \tLoss(validation) = 3.054423006832454e-05\n",
      "26400 8.755169041389803e-06 \tLoss(validation) = 2.9843173424458932e-05\n",
      "26500 8.659096034214553e-06 \tLoss(validation) = 2.9538141490076333e-05\n",
      "26600 1.6379795940602822e-05 \tLoss(validation) = 2.8230823846301426e-05\n",
      "26700 8.581493968943535e-06 \tLoss(validation) = 2.915900691582453e-05\n",
      "26800 8.485365323097722e-06 \tLoss(validation) = 2.8840084623666674e-05\n",
      "26900 8.678798435255907e-06 \tLoss(validation) = 3.05494961067047e-05\n",
      "27000 8.407160069391716e-06 \tLoss(validation) = 2.866781549391729e-05\n",
      "27100 8.598928124087626e-06 \tLoss(validation) = 2.7192669112537018e-05\n",
      "27200 8.391969286429125e-06 \tLoss(validation) = 2.8329236089406098e-05\n",
      "27300 8.251324892736315e-06 \tLoss(validation) = 2.8148402859981237e-05\n",
      "27400 1.0767087457800037e-05 \tLoss(validation) = 3.71183165274927e-05\n",
      "27500 8.20757691715393e-06 \tLoss(validation) = 2.838940204967688e-05\n",
      "27600 8.075353014185427e-06 \tLoss(validation) = 2.7578965524051735e-05\n",
      "27700 9.090614991882863e-06 \tLoss(validation) = 3.0967920421845434e-05\n",
      "27800 8.011361658054045e-06 \tLoss(validation) = 2.7337530015218925e-05\n",
      "27900 7.922569600090598e-06 \tLoss(validation) = 2.7104033843248323e-05\n",
      "28000 8.954630318138632e-06 \tLoss(validation) = 2.6978321538300263e-05\n",
      "28100 7.871719990788299e-06 \tLoss(validation) = 2.6864707989007277e-05\n",
      "28200 7.784592759120263e-06 \tLoss(validation) = 2.6653072708646945e-05\n",
      "28300 0.00041406266338934984 \tLoss(validation) = 0.00031618846594461537\n",
      "28400 7.785514596854765e-06 \tLoss(validation) = 2.6298628561158195e-05\n",
      "28500 7.692463568486712e-06 \tLoss(validation) = 2.6428614076991142e-05\n",
      "28600 7.611554527762901e-06 \tLoss(validation) = 2.6147736728052705e-05\n",
      "28700 0.00015408678143733184 \tLoss(validation) = 2.896138515308264e-05\n",
      "28800 7.581305352633811e-06 \tLoss(validation) = 2.6129532272374627e-05\n",
      "28900 7.486517522784126e-06 \tLoss(validation) = 2.5736463632559894e-05\n",
      "29000 9.797899952099229e-05 \tLoss(validation) = 0.00011616262370666617\n",
      "29100 7.426996038938177e-06 \tLoss(validation) = 2.5395795090131595e-05\n",
      "29200 7.342248423403949e-06 \tLoss(validation) = 2.5267283448229647e-05\n",
      "29300 8.057148109472382e-06 \tLoss(validation) = 2.4266806857615813e-05\n",
      "29400 7.2929980929468504e-06 \tLoss(validation) = 2.5089172148293897e-05\n",
      "29500 7.213367344400685e-06 \tLoss(validation) = 2.4853420561822717e-05\n",
      "29600 7.863612335094828e-06 \tLoss(validation) = 2.4535601838184538e-05\n",
      "29700 7.156584964782009e-06 \tLoss(validation) = 2.470100113346358e-05\n",
      "29800 0.0001637016234972368 \tLoss(validation) = 0.00026016088646063347\n",
      "29900 7.1045495254674126e-06 \tLoss(validation) = 2.4155743217452457e-05\n",
      "30000 7.0150338198966595e-06 \tLoss(validation) = 2.42386797502263e-05\n",
      "30100 9.950722292474122e-06 \tLoss(validation) = 2.3025972619297815e-05\n",
      "30200 6.9584535831067035e-06 \tLoss(validation) = 2.4062344374488067e-05\n",
      "30300 8.149256615224009e-06 \tLoss(validation) = 2.2899192663178955e-05\n",
      "30400 6.952931337071163e-06 \tLoss(validation) = 2.3508263398944965e-05\n",
      "30500 6.821906025953451e-06 \tLoss(validation) = 2.362129000075265e-05\n",
      "30600 1.2629731888513818e-05 \tLoss(validation) = 3.554564660848426e-05\n",
      "30700 6.764703125622521e-06 \tLoss(validation) = 2.3405508616681195e-05\n",
      "30800 1.3999251040003255e-05 \tLoss(validation) = 4.326439696463622e-05\n",
      "30900 6.713005613679182e-06 \tLoss(validation) = 2.2991480004360875e-05\n",
      "31000 6.631161494455346e-06 \tLoss(validation) = 2.3025631437219807e-05\n",
      "31100 8.872303267484836e-06 \tLoss(validation) = 2.4710348184829915e-05\n",
      "31200 6.57260413500909e-06 \tLoss(validation) = 2.283474871428972e-05\n",
      "31300 0.0006008532103925139 \tLoss(validation) = 0.0008102195279706589\n",
      "31400 6.536065585911855e-06 \tLoss(validation) = 2.2774128762297824e-05\n",
      "31500 6.462587705233473e-06 \tLoss(validation) = 2.2474164409464686e-05\n",
      "31600 0.0003231864069610072 \tLoss(validation) = 6.49982900750772e-05\n",
      "31700 6.428012124624745e-06 \tLoss(validation) = 2.227362258087077e-05\n",
      "31800 6.3549668214361855e-06 \tLoss(validation) = 2.2126574531314005e-05\n",
      "31900 3.869563539284574e-05 \tLoss(validation) = 8.324405303640292e-05\n",
      "32000 6.310559912469336e-06 \tLoss(validation) = 2.1948905422851918e-05\n",
      "32100 6.2429240854429305e-06 \tLoss(validation) = 2.1799603832666684e-05\n",
      "32200 6.743708123247791e-06 \tLoss(validation) = 2.2581334812468504e-05\n",
      "32300 6.1903439261552376e-06 \tLoss(validation) = 2.159136218153796e-05\n",
      "32400 7.135103269006078e-05 \tLoss(validation) = 7.438817277686335e-05\n",
      "32500 6.1443868582523374e-06 \tLoss(validation) = 2.1337565996717762e-05\n",
      "32600 6.07643056207584e-06 \tLoss(validation) = 2.1153437214838863e-05\n",
      "32700 6.548341959572776e-06 \tLoss(validation) = 2.24647602271301e-05\n",
      "32800 6.026500815527357e-06 \tLoss(validation) = 2.106750031758787e-05\n",
      "32900 8.216802260915209e-06 \tLoss(validation) = 2.8334955838931967e-05\n",
      "33000 5.988346800744984e-06 \tLoss(validation) = 2.0980906100507172e-05\n",
      "33100 5.9230431674702925e-06 \tLoss(validation) = 2.0738835223511213e-05\n",
      "33200 7.928777861504954e-06 \tLoss(validation) = 2.277646458815314e-05\n",
      "33300 5.873112258016374e-06 \tLoss(validation) = 2.0566565202434533e-05\n",
      "33400 3.412454481389114e-05 \tLoss(validation) = 0.00010241000598079284\n",
      "33500 5.830663300600526e-06 \tLoss(validation) = 2.0376353297681955e-05\n",
      "33600 5.768327535722254e-06 \tLoss(validation) = 2.0218998923412685e-05\n",
      "33700 5.9258385140679446e-06 \tLoss(validation) = 1.9667158638419546e-05\n",
      "33800 5.72997107657622e-06 \tLoss(validation) = 2.0084268555299468e-05\n",
      "33900 5.986354481929971e-06 \tLoss(validation) = 2.1864596624220974e-05\n",
      "34000 5.718170119705915e-06 \tLoss(validation) = 2.040159284541698e-05\n",
      "34100 5.637738980272514e-06 \tLoss(validation) = 1.9819735406397143e-05\n",
      "34200 0.0003833505124064635 \tLoss(validation) = 0.00016776271611583233\n",
      "34300 5.601149301378537e-06 \tLoss(validation) = 1.977102342724626e-05\n",
      "34400 5.5409311266117435e-06 \tLoss(validation) = 1.952974923175431e-05\n",
      "34500 6.901127979353754e-06 \tLoss(validation) = 1.8400032956831417e-05\n",
      "34600 5.496248661205144e-06 \tLoss(validation) = 1.938957422002687e-05\n",
      "34700 3.826987200958647e-05 \tLoss(validation) = 0.00012147331336236265\n",
      "34800 5.455459480004066e-06 \tLoss(validation) = 1.9201957461237137e-05\n",
      "34900 5.397587708774431e-06 \tLoss(validation) = 1.9081814741653073e-05\n",
      "35000 5.93564450266597e-06 \tLoss(validation) = 2.0029113517240456e-05\n",
      "35100 5.363036321798536e-06 \tLoss(validation) = 1.8948867754163452e-05\n",
      "35200 6.022680072678705e-06 \tLoss(validation) = 2.2092907020498674e-05\n",
      "35300 5.358510106152868e-06 \tLoss(validation) = 1.8650178023605537e-05\n",
      "35400 5.2898749499478185e-06 \tLoss(validation) = 1.8749499720000825e-05\n",
      "35500 1.9995256932011387e-05 \tLoss(validation) = 1.7764279156039523e-05\n",
      "35600 5.2500643475259756e-06 \tLoss(validation) = 1.8694450710939945e-05\n",
      "35700 5.197880414597051e-06 \tLoss(validation) = 1.8506544571956657e-05\n",
      "35800 5.240597429258333e-06 \tLoss(validation) = 1.7804606548359063e-05\n",
      "35900 5.158961346943589e-06 \tLoss(validation) = 1.83409092081369e-05\n",
      "36000 0.00025475810200648746 \tLoss(validation) = 0.00019009138421473572\n",
      "36100 5.12706716655865e-06 \tLoss(validation) = 1.828418785445906e-05\n",
      "36200 5.0745283451857815e-06 \tLoss(validation) = 1.8081099625357855e-05\n",
      "36300 5.487409301379486e-06 \tLoss(validation) = 2.322135872153873e-05\n",
      "36400 5.0400418678998475e-06 \tLoss(validation) = 1.792837967307277e-05\n",
      "36500 5.404403377283255e-06 \tLoss(validation) = 1.9849746207629337e-05\n",
      "36600 5.014178194972664e-06 \tLoss(validation) = 1.779071006663684e-05\n",
      "36700 4.949879579681697e-06 \tLoss(validation) = 1.7703332333125398e-05\n",
      "36800 6.157055887491013e-06 \tLoss(validation) = 1.8114257070068598e-05\n",
      "36900 4.922093126111451e-06 \tLoss(validation) = 1.7639789203384222e-05\n",
      "37000 3.778337656014536e-05 \tLoss(validation) = 0.0001400244237008115\n",
      "37100 4.889520690552542e-06 \tLoss(validation) = 1.7432042225140968e-05\n",
      "37200 4.8363259105737606e-06 \tLoss(validation) = 1.7394494256703402e-05\n",
      "37300 6.0822391290686765e-06 \tLoss(validation) = 1.6459470144973702e-05\n",
      "37400 4.798349999017822e-06 \tLoss(validation) = 1.727850559138245e-05\n",
      "37500 4.995942117510081e-05 \tLoss(validation) = 8.719285766030058e-05\n",
      "37600 4.764452310938333e-06 \tLoss(validation) = 1.7225899622649926e-05\n",
      "37700 4.717207812980626e-06 \tLoss(validation) = 1.708259166440301e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37800 5.039814444024644e-06 \tLoss(validation) = 1.7876409360437993e-05\n",
      "37900 4.685315402028113e-06 \tLoss(validation) = 1.6939036146465952e-05\n",
      "38000 9.490442036730173e-05 \tLoss(validation) = 6.797974297362722e-05\n",
      "38100 4.655501053849201e-06 \tLoss(validation) = 1.6798800683611803e-05\n",
      "38200 4.608161231163167e-06 \tLoss(validation) = 1.674069300535584e-05\n",
      "38300 5.004540999817329e-06 \tLoss(validation) = 1.781103007325447e-05\n",
      "38400 4.585485899925653e-06 \tLoss(validation) = 1.667563977226904e-05\n",
      "38500 6.368542986015127e-06 \tLoss(validation) = 1.676237708387417e-05\n",
      "38600 4.590739848768236e-06 \tLoss(validation) = 1.6941336728385016e-05\n",
      "38700 4.51401536327429e-06 \tLoss(validation) = 1.647779372318012e-05\n",
      "38800 9.72075292724354e-06 \tLoss(validation) = 2.5362187108995505e-05\n",
      "38900 4.492819074367469e-06 \tLoss(validation) = 1.6459565585668932e-05\n",
      "39000 4.449301321349501e-06 \tLoss(validation) = 1.6298407097843333e-05\n",
      "39100 5.35098667469331e-06 \tLoss(validation) = 1.68434517320697e-05\n",
      "39200 4.423921003255608e-06 \tLoss(validation) = 1.6232284665253117e-05\n",
      "39300 3.124728609698079e-05 \tLoss(validation) = 6.958064118058929e-05\n",
      "39400 4.395589107179742e-06 \tLoss(validation) = 1.6045231701236532e-05\n",
      "39500 4.351104043963425e-06 \tLoss(validation) = 1.601351467144612e-05\n",
      "39600 4.399646793924062e-06 \tLoss(validation) = 1.5401538293628784e-05\n",
      "39700 4.317147434553484e-06 \tLoss(validation) = 1.5962239199335375e-05\n",
      "39800 8.373546759823483e-06 \tLoss(validation) = 2.059878123904263e-05\n",
      "39900 4.293406862456272e-06 \tLoss(validation) = 1.5889621377134754e-05\n",
      "40000 4.253194221380499e-06 \tLoss(validation) = 1.5810786086378678e-05\n",
      "40100 4.341104243516104e-06 \tLoss(validation) = 1.649255022346012e-05\n",
      "40200 4.228030519703334e-06 \tLoss(validation) = 1.5729100065035893e-05\n",
      "40300 0.0001891531714319667 \tLoss(validation) = 0.0002940485649167037\n",
      "40400 4.214053164925839e-06 \tLoss(validation) = 1.567064729719476e-05\n",
      "40500 4.1729546866563005e-06 \tLoss(validation) = 1.5578046193353954e-05\n",
      "40600 2.0923667497036007e-05 \tLoss(validation) = 1.488503005457052e-05\n",
      "40700 4.151000529441145e-06 \tLoss(validation) = 1.5516217491454024e-05\n",
      "40800 4.1113501399912265e-06 \tLoss(validation) = 1.5426807679832782e-05\n",
      "40900 4.64217575609913e-06 \tLoss(validation) = 1.4919041305151195e-05\n",
      "41000 4.096274156808511e-06 \tLoss(validation) = 1.538352682263515e-05\n",
      "41100 4.058804042565502e-06 \tLoss(validation) = 1.528437614735794e-05\n",
      "41200 4.2435355233821714e-06 \tLoss(validation) = 1.4684526505685149e-05\n",
      "41300 4.038169529150114e-06 \tLoss(validation) = 1.526504012346735e-05\n",
      "41400 0.0001441191595305986 \tLoss(validation) = 0.00018082311097027029\n",
      "41500 4.0199817139624426e-06 \tLoss(validation) = 1.5140061230352022e-05\n",
      "41600 3.976889542216414e-06 \tLoss(validation) = 1.5086624539925392e-05\n",
      "41700 4.235008348829183e-06 \tLoss(validation) = 1.4654916702660584e-05\n",
      "41800 3.954188581026115e-06 \tLoss(validation) = 1.506340582694877e-05\n",
      "41900 0.0004409630993160397 \tLoss(validation) = 0.00044229956014826214\n",
      "42000 3.9481390640528995e-06 \tLoss(validation) = 1.506683441496346e-05\n",
      "42100 3.898692654981587e-06 \tLoss(validation) = 1.4907831747069947e-05\n",
      "42200 6.033777048811821e-06 \tLoss(validation) = 2.2320818338029183e-05\n",
      "42300 3.879171320093184e-06 \tLoss(validation) = 1.488743107814042e-05\n",
      "42400 3.845894581015367e-06 \tLoss(validation) = 1.485363904804229e-05\n",
      "42500 3.9791443627956065e-06 \tLoss(validation) = 1.558446892642369e-05\n",
      "42600 3.820422276810804e-06 \tLoss(validation) = 1.4743816685797978e-05\n",
      "42700 8.056112552370532e-06 \tLoss(validation) = 1.5491204421613478e-05\n",
      "42800 3.797432611661191e-06 \tLoss(validation) = 1.4658502877831054e-05\n",
      "42900 3.829835987096073e-06 \tLoss(validation) = 1.515198122090549e-05\n",
      "43000 3.83619276868932e-06 \tLoss(validation) = 1.4729088402840018e-05\n",
      "43100 3.7437527933674393e-06 \tLoss(validation) = 1.4563370931790172e-05\n",
      "43200 1.2828241772605518e-05 \tLoss(validation) = 5.6901872640010385e-05\n",
      "43300 3.7219841445212054e-06 \tLoss(validation) = 1.4474761560314682e-05\n",
      "43400 3.6881033879640196e-06 \tLoss(validation) = 1.4460412976913516e-05\n",
      "43500 3.742862743624449e-06 \tLoss(validation) = 1.4810421763400096e-05\n",
      "43600 3.672513205406816e-06 \tLoss(validation) = 1.4408060752614038e-05\n",
      "43700 3.841921353340518e-06 \tLoss(validation) = 1.3816047131305504e-05\n",
      "43800 3.6572329119399067e-06 \tLoss(validation) = 1.421823275806375e-05\n",
      "43900 3.6210364934031054e-06 \tLoss(validation) = 1.4275628823229847e-05\n",
      "44000 4.612521755896762e-06 \tLoss(validation) = 1.647109226126429e-05\n",
      "44100 3.6008868018085116e-06 \tLoss(validation) = 1.4226948275237395e-05\n",
      "44200 3.6134198863651774e-06 \tLoss(validation) = 1.4558309184554372e-05\n",
      "44300 3.5954368298745933e-06 \tLoss(validation) = 1.4133414603582111e-05\n",
      "44400 3.5504170645704665e-06 \tLoss(validation) = 1.4112628630497776e-05\n",
      "44500 2.288637010950544e-05 \tLoss(validation) = 2.2518663126389727e-05\n",
      "44600 3.533974615747658e-06 \tLoss(validation) = 1.4057856052911447e-05\n",
      "44700 3.5033863602636685e-06 \tLoss(validation) = 1.3977028239765842e-05\n",
      "44800 4.206389663153288e-06 \tLoss(validation) = 1.3360400397092988e-05\n",
      "44900 3.4843491343204516e-06 \tLoss(validation) = 1.3975007738019788e-05\n",
      "45000 8.751722157709317e-06 \tLoss(validation) = 5.928279342072111e-05\n",
      "45100 3.469874007171705e-06 \tLoss(validation) = 1.4006886987550954e-05\n",
      "45200 3.437685707599629e-06 \tLoss(validation) = 1.3840147455333106e-05\n",
      "45300 5.840444354080389e-06 \tLoss(validation) = 1.8362952074543923e-05\n",
      "45400 3.4233169286615187e-06 \tLoss(validation) = 1.3839392937744224e-05\n",
      "45500 3.3957048776430553e-06 \tLoss(validation) = 1.3778411572850066e-05\n",
      "45600 3.4272951052955215e-06 \tLoss(validation) = 1.4058039534215951e-05\n",
      "45700 3.3755002488649157e-06 \tLoss(validation) = 1.369951283631492e-05\n",
      "45800 4.9989102474763076e-05 \tLoss(validation) = 3.669802016941715e-05\n",
      "45900 3.3639163576676924e-06 \tLoss(validation) = 1.3715787536432392e-05\n",
      "46000 3.3339553985822e-06 \tLoss(validation) = 1.358806459464702e-05\n",
      "46100 6.549647936954323e-06 \tLoss(validation) = 1.319284093410198e-05\n",
      "46200 3.3156718906575226e-06 \tLoss(validation) = 1.3569766371520425e-05\n",
      "46300 0.00023376583183035092 \tLoss(validation) = 8.603470099414149e-05\n",
      "46400 3.3025421355268306e-06 \tLoss(validation) = 1.345149060625367e-05\n",
      "46500 3.2721262064095476e-06 \tLoss(validation) = 1.3443680491281113e-05\n",
      "46600 1.1059445222154878e-05 \tLoss(validation) = 1.4553377698796148e-05\n",
      "46700 3.2594433930462974e-06 \tLoss(validation) = 1.3427393686471652e-05\n",
      "46800 3.23313596044581e-06 \tLoss(validation) = 1.3368810725265785e-05\n",
      "46900 3.3526178271345396e-06 \tLoss(validation) = 1.2991243208513065e-05\n",
      "47000 3.214033124227681e-06 \tLoss(validation) = 1.3313557871655925e-05\n",
      "47100 2.2370346844070643e-05 \tLoss(validation) = 2.51642540082106e-05\n",
      "47200 3.1986336214292336e-06 \tLoss(validation) = 1.3301906932029527e-05\n",
      "47300 3.1719860264860934e-06 \tLoss(validation) = 1.321073887917423e-05\n",
      "47400 3.4254176353633864e-06 \tLoss(validation) = 1.3004198122324207e-05\n",
      "47500 3.1564787811525796e-06 \tLoss(validation) = 1.3170803630102547e-05\n",
      "47600 3.707999033714549e-05 \tLoss(validation) = 5.22110314381195e-05\n",
      "47700 3.1394291297307866e-06 \tLoss(validation) = 1.3091951204318608e-05\n",
      "47800 3.7771314313690683e-06 \tLoss(validation) = 1.537486971154027e-05\n",
      "47900 3.141671643349986e-06 \tLoss(validation) = 1.2984715591015657e-05\n",
      "48000 3.0936807824032255e-06 \tLoss(validation) = 1.301386289502432e-05\n",
      "48100 3.6046205621728142e-06 \tLoss(validation) = 1.2517962771772681e-05\n",
      "48200 3.0756484065394095e-06 \tLoss(validation) = 1.2957817684708634e-05\n",
      "48300 5.317371536419189e-06 \tLoss(validation) = 3.3621368527281115e-05\n",
      "48400 3.060442012515958e-06 \tLoss(validation) = 1.2910554387141808e-05\n",
      "48500 3.0391837064074762e-06 \tLoss(validation) = 1.2943794200623269e-05\n",
      "48600 3.123790518438949e-06 \tLoss(validation) = 1.304709929413062e-05\n",
      "48700 3.0190866651375576e-06 \tLoss(validation) = 1.2820267201883516e-05\n",
      "48800 2.2266236750351908e-05 \tLoss(validation) = 2.270551903696836e-05\n",
      "48900 3.0080249675617657e-06 \tLoss(validation) = 1.2763609838812508e-05\n",
      "49000 2.983461760614146e-06 \tLoss(validation) = 1.2710802853669863e-05\n",
      "49100 3.6922980336245235e-06 \tLoss(validation) = 1.4738152198729216e-05\n",
      "49200 2.97061969267275e-06 \tLoss(validation) = 1.2708237502051926e-05\n",
      "49300 3.7764026524117095e-06 \tLoss(validation) = 1.5422381145653188e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49400 2.9702432566453855e-06 \tLoss(validation) = 1.2594410464550915e-05\n",
      "49500 2.9355363755715584e-06 \tLoss(validation) = 1.2603284461323757e-05\n",
      "49600 7.786834371528142e-06 \tLoss(validation) = 2.8317909931203753e-05\n",
      "49700 2.9248262092750155e-06 \tLoss(validation) = 1.2549151099197439e-05\n",
      "49800 2.9003424341508375e-06 \tLoss(validation) = 1.2496269498709218e-05\n",
      "49900 3.557159763678675e-06 \tLoss(validation) = 1.2142752864744036e-05\n",
      "50000 2.8879271319527344e-06 \tLoss(validation) = 1.2485859140962342e-05\n",
      "50100 1.1836370868167677e-05 \tLoss(validation) = 2.2450690362659106e-05\n",
      "50200 2.878562172424063e-06 \tLoss(validation) = 1.2440913367661006e-05\n",
      "50300 2.8532524244693237e-06 \tLoss(validation) = 1.2388456926326649e-05\n",
      "50400 5.811034886653099e-06 \tLoss(validation) = 1.4594539693130578e-05\n",
      "50500 2.8401084288822397e-06 \tLoss(validation) = 1.2366126220486123e-05\n",
      "50600 2.8207084196767095e-06 \tLoss(validation) = 1.2356192014063556e-05\n",
      "50700 2.9319527631251236e-06 \tLoss(validation) = 1.2606013085922333e-05\n",
      "50800 2.8089572122979353e-06 \tLoss(validation) = 1.2279356992601432e-05\n",
      "50900 4.2370684914510865e-05 \tLoss(validation) = 8.233678952253404e-05\n",
      "51000 2.807126018626352e-06 \tLoss(validation) = 1.2322572864673868e-05\n",
      "51100 2.7738336672564984e-06 \tLoss(validation) = 1.2148972077908127e-05\n",
      "51200 2.8059847502262097e-06 \tLoss(validation) = 1.199675320666407e-05\n",
      "51300 2.7592294964761486e-06 \tLoss(validation) = 1.2137488062759924e-05\n",
      "51400 6.220837105637116e-05 \tLoss(validation) = 5.1028877598078504e-05\n",
      "51500 2.7478673669384074e-06 \tLoss(validation) = 1.2141099353592068e-05\n",
      "51600 2.7254195683142387e-06 \tLoss(validation) = 1.2030299573432247e-05\n",
      "51700 2.882243998984923e-06 \tLoss(validation) = 1.2875643490181502e-05\n",
      "51800 2.7123535838003495e-06 \tLoss(validation) = 1.2016273419639284e-05\n",
      "51900 6.434882949374385e-06 \tLoss(validation) = 1.2261497711715727e-05\n",
      "52000 3.504906632135578e-06 \tLoss(validation) = 1.3257421864772963e-05\n",
      "52100 1.4511162879785906e-05 \tLoss(validation) = 1.4011554795508906e-05\n",
      "52200 2.681939975320467e-06 \tLoss(validation) = 1.1872068524512617e-05\n",
      "52300 2.689047486103797e-06 \tLoss(validation) = 1.1786302250843091e-05\n",
      "52400 2.7787202748891693e-06 \tLoss(validation) = 1.2098776387522464e-05\n",
      "52500 2.647285178246887e-06 \tLoss(validation) = 1.1817520506726464e-05\n",
      "52600 2.876817840961095e-06 \tLoss(validation) = 1.1936603089121586e-05\n",
      "52700 2.6296042882736874e-06 \tLoss(validation) = 1.1784504904801485e-05\n",
      "52800 1.649403136852439e-05 \tLoss(validation) = 2.366643714668869e-05\n",
      "52900 2.6185528696660976e-06 \tLoss(validation) = 1.1736438421699397e-05\n",
      "53000 2.5992602771816657e-06 \tLoss(validation) = 1.1717220785976624e-05\n",
      "53100 2.6272047900871002e-06 \tLoss(validation) = 1.2203380474459232e-05\n",
      "53200 2.5873346081100105e-06 \tLoss(validation) = 1.1672684357037078e-05\n",
      "53300 3.680627397163201e-06 \tLoss(validation) = 2.2993627290024827e-05\n",
      "53400 2.574550756289127e-06 \tLoss(validation) = 1.161909235274395e-05\n",
      "53500 2.6288325106574544e-06 \tLoss(validation) = 1.2054450527834544e-05\n",
      "53600 2.5813785978208615e-06 \tLoss(validation) = 1.1658896109349302e-05\n",
      "53700 2.5442472698508467e-06 \tLoss(validation) = 1.1545199921650627e-05\n",
      "53800 5.597959615131107e-06 \tLoss(validation) = 1.1195536741254158e-05\n",
      "53900 2.5332999108460274e-06 \tLoss(validation) = 1.1502469397455185e-05\n",
      "54000 2.5144104389488166e-06 \tLoss(validation) = 1.1436260008853776e-05\n",
      "54100 2.623169704865868e-06 \tLoss(validation) = 1.2200985399558023e-05\n",
      "54200 2.508584590059027e-06 \tLoss(validation) = 1.1458968865697317e-05\n",
      "54300 2.5169336500208263e-06 \tLoss(validation) = 1.1196360598891527e-05\n",
      "54400 2.5347663121783975e-06 \tLoss(validation) = 1.1724548991940632e-05\n",
      "54500 2.479842547420113e-06 \tLoss(validation) = 1.1364698677048652e-05\n",
      "54600 3.785398264590946e-06 \tLoss(validation) = 1.116905704796095e-05\n",
      "54700 2.4670729145390584e-06 \tLoss(validation) = 1.1332888707647308e-05\n",
      "54800 9.491807386916165e-06 \tLoss(validation) = 1.537427333036541e-05\n",
      "54900 3.3172213437672024e-06 \tLoss(validation) = 1.0995461451262129e-05\n",
      "55000 2.65391014568303e-06 \tLoss(validation) = 1.0886915187569043e-05\n",
      "55100 2.678356102991005e-06 \tLoss(validation) = 1.115412014276756e-05\n",
      "55200 0.00011781179432070032 \tLoss(validation) = 9.624513623406882e-05\n",
      "55300 2.422052077262809e-06 \tLoss(validation) = 1.1164931219831703e-05\n",
      "55400 3.4403798242769804e-06 \tLoss(validation) = 1.4401200352033304e-05\n",
      "55500 2.4016878344655635e-06 \tLoss(validation) = 1.1136075176294099e-05\n",
      "55600 2.658516850866835e-06 \tLoss(validation) = 1.3410129288262025e-05\n",
      "55700 2.3883573944024056e-06 \tLoss(validation) = 1.1097733752489245e-05\n",
      "55800 8.815956025624548e-05 \tLoss(validation) = 0.00016138079472317822\n",
      "55900 2.3858399023088598e-06 \tLoss(validation) = 1.1149273631641527e-05\n",
      "56000 2.3619573797311894e-06 \tLoss(validation) = 1.1006395096858048e-05\n",
      "56100 5.742682840933438e-06 \tLoss(validation) = 2.3217090093722208e-05\n",
      "56200 2.360307058434718e-06 \tLoss(validation) = 1.1012106931120597e-05\n",
      "56300 2.3427447595992947e-06 \tLoss(validation) = 1.0933015276915131e-05\n",
      "56400 5.016509510908022e-06 \tLoss(validation) = 1.8219145093830247e-05\n",
      "56500 2.333270907851782e-06 \tLoss(validation) = 1.0916684278941478e-05\n",
      "56600 2.5700390551186446e-06 \tLoss(validation) = 1.2002037279178726e-05\n",
      "56700 2.338340962618168e-06 \tLoss(validation) = 1.0911450678253588e-05\n",
      "56800 2.3061717697336894e-06 \tLoss(validation) = 1.0856855943808126e-05\n",
      "56900 5.98880167847173e-06 \tLoss(validation) = 1.0774743157252927e-05\n",
      "57000 2.2956600565796906e-06 \tLoss(validation) = 1.0813544881694697e-05\n",
      "57100 0.00014014549009182497 \tLoss(validation) = 6.788242453822677e-05\n",
      "57200 2.286641735660423e-06 \tLoss(validation) = 1.0811203233426658e-05\n",
      "57300 2.2703283916311847e-06 \tLoss(validation) = 1.0680533072129276e-05\n",
      "57400 2.3908448124906747e-06 \tLoss(validation) = 1.0585146622120657e-05\n",
      "57500 2.2571226826365294e-06 \tLoss(validation) = 1.0700191204415459e-05\n",
      "57600 2.7298181449942103e-06 \tLoss(validation) = 1.4718732646931584e-05\n",
      "57700 2.2500406062672903e-06 \tLoss(validation) = 1.0679916540363615e-05\n",
      "57800 5.046232233194383e-06 \tLoss(validation) = 1.0755988673526707e-05\n",
      "57900 2.3636110305035417e-06 \tLoss(validation) = 1.0403867733420472e-05\n",
      "58000 2.5337119347600797e-05 \tLoss(validation) = 5.4774680432909704e-05\n",
      "58100 2.2219735951677414e-06 \tLoss(validation) = 1.0648218893508508e-05\n",
      "58200 4.2944382114937064e-05 \tLoss(validation) = 8.703253022795669e-05\n",
      "58300 2.2179620558374304e-06 \tLoss(validation) = 1.069919025514345e-05\n",
      "58400 2.1972797324368385e-06 \tLoss(validation) = 1.0598464345885706e-05\n",
      "58500 2.215325390456497e-06 \tLoss(validation) = 1.0266997963219322e-05\n",
      "58600 2.183065071700978e-06 \tLoss(validation) = 1.047507721984991e-05\n",
      "58700 1.4562780679641104e-05 \tLoss(validation) = 3.623434684518809e-05\n",
      "58800 2.1773947233734756e-06 \tLoss(validation) = 1.0468854030646013e-05\n",
      "58900 2.1613199013224316e-06 \tLoss(validation) = 1.0420907558014046e-05\n",
      "59000 5.570680538832824e-06 \tLoss(validation) = 1.4929324922134771e-05\n",
      "59100 2.155468376831438e-06 \tLoss(validation) = 1.0398830001893391e-05\n",
      "59200 2.8603661968458e-06 \tLoss(validation) = 1.2643720867173099e-05\n",
      "59300 2.1510168272652805e-06 \tLoss(validation) = 1.0392417621664143e-05\n",
      "59400 2.1301040589174657e-06 \tLoss(validation) = 1.0321822120465084e-05\n",
      "59500 3.401292214753326e-06 \tLoss(validation) = 1.034984593192737e-05\n",
      "59600 2.122055929001652e-06 \tLoss(validation) = 1.0311675640572805e-05\n",
      "59700 5.226955039941274e-06 \tLoss(validation) = 1.1650203172950076e-05\n",
      "59800 2.4309419641313334e-06 \tLoss(validation) = 1.0037071569151698e-05\n",
      "59900 4.5438637027525024e-05 \tLoss(validation) = 8.467093047133431e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlnics.Losses.PINN_Loss at 0x12ff53fd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = Training.plot_loss(pinn_trainer, pinn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### 4.5.3 Train PDNN\n",
    "\n",
    "Given a training set $X_{PDNN} = ((\\boldsymbol{\\mu}^{(1)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(1)})), \\dots, (\\boldsymbol{\\mu}^{(n)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(n)})))$ of parameter and high fidelity solution pairs for the PDE, we train a Projection-Driven Neural Network (PDNN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "$$L_{PDNN}(X_{PDNN}; W) = \\frac1n \\sum_{i=1}^n \\|\\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu}^{(i)})\\|_2^2,$$\n",
    "where for a given $\\boldsymbol{\\mu}$, $\\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu})$ is the projection of $\\operatorname{HF}(\\boldsymbol{\\mu})$ onto the reduced order solution space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pdnn = Normalization.MinMaxNormalization(input_normalization=True)\n",
    "output_normalization_pdnn = Normalization.MinMaxNormalization()\n",
    "\n",
    "pdnn_net  = NN.RONN(\"PDNN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pdnn_loss = Losses.PDNN_Loss(pdnn_net, output_normalization_pdnn)\n",
    "data      = RONNData.RONNDataLoader(pdnn_net, validation_proportion=0.2)\n",
    "optimizer = torch.optim.Adam(pdnn_net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.99999)\n",
    "\n",
    "pdnn_trainer = Training.PDNNTrainer(\n",
    "    pdnn_net, data, pdnn_loss, optimizer, scheduler,\n",
    "    input_normalization_pdnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pdnn_net, data, pdnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pdnn_trainer, pdnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Train PRNN\n",
    "\n",
    "We train a Physics-Reinforced Neural Network (PRNN) $N_W(\\boldsymbol{\\mu})$ dependnent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PRNN}(X_{PINN}, X_{PDNN}; W) = L_{PINN}(X_{PINN}; W) + \\omega L_{PDNN}(X_{PDNN}; W),$$\n",
    "\n",
    "where $\\omega$ is a scaling parameter which can be chosen freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_prnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_prnn = Normalization.StandardNormalization()\n",
    "\n",
    "omega = 1.\n",
    "prnn_net  = NN.RONN(f\"PRNN_{omega}\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "prnn_loss = Losses.PRNN_Loss(prnn_net, output_normalization_prnn, omega=omega)\n",
    "data      = RONNData.RONNDataLoader(prnn_net, validation_proportion=0.2,\n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(prnn_net.parameters(), lr=0.001)\n",
    "\n",
    "prnn_trainer = Training.PRNNTrainer(\n",
    "    prnn_net, data, prnn_loss, optimizer,\n",
    "    input_normalization_prnn, num_epochs=1000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    prnn_net, data, prnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(prnn_trainer, prnn_net, separate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Perform an error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 Reduction Method Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduction_method.initialize_testing_set(100, DEIM=60)\n",
    "#reduction_method.error_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 PINN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mu = torch.tensor(reduction_method.testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  |F(x)| = 0.394212; step 1\n",
      "1:  |F(x)| = 0.0190567; step 1\n",
      "2:  |F(x)| = 0.000880559; step 1\n",
      "3:  |F(x)| = 5.2079e-05; step 1\n",
      "4:  |F(x)| = 3.13153e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.70135; step 0.322901\n",
      "1:  |F(x)| = 2.21561; step 0.370285\n",
      "2:  |F(x)| = 0.549721; step 1\n",
      "3:  |F(x)| = 0.0609536; step 1\n",
      "4:  |F(x)| = 0.000921454; step 1\n",
      "5:  |F(x)| = 1.19093e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.67303; step 0.314248\n",
      "1:  |F(x)| = 1.54025; step 0.509104\n",
      "2:  |F(x)| = 0.0379531; step 1\n",
      "3:  |F(x)| = 0.000660432; step 1\n",
      "4:  |F(x)| = 6.50664e-06; step 1\n",
      "5:  |F(x)| = 3.21102e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 4.03867; step 0.264622\n",
      "1:  |F(x)| = 1.65166; step 1\n",
      "2:  |F(x)| = 0.296205; step 1\n",
      "3:  |F(x)| = 0.016783; step 1\n",
      "4:  |F(x)| = 0.000333296; step 1\n",
      "5:  |F(x)| = 1.4354e-05; step 1\n",
      "6:  |F(x)| = 5.49098e-07; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 2.41106; step 0.535375\n",
      "1:  |F(x)| = 0.343378; step 1\n",
      "2:  |F(x)| = 0.0224836; step 1\n",
      "3:  |F(x)| = 0.000198396; step 1\n",
      "4:  |F(x)| = 3.4571e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.69973; step 0.311281\n",
      "1:  |F(x)| = 2.54219; step 0.30155\n",
      "2:  |F(x)| = 0.5166; step 1\n",
      "3:  |F(x)| = 0.0455886; step 1\n",
      "4:  |F(x)| = 0.000531772; step 1\n",
      "5:  |F(x)| = 5.04439e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.65506; step 0.51497\n",
      "1:  |F(x)| = 0.840332; step 1\n",
      "2:  |F(x)| = 0.157813; step 1\n",
      "3:  |F(x)| = 0.00796862; step 1\n",
      "4:  |F(x)| = 2.01175e-05; step 1\n",
      "5:  |F(x)| = 2.47299e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.7022; step 0.311989\n",
      "1:  |F(x)| = 2.49023; step 0.315415\n",
      "2:  |F(x)| = 0.472086; step 1\n",
      "3:  |F(x)| = 0.0387334; step 1\n",
      "4:  |F(x)| = 0.000421111; step 1\n",
      "5:  |F(x)| = 4.05376e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.24199; step 0.406934\n",
      "1:  |F(x)| = 0.927809; step 1\n",
      "2:  |F(x)| = 0.140424; step 1\n",
      "3:  |F(x)| = 0.00458498; step 1\n",
      "4:  |F(x)| = 2.44839e-05; step 1\n",
      "5:  |F(x)| = 3.67188e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.687192; step 1\n",
      "1:  |F(x)| = 0.0686505; step 1\n",
      "2:  |F(x)| = 0.0011071; step 1\n",
      "3:  |F(x)| = 1.22499e-05; step 1\n",
      "4:  |F(x)| = 1.83997e-07; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 2.26928; step 0.443911\n",
      "1:  |F(x)| = 0.156514; step 1\n",
      "2:  |F(x)| = 0.00764508; step 1\n",
      "3:  |F(x)| = 0.000290587; step 1\n",
      "4:  |F(x)| = 8.8111e-06; step 1\n",
      "5:  |F(x)| = 3.6274e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 1.06948; step 1\n",
      "1:  |F(x)| = 0.145771; step 1\n",
      "2:  |F(x)| = 0.00455459; step 1\n",
      "3:  |F(x)| = 7.8669e-05; step 1\n",
      "4:  |F(x)| = 2.11335e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 2.30498; step 0.447113\n",
      "1:  |F(x)| = 0.128234; step 1\n",
      "2:  |F(x)| = 0.00488281; step 1\n",
      "3:  |F(x)| = 0.000145314; step 1\n",
      "4:  |F(x)| = 3.92961e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 0.316237; step 1\n",
      "1:  |F(x)| = 0.0108815; step 1\n",
      "2:  |F(x)| = 0.000141761; step 1\n",
      "3:  |F(x)| = 3.07316e-06; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 2.91467; step 0.448756\n",
      "1:  |F(x)| = 0.613088; step 1\n",
      "2:  |F(x)| = 0.0558539; step 1\n",
      "3:  |F(x)| = 0.00111156; step 1\n",
      "4:  |F(x)| = 3.27059e-05; step 1\n",
      "5:  |F(x)| = 1.07533e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.806187; step 1\n",
      "1:  |F(x)| = 0.0746407; step 1\n",
      "2:  |F(x)| = 0.00540705; step 1\n",
      "3:  |F(x)| = 0.000529406; step 1\n",
      "4:  |F(x)| = 5.39888e-05; step 1\n",
      "5:  |F(x)| = 5.48722e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 1.59958; step 1\n",
      "1:  |F(x)| = 0.2725; step 1\n",
      "2:  |F(x)| = 0.0142417; step 1\n",
      "3:  |F(x)| = 0.000368015; step 1\n",
      "4:  |F(x)| = 1.63352e-05; step 1\n",
      "5:  |F(x)| = 6.83188e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.51566; step 0.357926\n",
      "1:  |F(x)| = 1.09023; step 1\n",
      "2:  |F(x)| = 0.170077; step 1\n",
      "3:  |F(x)| = 0.00634166; step 1\n",
      "4:  |F(x)| = 6.21158e-05; step 1\n",
      "5:  |F(x)| = 1.42504e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 1.73574; step 1\n",
      "1:  |F(x)| = 0.325815; step 1\n",
      "2:  |F(x)| = 0.0193616; step 1\n",
      "3:  |F(x)| = 0.000338262; step 1\n",
      "4:  |F(x)| = 1.22854e-05; step 1\n",
      "5:  |F(x)| = 4.00436e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.70017; step 0.31126\n",
      "1:  |F(x)| = 2.51032; step 0.309776\n",
      "2:  |F(x)| = 0.485373; step 1\n",
      "3:  |F(x)| = 0.0407879; step 1\n",
      "4:  |F(x)| = 0.00044597; step 1\n",
      "5:  |F(x)| = 4.51055e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 1.52975; step 1\n",
      "1:  |F(x)| = 0.238225; step 1\n",
      "2:  |F(x)| = 0.0124398; step 1\n",
      "3:  |F(x)| = 0.000538247; step 1\n",
      "4:  |F(x)| = 3.29589e-05; step 1\n",
      "5:  |F(x)| = 1.96005e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.70252; step 0.312644\n",
      "1:  |F(x)| = 1.40212; step 0.546132\n",
      "2:  |F(x)| = 0.0239868; step 1\n",
      "3:  |F(x)| = 0.000461723; step 1\n",
      "4:  |F(x)| = 5.64322e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 2.79426; step 0.481912\n",
      "1:  |F(x)| = 0.424397; step 1\n",
      "2:  |F(x)| = 0.0287778; step 1\n",
      "3:  |F(x)| = 0.000837108; step 1\n",
      "4:  |F(x)| = 4.71607e-05; step 1\n",
      "5:  |F(x)| = 2.43147e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.70315; step 0.312028\n",
      "1:  |F(x)| = 2.69048; step 0.264508\n",
      "2:  |F(x)| = 0.697278; step 1\n",
      "3:  |F(x)| = 0.0773257; step 1\n",
      "4:  |F(x)| = 0.00134566; step 1\n",
      "5:  |F(x)| = 6.80406e-06; step 1\n",
      "6:  |F(x)| = 8.25255e-08; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 3.72432; step 0.316827\n",
      "1:  |F(x)| = 2.28859; step 0.352134\n",
      "2:  |F(x)| = 0.555933; step 1\n",
      "3:  |F(x)| = 0.0600523; step 1\n",
      "4:  |F(x)| = 0.000844335; step 1\n",
      "5:  |F(x)| = 2.99186e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 4.67222; step 0.151066\n",
      "1:  |F(x)| = 3.52605; step 0.242597\n",
      "2:  |F(x)| = 1.62872; step 0.509567\n",
      "3:  |F(x)| = 0.127552; step 1\n",
      "4:  |F(x)| = 0.00388241; step 1\n",
      "5:  |F(x)| = 4.10764e-05; step 1\n",
      "6:  |F(x)| = 9.78125e-07; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 3.19265; step 0.406206\n",
      "1:  |F(x)| = 1.21573; step 1\n",
      "2:  |F(x)| = 0.201068; step 1\n",
      "3:  |F(x)| = 0.00857703; step 1\n",
      "4:  |F(x)| = 4.99541e-05; step 1\n",
      "5:  |F(x)| = 1.29416e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.71653; step 0.4828\n",
      "1:  |F(x)| = 0.488855; step 1\n",
      "2:  |F(x)| = 0.0395256; step 1\n",
      "3:  |F(x)| = 0.000580765; step 1\n",
      "4:  |F(x)| = 1.37775e-05; step 1\n",
      "5:  |F(x)| = 3.61329e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.6468; step 0.325052\n",
      "1:  |F(x)| = 2.19782; step 0.380429\n",
      "2:  |F(x)| = 0.29721; step 1\n",
      "3:  |F(x)| = 0.0171291; step 1\n",
      "4:  |F(x)| = 0.000145932; step 1\n",
      "5:  |F(x)| = 3.09785e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.26448; step 0.401657\n",
      "1:  |F(x)| = 0.802523; step 1\n",
      "2:  |F(x)| = 0.0965464; step 1\n",
      "3:  |F(x)| = 0.00238464; step 1\n",
      "4:  |F(x)| = 4.05354e-05; step 1\n",
      "5:  |F(x)| = 1.08866e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.90642; step 0.462631\n",
      "1:  |F(x)| = 0.484039; step 1\n",
      "2:  |F(x)| = 0.0359; step 1\n",
      "3:  |F(x)| = 0.00103329; step 1\n",
      "4:  |F(x)| = 5.67703e-05; step 1\n",
      "5:  |F(x)| = 2.92921e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.0599785; step 1\n",
      "1:  |F(x)| = 0.00136776; step 1\n",
      "2:  |F(x)| = 5.9676e-05; step 1\n",
      "3:  |F(x)| = 2.69308e-06; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 2.30604; step 0.444242\n",
      "1:  |F(x)| = 0.148364; step 1\n",
      "2:  |F(x)| = 0.00687532; step 1\n",
      "3:  |F(x)| = 0.000242245; step 1\n",
      "4:  |F(x)| = 7.05078e-06; step 1\n",
      "5:  |F(x)| = 2.95427e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.492184; step 1\n",
      "1:  |F(x)| = 0.0332702; step 1\n",
      "2:  |F(x)| = 0.000341934; step 1\n",
      "3:  |F(x)| = 4.51299e-06; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 0.107175; step 1\n",
      "1:  |F(x)| = 0.00203669; step 1\n",
      "2:  |F(x)| = 6.5317e-05; step 1\n",
      "3:  |F(x)| = 2.23294e-06; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 0.19858; step 1\n",
      "1:  |F(x)| = 0.014002; step 1\n",
      "2:  |F(x)| = 0.00193695; step 1\n",
      "3:  |F(x)| = 0.00028212; step 1\n",
      "4:  |F(x)| = 4.12738e-05; step 1\n",
      "5:  |F(x)| = 6.04135e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.373927; step 1\n",
      "1:  |F(x)| = 0.0284063; step 1\n",
      "2:  |F(x)| = 0.00364454; step 1\n",
      "3:  |F(x)| = 0.000513669; step 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4:  |F(x)| = 7.28482e-05; step 1\n",
      "5:  |F(x)| = 1.034e-05; step 1\n",
      "6:  |F(x)| = 1.46783e-06; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 3.4657; step 0.357005\n",
      "1:  |F(x)| = 1.50591; step 1\n",
      "2:  |F(x)| = 0.259047; step 1\n",
      "3:  |F(x)| = 0.0130297; step 1\n",
      "4:  |F(x)| = 0.000130834; step 1\n",
      "5:  |F(x)| = 4.39495e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 1.35616; step 1\n",
      "1:  |F(x)| = 0.216832; step 1\n",
      "2:  |F(x)| = 0.0093226; step 1\n",
      "3:  |F(x)| = 0.000174528; step 1\n",
      "4:  |F(x)| = 5.69636e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 1.89058; step 1\n",
      "1:  |F(x)| = 0.343235; step 1\n",
      "2:  |F(x)| = 0.0217073; step 1\n",
      "3:  |F(x)| = 0.000751743; step 1\n",
      "4:  |F(x)| = 4.41492e-05; step 1\n",
      "5:  |F(x)| = 2.45963e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.98079; step 0.439385\n",
      "1:  |F(x)| = 0.740162; step 1\n",
      "2:  |F(x)| = 0.081946; step 1\n",
      "3:  |F(x)| = 0.00176541; step 1\n",
      "4:  |F(x)| = 2.94179e-05; step 1\n",
      "5:  |F(x)| = 7.97182e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.66219; step 0.314577\n",
      "1:  |F(x)| = 1.47892; step 0.521328\n",
      "2:  |F(x)| = 0.0282744; step 1\n",
      "3:  |F(x)| = 0.000496838; step 1\n",
      "4:  |F(x)| = 4.68371e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.6881; step 0.313008\n",
      "1:  |F(x)| = 1.42952; step 0.537049\n",
      "2:  |F(x)| = 0.0247616; step 1\n",
      "3:  |F(x)| = 0.000466795; step 1\n",
      "4:  |F(x)| = 5.07321e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 0.113002; step 1\n",
      "1:  |F(x)| = 0.00682016; step 1\n",
      "2:  |F(x)| = 0.000850342; step 1\n",
      "3:  |F(x)| = 0.000110422; step 1\n",
      "4:  |F(x)| = 1.42956e-05; step 1\n",
      "5:  |F(x)| = 1.85022e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.922267; step 1\n",
      "1:  |F(x)| = 0.0929734; step 1\n",
      "2:  |F(x)| = 0.00506442; step 1\n",
      "3:  |F(x)| = 0.000391902; step 1\n",
      "4:  |F(x)| = 3.21878e-05; step 1\n",
      "5:  |F(x)| = 2.62825e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.149568; step 1\n",
      "1:  |F(x)| = 0.0104953; step 1\n",
      "2:  |F(x)| = 0.00151077; step 1\n",
      "3:  |F(x)| = 0.000228859; step 1\n",
      "4:  |F(x)| = 3.45464e-05; step 1\n",
      "5:  |F(x)| = 5.2128e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.158743; step 1\n",
      "1:  |F(x)| = 0.00527434; step 1\n",
      "2:  |F(x)| = 0.000300447; step 1\n",
      "3:  |F(x)| = 1.82729e-05; step 1\n",
      "4:  |F(x)| = 1.11592e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.6981; step 0.313331\n",
      "1:  |F(x)| = 1.586; step 0.503288\n",
      "2:  |F(x)| = 0.0495035; step 1\n",
      "3:  |F(x)| = 0.000901918; step 1\n",
      "4:  |F(x)| = 5.76677e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 0.242295; step 1\n",
      "1:  |F(x)| = 0.00787667; step 1\n",
      "2:  |F(x)| = 0.000327414; step 1\n",
      "3:  |F(x)| = 1.59073e-05; step 1\n",
      "4:  |F(x)| = 7.78052e-07; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.71304; step 0.314436\n",
      "1:  |F(x)| = 1.87852; step 0.441005\n",
      "2:  |F(x)| = 0.149246; step 1\n",
      "3:  |F(x)| = 0.00520094; step 1\n",
      "4:  |F(x)| = 2.42697e-05; step 1\n",
      "5:  |F(x)| = 6.07281e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.238107; step 1\n",
      "1:  |F(x)| = 0.00576858; step 1\n",
      "2:  |F(x)| = 8.80339e-05; step 1\n",
      "3:  |F(x)| = 1.94524e-06; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 3.01687; step 0.444294\n",
      "1:  |F(x)| = 2.09875; step 0.293876\n",
      "2:  |F(x)| = 0.577531; step 1\n",
      "3:  |F(x)| = 0.070736; step 1\n",
      "4:  |F(x)| = 0.00136225; step 1\n",
      "5:  |F(x)| = 8.32567e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 1.12357; step 1\n",
      "1:  |F(x)| = 0.149443; step 1\n",
      "2:  |F(x)| = 0.00511426; step 1\n",
      "3:  |F(x)| = 0.000134675; step 1\n",
      "4:  |F(x)| = 4.99238e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 2.13896; step 1\n",
      "1:  |F(x)| = 0.42545; step 1\n",
      "2:  |F(x)| = 0.0308465; step 1\n",
      "3:  |F(x)| = 0.000842443; step 1\n",
      "4:  |F(x)| = 4.56316e-05; step 1\n",
      "5:  |F(x)| = 2.26734e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.67903; step 0.31213\n",
      "1:  |F(x)| = 2.70526; step 0.255438\n",
      "2:  |F(x)| = 0.710506; step 1\n",
      "3:  |F(x)| = 0.0805953; step 1\n",
      "4:  |F(x)| = 0.00139456; step 1\n",
      "5:  |F(x)| = 5.56569e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.68539; step 0.31679\n",
      "1:  |F(x)| = 2.163; step 0.369189\n",
      "2:  |F(x)| = 0.315602; step 1\n",
      "3:  |F(x)| = 0.0206657; step 1\n",
      "4:  |F(x)| = 0.000135204; step 1\n",
      "5:  |F(x)| = 3.72684e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.450367; step 1\n",
      "1:  |F(x)| = 0.0248578; step 1\n",
      "2:  |F(x)| = 0.000307832; step 1\n",
      "3:  |F(x)| = 6.32454e-06; step 1\n",
      "4:  |F(x)| = 1.27215e-07; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 0.762725; step 1\n",
      "1:  |F(x)| = 0.0808944; step 1\n",
      "2:  |F(x)| = 0.00154523; step 1\n",
      "3:  |F(x)| = 2.12906e-05; step 1\n",
      "4:  |F(x)| = 3.99076e-07; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 2.77132; step 0.475467\n",
      "1:  |F(x)| = 0.587397; step 1\n",
      "2:  |F(x)| = 0.0573797; step 1\n",
      "3:  |F(x)| = 0.000922802; step 1\n",
      "4:  |F(x)| = 1.30142e-05; step 1\n",
      "5:  |F(x)| = 2.83831e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.6259; step 0.313228\n",
      "1:  |F(x)| = 2.24615; step 0.361338\n",
      "2:  |F(x)| = 0.278212; step 1\n",
      "3:  |F(x)| = 0.0158171; step 1\n",
      "4:  |F(x)| = 7.46088e-05; step 1\n",
      "5:  |F(x)| = 2.67813e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.1731; step 0.412939\n",
      "1:  |F(x)| = 1.73853; step 0.42792\n",
      "2:  |F(x)| = 0.198468; step 1\n",
      "3:  |F(x)| = 0.00902263; step 1\n",
      "4:  |F(x)| = 2.63484e-05; step 1\n",
      "5:  |F(x)| = 3.91869e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 1.3031; step 1\n",
      "1:  |F(x)| = 0.248157; step 1\n",
      "2:  |F(x)| = 0.0133215; step 1\n",
      "3:  |F(x)| = 7.10341e-05; step 1\n",
      "4:  |F(x)| = 7.10121e-07; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 0.993478; step 1\n",
      "1:  |F(x)| = 0.122255; step 1\n",
      "2:  |F(x)| = 0.00352719; step 1\n",
      "3:  |F(x)| = 8.06439e-05; step 1\n",
      "4:  |F(x)| = 2.53052e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.70727; step 0.315407\n",
      "1:  |F(x)| = 1.46791; step 0.533926\n",
      "2:  |F(x)| = 0.0366274; step 1\n",
      "3:  |F(x)| = 0.000607335; step 1\n",
      "4:  |F(x)| = 5.49597e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.71866; step 0.320578\n",
      "1:  |F(x)| = 2.37133; step 0.336608\n",
      "2:  |F(x)| = 0.884153; step 1\n",
      "3:  |F(x)| = 0.139165; step 1\n",
      "4:  |F(x)| = 0.00495852; step 1\n",
      "5:  |F(x)| = 1.25189e-05; step 1\n",
      "6:  |F(x)| = 9.90523e-08; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 0.643464; step 1\n",
      "1:  |F(x)| = 0.0496936; step 1\n",
      "2:  |F(x)| = 0.00331857; step 1\n",
      "3:  |F(x)| = 0.00029282; step 1\n",
      "4:  |F(x)| = 2.65507e-05; step 1\n",
      "5:  |F(x)| = 2.40653e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.09802; step 0.418326\n",
      "1:  |F(x)| = 0.802369; step 1\n",
      "2:  |F(x)| = 0.088489; step 1\n",
      "3:  |F(x)| = 0.00217838; step 1\n",
      "4:  |F(x)| = 5.31519e-05; step 1\n",
      "5:  |F(x)| = 1.79057e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.954647; step 1\n",
      "1:  |F(x)| = 0.100093; step 1\n",
      "2:  |F(x)| = 0.00727553; step 1\n",
      "3:  |F(x)| = 0.000729251; step 1\n",
      "4:  |F(x)| = 7.70242e-05; step 1\n",
      "5:  |F(x)| = 8.09964e-06; step 1\n",
      "6:  |F(x)| = 8.51117e-07; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 1.21889; step 1\n",
      "1:  |F(x)| = 0.164793; step 1\n",
      "2:  |F(x)| = 0.00660449; step 1\n",
      "3:  |F(x)| = 0.000243674; step 1\n",
      "4:  |F(x)| = 1.19673e-05; step 1\n",
      "5:  |F(x)| = 5.72454e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.12624; step 0.419312\n",
      "1:  |F(x)| = 1.28621; step 1\n",
      "2:  |F(x)| = 0.230246; step 1\n",
      "3:  |F(x)| = 0.011455; step 1\n",
      "4:  |F(x)| = 5.18791e-05; step 1\n",
      "5:  |F(x)| = 9.69034e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.6721; step 0.311265\n",
      "1:  |F(x)| = 2.41697; step 0.327497\n",
      "2:  |F(x)| = 0.395262; step 1\n",
      "3:  |F(x)| = 0.028816; step 1\n",
      "4:  |F(x)| = 0.000228873; step 1\n",
      "5:  |F(x)| = 3.10184e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.37164; step 0.373784\n",
      "1:  |F(x)| = 1.36088; step 1\n",
      "2:  |F(x)| = 0.225949; step 1\n",
      "3:  |F(x)| = 0.0102895; step 1\n",
      "4:  |F(x)| = 9.5484e-05; step 1\n",
      "5:  |F(x)| = 2.95988e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.95483; step 0.441936\n",
      "1:  |F(x)| = 0.644733; step 1\n",
      "2:  |F(x)| = 0.0604671; step 1\n",
      "3:  |F(x)| = 0.00127013; step 1\n",
      "4:  |F(x)| = 3.81135e-05; step 1\n",
      "5:  |F(x)| = 1.29376e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.6591; step 0.311946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  |F(x)| = 2.41403; step 0.325485\n",
      "2:  |F(x)| = 0.393784; step 1\n",
      "3:  |F(x)| = 0.0287992; step 1\n",
      "4:  |F(x)| = 0.000216782; step 1\n",
      "5:  |F(x)| = 2.85821e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.70587; step 0.319778\n",
      "1:  |F(x)| = 1.97167; step 0.423683\n",
      "2:  |F(x)| = 0.240925; step 1\n",
      "3:  |F(x)| = 0.0129015; step 1\n",
      "4:  |F(x)| = 4.14393e-05; step 1\n",
      "5:  |F(x)| = 5.71371e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.0805712; step 1\n",
      "1:  |F(x)| = 0.000435473; step 1\n",
      "2:  |F(x)| = 9.89234e-07; step 1\n",
      "scipy solver converged in 4 iterations.\n",
      "0:  |F(x)| = 4.207; step 0.235049\n",
      "1:  |F(x)| = 2.62447; step 0.366722\n",
      "2:  |F(x)| = 0.640869; step 1\n",
      "3:  |F(x)| = 0.0735278; step 1\n",
      "4:  |F(x)| = 0.00137461; step 1\n",
      "5:  |F(x)| = 9.8221e-06; step 1\n",
      "6:  |F(x)| = 1.52031e-07; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 0.303829; step 1\n",
      "1:  |F(x)| = 0.0189419; step 1\n",
      "2:  |F(x)| = 0.00198217; step 1\n",
      "3:  |F(x)| = 0.000226294; step 1\n",
      "4:  |F(x)| = 2.59794e-05; step 1\n",
      "5:  |F(x)| = 2.9847e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.63745; step 0.31525\n",
      "1:  |F(x)| = 1.31354; step 0.55572\n",
      "2:  |F(x)| = 0.0154292; step 1\n",
      "3:  |F(x)| = 0.000294438; step 1\n",
      "4:  |F(x)| = 2.98516e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.53547; step 0.350406\n",
      "1:  |F(x)| = 2.84731; step 0.190101\n",
      "2:  |F(x)| = 1.40893; step 0.472051\n",
      "3:  |F(x)| = 0.105936; step 1\n",
      "4:  |F(x)| = 0.00277914; step 1\n",
      "5:  |F(x)| = 2.95888e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.0272009; step 1\n",
      "1:  |F(x)| = 0.00129934; step 1\n",
      "2:  |F(x)| = 0.000152531; step 1\n",
      "3:  |F(x)| = 1.77572e-05; step 1\n",
      "4:  |F(x)| = 2.06357e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.69832; step 0.317718\n",
      "1:  |F(x)| = 1.40627; step 0.547805\n",
      "2:  |F(x)| = 0.0302271; step 1\n",
      "3:  |F(x)| = 0.000485293; step 1\n",
      "4:  |F(x)| = 4.99724e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.6936; step 0.326162\n",
      "1:  |F(x)| = 2.35547; step 0.339968\n",
      "2:  |F(x)| = 1.0353; step 0.51967\n",
      "3:  |F(x)| = 0.0443771; step 1\n",
      "4:  |F(x)| = 0.000539006; step 1\n",
      "5:  |F(x)| = 3.51573e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.068411; step 1\n",
      "1:  |F(x)| = 0.00321497; step 1\n",
      "2:  |F(x)| = 0.000318778; step 1\n",
      "3:  |F(x)| = 3.23844e-05; step 1\n",
      "4:  |F(x)| = 3.28216e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.69062; step 0.316426\n",
      "1:  |F(x)| = 2.15454; step 0.372363\n",
      "2:  |F(x)| = 0.311491; step 1\n",
      "3:  |F(x)| = 0.020198; step 1\n",
      "4:  |F(x)| = 0.000122518; step 1\n",
      "5:  |F(x)| = 3.30214e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.280053; step 1\n",
      "1:  |F(x)| = 0.0147472; step 1\n",
      "2:  |F(x)| = 0.00124891; step 1\n",
      "3:  |F(x)| = 0.000116255; step 1\n",
      "4:  |F(x)| = 1.08818e-05; step 1\n",
      "5:  |F(x)| = 1.01929e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.56008; step 0.350201\n",
      "1:  |F(x)| = 1.15959; step 1\n",
      "2:  |F(x)| = 0.191158; step 1\n",
      "3:  |F(x)| = 0.00790269; step 1\n",
      "4:  |F(x)| = 6.57855e-05; step 1\n",
      "5:  |F(x)| = 1.43496e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.23452; step 1\n",
      "1:  |F(x)| = 0.008005; step 1\n",
      "2:  |F(x)| = 1.76422e-05; step 1\n",
      "3:  |F(x)| = 5.98426e-08; step 1\n",
      "scipy solver converged in 5 iterations.\n",
      "0:  |F(x)| = 0.794617; step 1\n",
      "1:  |F(x)| = 0.0716055; step 1\n",
      "2:  |F(x)| = 0.00248964; step 1\n",
      "3:  |F(x)| = 0.000125572; step 1\n",
      "4:  |F(x)| = 6.74639e-06; step 1\n",
      "5:  |F(x)| = 3.59197e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.70683; step 0.312952\n",
      "1:  |F(x)| = 1.36832; step 0.555054\n",
      "2:  |F(x)| = 0.0220522; step 1\n",
      "3:  |F(x)| = 0.00043703; step 1\n",
      "4:  |F(x)| = 5.81585e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.20302; step 0.407543\n",
      "1:  |F(x)| = 1.77138; step 0.423277\n",
      "2:  |F(x)| = 0.207585; step 1\n",
      "3:  |F(x)| = 0.00975884; step 1\n",
      "4:  |F(x)| = 3.13229e-05; step 1\n",
      "5:  |F(x)| = 4.4493e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 3.61135; step 0.315127\n",
      "1:  |F(x)| = 2.4763; step 0.299647\n",
      "2:  |F(x)| = 0.459671; step 1\n",
      "3:  |F(x)| = 0.0384433; step 1\n",
      "4:  |F(x)| = 0.000311259; step 1\n",
      "5:  |F(x)| = 8.79807e-06; step 1\n",
      "6:  |F(x)| = 3.82541e-07; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 3.65786; step 0.31255\n",
      "1:  |F(x)| = 2.53827; step 0.293544\n",
      "2:  |F(x)| = 0.509589; step 1\n",
      "3:  |F(x)| = 0.0454301; step 1\n",
      "4:  |F(x)| = 0.000468758; step 1\n",
      "5:  |F(x)| = 4.67394e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.500211; step 1\n",
      "1:  |F(x)| = 0.0296873; step 1\n",
      "2:  |F(x)| = 0.00141019; step 1\n",
      "3:  |F(x)| = 9.05682e-05; step 1\n",
      "4:  |F(x)| = 5.91908e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 0.534604; step 1\n",
      "1:  |F(x)| = 0.0430642; step 1\n",
      "2:  |F(x)| = 0.00475513; step 1\n",
      "3:  |F(x)| = 0.000616134; step 1\n",
      "4:  |F(x)| = 8.05935e-05; step 1\n",
      "5:  |F(x)| = 1.05553e-05; step 1\n",
      "6:  |F(x)| = 1.38271e-06; step 1\n",
      "scipy solver converged in 8 iterations.\n",
      "0:  |F(x)| = 0.140908; step 1\n",
      "1:  |F(x)| = 0.00622155; step 1\n",
      "2:  |F(x)| = 0.000514962; step 1\n",
      "3:  |F(x)| = 4.46762e-05; step 1\n",
      "4:  |F(x)| = 3.89056e-06; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 2.11474; step 0.444912\n",
      "1:  |F(x)| = 0.187233; step 1\n",
      "2:  |F(x)| = 0.00976729; step 1\n",
      "3:  |F(x)| = 0.000429329; step 1\n",
      "4:  |F(x)| = 1.43788e-05; step 1\n",
      "5:  |F(x)| = 6.037e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 2.75533; step 0.488711\n",
      "1:  |F(x)| = 0.407357; step 1\n",
      "2:  |F(x)| = 0.0268612; step 1\n",
      "3:  |F(x)| = 0.000697545; step 1\n",
      "4:  |F(x)| = 3.61373e-05; step 1\n",
      "5:  |F(x)| = 1.70608e-06; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "0:  |F(x)| = 0.543308; step 1\n",
      "1:  |F(x)| = 0.0349973; step 1\n",
      "2:  |F(x)| = 0.000669803; step 1\n",
      "3:  |F(x)| = 2.03647e-05; step 1\n",
      "4:  |F(x)| = 6.24473e-07; step 1\n",
      "scipy solver converged in 6 iterations.\n",
      "0:  |F(x)| = 3.58463; step 0.346725\n",
      "1:  |F(x)| = 1.62444; step 0.523049\n",
      "2:  |F(x)| = 0.160058; step 1\n",
      "3:  |F(x)| = 0.00605666; step 1\n",
      "4:  |F(x)| = 1.70844e-05; step 1\n",
      "5:  |F(x)| = 1.98615e-07; step 1\n",
      "scipy solver converged in 7 iterations.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 9.231745e-03\n",
      "  2 SNES Function norm 1.364570e-06\n",
      "  3 SNES Function norm 7.300742e-14\n",
      "PETSc SNES solver converged in 3 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 1.358880e+00\n",
      "  2 SNES Function norm 1.221297e+00\n",
      "  3 SNES Function norm 1.096961e+00\n",
      "  4 SNES Function norm 8.803224e-01\n",
      "  5 SNES Function norm 7.414654e-01\n",
      "  6 SNES Function norm 1.777721e-01\n",
      "  7 SNES Function norm 1.708416e-02\n",
      "  8 SNES Function norm 1.866143e-04\n",
      "  9 SNES Function norm 2.247364e-08\n",
      "  10 SNES Function norm 4.026121e-15\n",
      "PETSc SNES solver converged in 10 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 1.359420e+00\n",
      "  2 SNES Function norm 1.223018e+00\n",
      "  3 SNES Function norm 1.099886e+00\n",
      "  4 SNES Function norm 9.886593e-01\n",
      "  5 SNES Function norm 8.882236e-01\n",
      "  6 SNES Function norm 7.748572e-01\n",
      "  7 SNES Function norm 5.204680e-01\n",
      "  8 SNES Function norm 1.919210e-01\n",
      "  9 SNES Function norm 2.443329e-02\n",
      "  10 SNES Function norm 4.920248e-04\n",
      "  11 SNES Function norm 2.000463e-07\n",
      "  12 SNES Function norm 3.261512e-14\n",
      "PETSc SNES solver converged in 12 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 1.701055e-02\n",
      "  2 SNES Function norm 9.240785e-06\n",
      "  3 SNES Function norm 6.017718e-12\n",
      "PETSc SNES solver converged in 3 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 1.358922e+00\n",
      "  2 SNES Function norm 1.221068e+00\n",
      "  3 SNES Function norm 1.095995e+00\n",
      "  4 SNES Function norm 9.830873e-01\n",
      "  5 SNES Function norm 8.817823e-01\n",
      "  6 SNES Function norm 6.332811e-01\n",
      "  7 SNES Function norm 2.626969e-01\n",
      "  8 SNES Function norm 3.625335e-02\n",
      "  9 SNES Function norm 9.215717e-04\n",
      "  10 SNES Function norm 6.087616e-07\n",
      "  11 SNES Function norm 2.587516e-13\n",
      "PETSc SNES solver converged in 11 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 6.308568e-02\n",
      "  2 SNES Function norm 5.532350e-04\n",
      "  3 SNES Function norm 5.665513e-08\n",
      "  4 SNES Function norm 4.062404e-15\n",
      "PETSc SNES solver converged in 4 iterations with convergence reason 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 8.588561e-01\n",
      "  2 SNES Function norm 2.404358e-01\n",
      "  3 SNES Function norm 2.988763e-02\n",
      "  4 SNES Function norm 5.398518e-04\n",
      "  5 SNES Function norm 1.828867e-07\n",
      "  6 SNES Function norm 2.169189e-14\n",
      "PETSc SNES solver converged in 6 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 6.984613e-02\n",
      "  2 SNES Function norm 2.635570e-03\n",
      "  3 SNES Function norm 2.999618e-06\n",
      "  4 SNES Function norm 3.783281e-12\n",
      "PETSc SNES solver converged in 4 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 2.008055e-01\n",
      "  2 SNES Function norm 1.584474e-02\n",
      "  3 SNES Function norm 9.876424e-05\n",
      "  4 SNES Function norm 3.834779e-09\n",
      "  5 SNES Function norm 3.984064e-15\n",
      "PETSc SNES solver converged in 5 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 1.358658e+00\n",
      "  2 SNES Function norm 1.220638e+00\n",
      "  3 SNES Function norm 1.095892e+00\n",
      "  4 SNES Function norm 9.127649e-01\n",
      "  5 SNES Function norm 8.953108e-01\n",
      "  6 SNES Function norm 2.270677e-01\n",
      "  7 SNES Function norm 2.588345e-02\n",
      "  8 SNES Function norm 4.161418e-04\n",
      "  9 SNES Function norm 1.092991e-07\n",
      "  10 SNES Function norm 8.503965e-15\n",
      "PETSc SNES solver converged in 10 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 1.263467e+00\n",
      "  2 SNES Function norm 9.416162e-01\n",
      "  3 SNES Function norm 8.131105e-01\n",
      "  4 SNES Function norm 2.155042e-01\n",
      "  5 SNES Function norm 2.550247e-02\n",
      "  6 SNES Function norm 4.334898e-04\n",
      "  7 SNES Function norm 1.295561e-07\n",
      "  8 SNES Function norm 1.248395e-14\n",
      "PETSc SNES solver converged in 8 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 1.358065e+00\n",
      "  2 SNES Function norm 1.218687e+00\n",
      "  3 SNES Function norm 1.092664e+00\n",
      "  4 SNES Function norm 9.797345e-01\n",
      "  5 SNES Function norm 7.147442e-01\n",
      "  6 SNES Function norm 3.047405e-01\n",
      "  7 SNES Function norm 4.314524e-02\n",
      "  8 SNES Function norm 1.182878e-03\n",
      "  9 SNES Function norm 9.192843e-07\n",
      "  10 SNES Function norm 5.414963e-13\n",
      "PETSc SNES solver converged in 10 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 1.358167e+00\n",
      "  2 SNES Function norm 1.219262e+00\n",
      "  3 SNES Function norm 1.093798e+00\n",
      "  4 SNES Function norm 9.464493e-01\n",
      "  5 SNES Function norm 4.981476e-01\n",
      "  6 SNES Function norm 5.572376e-02\n",
      "  7 SNES Function norm 2.001665e-03\n",
      "  8 SNES Function norm 2.697909e-06\n",
      "  9 SNES Function norm 4.754124e-12\n",
      "PETSc SNES solver converged in 9 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 1.194181e-01\n",
      "  2 SNES Function norm 4.108202e-03\n",
      "  3 SNES Function norm 4.999155e-06\n",
      "  4 SNES Function norm 7.508608e-12\n",
      "PETSc SNES solver converged in 4 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 1.211002e-01\n",
      "  2 SNES Function norm 2.939662e-03\n",
      "  3 SNES Function norm 2.155736e-06\n",
      "  4 SNES Function norm 1.225369e-12\n",
      "PETSc SNES solver converged in 4 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 3.188045e-02\n",
      "  2 SNES Function norm 7.018276e-05\n",
      "  3 SNES Function norm 5.761991e-10\n",
      "PETSc SNES solver converged in 3 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 1.357519e+00\n",
      "  2 SNES Function norm 1.217556e+00\n",
      "  3 SNES Function norm 1.091399e+00\n",
      "  4 SNES Function norm 9.625822e-01\n",
      "  5 SNES Function norm 5.564778e-01\n",
      "  6 SNES Function norm 8.565371e-02\n",
      "  7 SNES Function norm 4.402787e-03\n",
      "  8 SNES Function norm 1.275745e-05\n",
      "  9 SNES Function norm 1.051262e-10\n",
      "PETSc SNES solver converged in 9 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 6.588456e-01\n",
      "  2 SNES Function norm 1.481554e-01\n",
      "  3 SNES Function norm 1.036785e-02\n",
      "  4 SNES Function norm 5.488756e-05\n",
      "  5 SNES Function norm 1.564271e-09\n",
      "  6 SNES Function norm 3.943656e-15\n",
      "PETSc SNES solver converged in 6 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 1.411702e-01\n",
      "  2 SNES Function norm 8.375997e-03\n",
      "  3 SNES Function norm 2.710649e-05\n",
      "  4 SNES Function norm 2.808832e-10\n",
      "PETSc SNES solver converged in 4 iterations with convergence reason 2.\n",
      "  0 SNES Function norm 1.510614e+00\n",
      "  1 SNES Function norm 8.414482e-01\n",
      "  2 SNES Function norm 2.452427e-01\n",
      "  3 SNES Function norm 3.386376e-02\n",
      "  4 SNES Function norm 7.692028e-04\n",
      "  5 SNES Function norm 4.153829e-07\n",
      "  6 SNES Function norm 1.228352e-13\n",
      "PETSc SNES solver converged in 6 iterations with convergence reason 2.\n",
      "#################################### N = 8 #####################################\n",
      "ERROR\tNN-HF\t\t\tNN-RO\t\t\tRO-HF\n",
      "min\t0.0006421570077271726\t0.00044293358068267865\t3.340947511785926e-05\n",
      "mean\t0.005375954860894383\t0.0031414540826186694\t0.003311772648601754\n",
      "max\t0.02273853318978732\t0.022739082160739596\t0.011460726273859885\n"
     ]
    }
   ],
   "source": [
    "errors = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pinn_net, test_mu, input_normalization_pinn, output_normalization_pinn, relative=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pinn_net, (5., 5.), input_normalization_pinn, output_normalization_pinn, colorbar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3 PDNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pdnn_net, test_mu, input_normalization_pdnn, output_normalization_pdnn, relative=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pdnn_net, (5., 5.), input_normalization_pdnn, output_normalization_pdnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.4 PRNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    prnn_net, test_mu, input_normalization_prnn, output_normalization_prnn, relative=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    prnn_net, (5., 5.), input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.5 Neural Network Error Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = dict()\n",
    "nets[\"pinn_net\"] = pinn_net\n",
    "nets[\"pdnn_net\"] = pdnn_net\n",
    "nets[\"prnn_net\"] = prnn_net\n",
    "\n",
    "input_normalizations = dict()\n",
    "input_normalizations[\"pinn_net\"] = input_normalization_pinn\n",
    "input_normalizations[\"pdnn_net\"] = input_normalization_pdnn\n",
    "input_normalizations[\"prnn_net\"] = input_normalization_prnn\n",
    "\n",
    "output_normalizations = dict()\n",
    "output_normalizations[\"pinn_net\"] = output_normalization_pinn\n",
    "output_normalizations[\"pdnn_net\"] = output_normalization_pdnn\n",
    "output_normalizations[\"prnn_net\"] = output_normalization_prnn\n",
    "\n",
    "_ = ErrorAnalysis.error_analysis_by_network(\n",
    "    nets, test_mu, input_normalizations, output_normalizations, relative=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Perform a speedup analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method.speedup_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
