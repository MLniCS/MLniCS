{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 08 - Non linear Parabolic problem\n",
    "**_Keywords: exact parametrized functions, POD-Galerkin_**\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "In this tutorial, we consider the FitzHugh-Nagumo (F-N) system. The F-N system is used to describe neuron excitable systems. The nonlinear parabolic problem for the F-N system is defined on the interval $I=[0,L]$. Let $x\\in I$, $t\\geq0$\n",
    "\n",
    "$$\\begin{cases} \n",
    "    \\varepsilon u_t(x,t) =\\varepsilon^2u_{xx}(x,t)+g(u(x,t))-\\omega(x,t)+c, & x\\in I,\\quad t\\geq 0, \\\\\n",
    "    \\omega_t(x,t) =bu(x,t)-\\gamma\\omega(x,t)+c, & x\\in I,\\quad t\\geq 0, \\\\\n",
    "    u(x,0) = 0,\\quad\\omega(x,0)=0, & x\\in I, \\\\\n",
    "    u_x(0,t)=-i_0(t),\\quad u_x(L,t)=0, & t\\geq 0,\n",
    "\\end{cases}$$\n",
    "\n",
    "where the nonlinear function is defined by\n",
    "$$g(u) = u(u-0.1)(1-u)$$\n",
    "\n",
    "and the parameters are given by $L = 1$, $\\varepsilon = 0.015$, $b = 0.5$, $\\gamma = 2$, and $c = 0.05$. The stimulus $i_0(t)=50000t^3\\exp(-15t)$. The variables $u$ and $\\omega$ represent the $\\textit{voltage}$ and the $\\textit{recovery of voltage}$, respectively. \n",
    "\n",
    "In order to obtain an exact solution of the problem we pursue a model reduction by means of a POD-Galerkin reduced order method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Formulation for the F-N system\n",
    "\n",
    "Let $u,\\omega$ the solutions in the domain $I$.\n",
    "\n",
    "For this problem we want to find $\\boldsymbol{u}=(u,\\omega)$ such that\n",
    "\n",
    "$$\n",
    "m\\left(\\partial_t\\boldsymbol{u}(t),\\boldsymbol{v}\\right)+a\\left(\\boldsymbol{u}(t),\\boldsymbol{v}\\right)+c\\left(u(t),v\\right)=f(\\boldsymbol{v})\\quad \\forall \\boldsymbol{v}=(v,\\tilde{v}), \\text{ with }v,\\tilde{v} \\in\\mathbb{V},\\quad\\forall t\\geq0\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "* the function space $\\mathbb{V}$ is defined as\n",
    "$$\n",
    "\\mathbb{V} = \\{v\\in L^2(I) : v|_{\\{0\\}}=0\\}\n",
    "$$\n",
    "* the bilinear form $m(\\cdot, \\cdot): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$m(\\partial\\boldsymbol{u}(t), \\boldsymbol{v})=\\varepsilon\\int_{I}\\frac{\\partial u}{\\partial t}v \\ d\\boldsymbol{x} \\ + \\ \\int_{I}\\frac{\\partial\\omega}{\\partial t}\\tilde{v} \\ d\\boldsymbol{x},$$\n",
    "* the bilinear form $a(\\cdot, \\cdot): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$a(\\boldsymbol{u}(t), \\boldsymbol{v})=\\varepsilon^2\\int_{I} \\nabla u\\cdot \\nabla v \\ d\\boldsymbol{x}+\\int_{I}\\omega v \\ d\\boldsymbol{x} \\ - \\ b\\int_{I} u\\tilde{v} \\ d\\boldsymbol{x}+\\gamma\\int_{I}\\omega\\tilde{v} \\ d\\boldsymbol{x},$$\n",
    "* the bilinear form $c(\\cdot, \\cdot): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$c(u, v)=-\\int_{I} g(u)v \\ d\\boldsymbol{x},$$\n",
    "* the linear form $f(\\cdot): \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$f(\\boldsymbol{v})= c\\int_{I}\\left(v+\\tilde{v}\\right) \\ d\\boldsymbol{x} \\ + \\ \\varepsilon^2i_0(t)\\int_{\\{0\\}}v \\ d\\boldsymbol{s}.$$\n",
    "\n",
    "The output of interest $s(t)$ is given by\n",
    "$$s(t) = c\\int_{I}\\left[u(t)+\\omega(t)\\right] \\ d\\boldsymbol{x} \\ + \\ \\varepsilon^2i_0(t)\\int_{\\{0\\}}u(t) \\ d\\boldsymbol{s} $$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from mlnics import NN, Losses, Normalization, RONNData, IO, Training, ErrorAnalysis\n",
    "from dolfin import *\n",
    "from rbnics import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Affine Decomposition \n",
    "\n",
    "We set the variables $u:=u_1$, $\\omega:=u_2$ and the test functions $v:=v_1$, $\\tilde{v}:=v_2$.\n",
    "For this problem the affine decomposition is straightforward:\n",
    "    $$m(\\boldsymbol{u},\\boldsymbol{v})=\\underbrace{\\varepsilon}_{\\Theta^{m}_0}\\underbrace{\\int_{I}u_1v_1 \\ d\\boldsymbol{x}}_{m_0(u_1,v_1)} \\ + \\ \\underbrace{1}_{\\Theta^{m}_1}\\underbrace{\\int_{I}u_2v_2 \\ d\\boldsymbol{x}}_{m_1(u_2,v_2)},$$\n",
    "$$a(\\boldsymbol{u},\\boldsymbol{v})=\\underbrace{\\varepsilon^2}_{\\Theta^{a}_0}\\underbrace{\\int_{I}\\nabla u_1 \\cdot \\nabla v_1 \\ d\\boldsymbol{x}}_{a_0(u_1,v_1)} \\ + \\ \\underbrace{1}_{\\Theta^{a}_1}\\underbrace{\\int_{I}u_2v_1 \\ d\\boldsymbol{x}}_{a_1(u_2,v_1)} \\ + \\ \\underbrace{-b}_{\\Theta^{a}_2}\\underbrace{\\int_{I}u_1v_2 \\ d\\boldsymbol{x}}_{a_2(u_1,v_2)} \\ + \\ \\underbrace{\\gamma}_{\\Theta^{a}_3}\\underbrace{\\int_{I}u_2v_2 \\ d\\boldsymbol{x}}_{a_3(u_2,v_2)},$$\n",
    "$$c(u,v)=\\underbrace{-1}_{\\Theta^{c}_0}\\underbrace{\\int_{I}g(u_1)v_1 \\ d\\boldsymbol{x}}_{c_0(u_1,v_1)},$$\n",
    "$$f(\\boldsymbol{v}) = \\underbrace{c}_{\\Theta^{f}_0} \\underbrace{\\int_{I}(v_1 + v_2) \\ d\\boldsymbol{x}}_{f_0(v_1,v_2)} \\ + \\ \\underbrace{\\varepsilon^2i_0(t)}_{\\Theta^{f}_1} \\underbrace{\\int_{\\{0\\}} v_1 \\ d\\boldsymbol{s}}_{f_1(v_1)}.$$\n",
    "We will implement the numerical discretization of the problem in the class\n",
    "```\n",
    "class FitzHughNagumo(NonlinearParabolicProblem):\n",
    "```\n",
    "by specifying the coefficients $\\Theta^{m}_*$, $\\Theta^{a}_*$, $\\Theta^{c}_*$ and $\\Theta^{f}_*$ in the method\n",
    "```\n",
    "    def compute_theta(self, term):\n",
    "```\n",
    "and the bilinear forms $m_*(\\boldsymbol{u}, \\boldsymbol{v})$, $a_*(\\boldsymbol{u}, \\boldsymbol{v})$, $c_*(u, v)$ and linear forms $f_*(\\boldsymbol{v})$ in\n",
    "```\n",
    "    def assemble_operator(self, term):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DEIM(\"online\", basis_generation=\"Greedy\")\n",
    "@ExactParametrizedFunctions(\"offline\")\n",
    "class FitzHughNagumo(NonlinearParabolicProblem):\n",
    "\n",
    "    # Default initialization of members\n",
    "    def __init__(self, V, **kwargs):\n",
    "        # Call the standard initialization\n",
    "        NonlinearParabolicProblem.__init__(self, V, **kwargs)\n",
    "        # ... and also store FEniCS data structures for assembly\n",
    "        assert \"subdomains\" in kwargs\n",
    "        assert \"boundaries\" in kwargs\n",
    "        self.subdomains, self.boundaries = kwargs[\"subdomains\"], kwargs[\"boundaries\"]\n",
    "        self.du = TrialFunction(V)\n",
    "        (self.du1, self.du2) = split(self.du)\n",
    "        self.u = self._solution\n",
    "        (self.u1, self.u2) = split(self.u)\n",
    "        self.v = TestFunction(V)\n",
    "        (self.v1, self.v2) = split(self.v)\n",
    "        self.dx = Measure(\"dx\")(subdomain_data=self.subdomains)\n",
    "        self.ds = Measure(\"ds\")(subdomain_data=self.boundaries)\n",
    "        # Problem coefficients\n",
    "        self.epsilon = 0.015\n",
    "        self.b = 0.5\n",
    "        self.gamma = 2\n",
    "        self.c = 0.05\n",
    "        self.i0 = lambda t: 50000 * t**3 * exp(-15 * t)\n",
    "        self.g = lambda v: v * (v - 0.1) * (1 - v)\n",
    "        # Customize time stepping parameters\n",
    "        self._time_stepping_parameters.update({\n",
    "            \"report\": True,\n",
    "            \"snes_solver\": {\n",
    "                \"linear_solver\": \"umfpack\",\n",
    "                \"maximum_iterations\": 20,\n",
    "                \"report\": True\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Return custom problem name\n",
    "    def name(self):\n",
    "        return \"FitzHughNagumoDEIM\"\n",
    "\n",
    "    # Return theta multiplicative terms of the affine expansion of the problem.\n",
    "    @compute_theta_for_derivatives\n",
    "    def compute_theta(self, term):\n",
    "        if term == \"m\":\n",
    "            theta_m0 = self.epsilon\n",
    "            theta_m1 = 1.\n",
    "            return (theta_m0, theta_m1)\n",
    "        elif term == \"a\":\n",
    "            theta_a0 = self.epsilon**2\n",
    "            theta_a1 = 1.\n",
    "            theta_a2 = - self.b\n",
    "            theta_a3 = self.gamma\n",
    "            return (theta_a0, theta_a1, theta_a2, theta_a3)\n",
    "        elif term == \"c\":\n",
    "            theta_c0 = - 1.\n",
    "            return (theta_c0,)\n",
    "        elif term == \"f\":\n",
    "            t = self.t\n",
    "            theta_f0 = self.c\n",
    "            theta_f1 = self.epsilon**2 * self.i0(t)\n",
    "            return (theta_f0, theta_f1)\n",
    "        elif term == \"s\":\n",
    "            t = self.t\n",
    "            theta_s0 = self.c\n",
    "            theta_s1 = self.epsilon**2 * self.i0(t)\n",
    "            return (theta_s0, theta_s1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for compute_theta().\")\n",
    "\n",
    "    # Return forms resulting from the discretization of the affine expansion of the problem operators.\n",
    "    @assemble_operator_for_derivatives\n",
    "    def assemble_operator(self, term):\n",
    "        (v1, v2) = (self.v1, self.v2)\n",
    "        dx = self.dx\n",
    "        if term == \"m\":\n",
    "            (u1, u2) = (self.du1, self.du2)\n",
    "            m0 = u1 * v1 * dx\n",
    "            m1 = u2 * v2 * dx\n",
    "            return (m0, m1)\n",
    "        elif term == \"a\":\n",
    "            (u1, u2) = (self.du1, self.du2)\n",
    "            a0 = inner(grad(u1), grad(v1)) * dx\n",
    "            a1 = u2 * v1 * dx\n",
    "            a2 = u1 * v2 * dx\n",
    "            a3 = u2 * v2 * dx\n",
    "            return (a0, a1, a2, a3)\n",
    "        elif term == \"c\":\n",
    "            u1 = self.u1\n",
    "            c0 = self.g(u1) * v1 * dx\n",
    "            return (c0,)\n",
    "        elif term == \"f\":\n",
    "            ds = self.ds\n",
    "            f0 = v1 * dx + v2 * dx\n",
    "            f1 = v1 * ds(1)\n",
    "            return (f0, f1)\n",
    "        elif term == \"s\":\n",
    "            (v1, v2) = (self.v1, self.v2)\n",
    "            ds = self.ds\n",
    "            s0 = v1 * dx + v2 * dx\n",
    "            s1 = v1 * ds(1)\n",
    "            return (s0, s1)\n",
    "        elif term == \"inner_product\":\n",
    "            (u1, u2) = (self.du1, self.du2)\n",
    "            x0 = inner(grad(u1), grad(v1)) * dx + u2 * v2 * dx\n",
    "            return (x0,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for assemble_operator().\")\n",
    "\n",
    "\n",
    "# Customize the resulting reduced problem\n",
    "@CustomizeReducedProblemFor(NonlinearParabolicProblem)\n",
    "def CustomizeReducedNonlinearParabolic(ReducedNonlinearParabolic_Base):\n",
    "    class ReducedNonlinearParabolic(ReducedNonlinearParabolic_Base):\n",
    "        def __init__(self, truth_problem, **kwargs):\n",
    "            ReducedNonlinearParabolic_Base.__init__(self, truth_problem, **kwargs)\n",
    "            self._time_stepping_parameters.update({\n",
    "                \"report\": True,\n",
    "                \"nonlinear_solver\": {\n",
    "                    \"report\": True,\n",
    "                    \"line_search\": \"wolfe\"\n",
    "                }\n",
    "            })\n",
    "\n",
    "    return ReducedNonlinearParabolic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main program\n",
    "\n",
    "### 4.1. Read the mesh for this problem\n",
    "The mesh was generated by the [data/generate_mesh.ipynb](data/generate_mesh.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = Mesh(\"data/interval.xml\")\n",
    "subdomains = MeshFunction(\"size_t\", mesh, \"data/interval_physical_region.xml\")\n",
    "boundaries = MeshFunction(\"size_t\", mesh, \"data/interval_facet_region.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create Finite Element space (Lagrange P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = VectorFunctionSpace(mesh, \"Lagrange\", 1, dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Allocate an object of the FitzHughNagumo class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = FitzHughNagumo(V, subdomains=subdomains, boundaries=boundaries)\n",
    "mu_range = []\n",
    "problem.set_mu_range(mu_range)\n",
    "problem.set_time_step_size(0.02)\n",
    "problem.set_final_time(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Prepare reduction with a POD-Galerkin method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method = PODGalerkin(problem)\n",
    "reduction_method.set_Nmax(20, DEIM=20)\n",
    "reduction_method.set_tolerance(0, DEIM=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Perform the offline phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Fit Reduction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreduction_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_training_set\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEIM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEquispacedDistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m reduced_problem \u001b[38;5;241m=\u001b[39m reduction_method\u001b[38;5;241m.\u001b[39moffline()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLniCS/lib/python3.10/site-packages/RBniCS-0.1.dev1-py3.10.egg/rbnics/eim/reduction_methods/deim_decorated_reduction_method.py:79\u001b[0m, in \u001b[0;36mDEIMDecoratedReductionMethod.<locals>.DEIMDecoratedReductionMethod_Class.initialize_training_set\u001b[0;34m(self, ntrain, enable_import, sampling, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetter\u001b[39m(DEIM_reduction, ntrain_DEIM):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# kwargs are not needed\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DEIM_reduction\u001b[38;5;241m.\u001b[39minitialize_training_set(ntrain_DEIM, enable_import, sampling)\n\u001b[0;32m---> 79\u001b[0m import_successful_DEIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_propagate_setter_from_kwargs_to_DEIM_reductions\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m import_successful \u001b[38;5;129;01mand\u001b[39;00m import_successful_DEIM\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLniCS/lib/python3.10/site-packages/RBniCS-0.1.dev1-py3.10.egg/rbnics/eim/reduction_methods/deim_decorated_reduction_method.py:118\u001b[0m, in \u001b[0;36mDEIMDecoratedReductionMethod.<locals>.DEIMDecoratedReductionMethod_Class._propagate_setter_from_kwargs_to_DEIM_reductions\u001b[0;34m(self, setter, Type, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (term, DEIM_reductions_term) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDEIM_reductions\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (_, DEIM_reduction_term_q) \u001b[38;5;129;01min\u001b[39;00m DEIM_reductions_term\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 118\u001b[0m             current_return_value \u001b[38;5;241m=\u001b[39m \u001b[43msetter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEIM_reduction_term_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwarg_DEIM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m             return_value \u001b[38;5;241m=\u001b[39m current_return_value \u001b[38;5;129;01mand\u001b[39;00m return_value\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m return_value\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLniCS/lib/python3.10/site-packages/RBniCS-0.1.dev1-py3.10.egg/rbnics/eim/reduction_methods/deim_decorated_reduction_method.py:77\u001b[0m, in \u001b[0;36mDEIMDecoratedReductionMethod.<locals>.DEIMDecoratedReductionMethod_Class.initialize_training_set.<locals>.setter\u001b[0;34m(DEIM_reduction, ntrain_DEIM)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetter\u001b[39m(DEIM_reduction, ntrain_DEIM):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# kwargs are not needed\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEIM_reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_training_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mntrain_DEIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_import\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLniCS/lib/python3.10/site-packages/RBniCS-0.1.dev1-py3.10.egg/rbnics/eim/reduction_methods/time_dependent_eim_approximation_reduction_method.py:28\u001b[0m, in \u001b[0;36mTimeDependentEIMApproximationReductionMethod.initialize_training_set\u001b[0;34m(self, ntrain, enable_import, sampling, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m time_import_successful \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enable_import:\n\u001b[0;32m---> 28\u001b[0m     time_import_successful \u001b[38;5;241m=\u001b[39m \u001b[43mtime_training_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfolder\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_set\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime_training_set\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(time_training_set) \u001b[38;5;241m==\u001b[39m ntrain)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m time_import_successful:\n\u001b[1;32m     31\u001b[0m     time_sampling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_time_sampling(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MLniCS/lib/python3.10/site-packages/RBniCS-0.1.dev1-py3.10.egg/rbnics/utils/io/exportable_list.py:55\u001b[0m, in \u001b[0;36mExportableList.load\u001b[0;34m(self, directory, filename)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reduction_method.initialize_training_set(1, DEIM=1, sampling=EquispacedDistribution())\n",
    "reduced_problem = reduction_method.offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 Train PINN\n",
    "\n",
    "Given a training set $X_{PINN} = (\\boldsymbol{\\mu}^{(1)}, \\dots, \\boldsymbol{\\mu}^{(n)})$ of parameters for the PDE, we train a Physics-Informed Neural Network (PINN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PINN}(X_{PINN}; W) = \\frac1n \\sum_{i=1}^n \\left\\|M(\\boldsymbol{\\mu^{(i)}})\\frac{\\partial \\operatorname{N}_W}{\\partial t}(\\boldsymbol{\\mu^{(i)}}) + A(\\boldsymbol{\\mu^{(i)}}) \\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\boldsymbol{f}(\\boldsymbol{\\mu}^{(i)}) + \\boldsymbol{c}(\\boldsymbol{\\mu}^{(i)})\\right\\|_2^2$$\n",
    "\n",
    "over $W$, where for a given $\\boldsymbol{\\mu}$, $M(\\boldsymbol{\\mu})$ is the mass matrix corresponding to the bilinear form $m$, $A(\\boldsymbol{\\mu})$ is the assembled matrix corresponding to the bilinear form $a$, $\\boldsymbol{f}(\\boldsymbol{\\mu})$ is the assembled vector corresponding to the linear form $f$, and $\\boldsymbol{c}(\\boldsymbol{\\mu})$ is a vector corresponding to the nonlinear form $c$. The partial derivative $\\frac{\\partial \\operatorname{N}_W}{\\partial t}(\\boldsymbol{\\mu^{(i)}})$ is computed via automatic differentiation in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pinn = Normalization.MinMaxNormalization(input_normalization=True)\n",
    "output_normalization_pinn = Normalization.MinMaxNormalization()\n",
    "\n",
    "pinn_net  = NN.RONN(\"PINN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pinn_loss = Losses.PINN_Loss(pinn_net, output_normalization_pinn)\n",
    "data      = RONNData.RONNDataLoader(pinn_net, validation_proportion=0.2, \n",
    "                                    num_without_snapshots=1)\n",
    "optimizer = torch.optim.Adam(pinn_net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.99999)\n",
    "\n",
    "pinn_trainer = Training.PINNTrainer(\n",
    "    pinn_net, data, pinn_loss, optimizer, scheduler,\n",
    "    input_normalization_pinn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pinn_net, data, pinn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam(pinn_net.parameters(), lr=0.001)\n",
    "#pinn_trainer.optimizer = optimizer\n",
    "pinn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_loss.operators['c'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rbnics.backends.dolfin.evaluate import evaluate\n",
    "problem.set_time(0.)\n",
    "evaluate(problem.operator['c'][0])[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_net.time_augmented_mu.shape, pinn_loss.operators['c'].shape, 51*80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pinn_trainer, pinn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 Train PDNN\n",
    "\n",
    "Given a training set $X_{PDNN} = ((\\boldsymbol{\\mu}^{(1)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(1)})), \\dots, (\\boldsymbol{\\mu}^{(n)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(n)})))$ of parameter and high fidelity solution pairs for the PDE, we train a Projection-Driven Neural Network (PDNN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "$$L_{PDNN}(X_{PDNN}; W) = \\frac1n \\sum_{i=1}^n \\|\\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu}^{(i)})\\|_2^2,$$\n",
    "where for a given $\\boldsymbol{\\mu}$, $\\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu})$ is the projection of $\\operatorname{HF}(\\boldsymbol{\\mu})$ onto the reduced order solution space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pdnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pdnn = Normalization.StandardNormalization()\n",
    "\n",
    "pdnn_net  = NN.RONN(\"PDNN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pdnn_loss = Losses.PDNN_Loss(pdnn_net, output_normalization_pdnn)\n",
    "data      = RONNData.RONNDataLoader(pdnn_net, validation_proportion=0.2)\n",
    "optimizer = torch.optim.Adam(pdnn_net.parameters(), lr=0.001)\n",
    "\n",
    "pdnn_trainer = Training.PDNNTrainer(\n",
    "    pdnn_net, data, pdnn_loss, optimizer,\n",
    "    input_normalization_pdnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pdnn_net, data, pdnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pdnn_trainer, pdnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Train PRNN\n",
    "\n",
    "We train a Physics-Reinforced Neural Network (PRNN) $N_W(\\boldsymbol{\\mu})$ dependnent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PRNN}(X_{PINN}, X_{PDNN}; W) = L_{PINN}(X_{PINN}; W) + \\omega L_{PDNN}(X_{PDNN}; W),$$\n",
    "\n",
    "where $\\omega$ is a scaling parameter which can be chosen freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_prnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_prnn = Normalization.StandardNormalization()\n",
    "\n",
    "omega = 1.\n",
    "prnn_net  = NN.RONN(f\"PRNN_{omega}\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "prnn_loss = Losses.PRNN_Loss(prnn_net, output_normalization_prnn, omega=omega)\n",
    "data      = RONNData.RONNDataLoader(prnn_net, validation_proportion=0.2,\n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(prnn_net.parameters(), lr=0.001)\n",
    "\n",
    "prnn_trainer = Training.PRNNTrainer(\n",
    "    prnn_net, data, prnn_loss, optimizer,\n",
    "    input_normalization_prnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    prnn_net, data, prnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(prnn_trainer, prnn_net, separate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Perform an error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 Reduction Method Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduction_method.initialize_testing_set(1)\n",
    "reduction_method.error_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 PINN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mu = torch.tensor(reduction_method.testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pinn_net, test_mu, input_normalization_pinn, output_normalization_pinn, relative=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pinn_net, tuple(), input_normalization_pinn, output_normalization_pinn, t=0, component=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3 PDNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pdnn_net, test_mu, input_normalization_pdnn, output_normalization_pdnn, relative=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pdnn_net, tuple(), input_normalization_pdnn, output_normalization_pdnn, t=0, component=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.4 PRNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    prnn_net, test_mu, input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    prnn_net, tuple(), input_normalization_prnn, output_normalization_prnn, t=0, component=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.5 Neural Network Error Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = dict()\n",
    "nets[\"pinn_net\"] = pinn_net\n",
    "nets[\"pdnn_net\"] = pdnn_net\n",
    "nets[\"prnn_net\"] = prnn_net\n",
    "\n",
    "input_normalizations = dict()\n",
    "input_normalizations[\"pinn_net\"] = input_normalization_pinn\n",
    "input_normalizations[\"pdnn_net\"] = input_normalization_pdnn\n",
    "input_normalizations[\"prnn_net\"] = input_normalization_prnn\n",
    "\n",
    "output_normalizations = dict()\n",
    "output_normalizations[\"pinn_net\"] = output_normalization_pinn\n",
    "output_normalizations[\"pdnn_net\"] = output_normalization_pdnn\n",
    "output_normalizations[\"prnn_net\"] = output_normalization_prnn\n",
    "\n",
    "_ = ErrorAnalysis.error_analysis_by_network(\n",
    "    nets, test_mu, input_normalizations, output_normalizations, relative=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Perform a speedup analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method.speedup_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
