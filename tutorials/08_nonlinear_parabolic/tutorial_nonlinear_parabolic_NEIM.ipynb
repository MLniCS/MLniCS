{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 08 - Non linear Parabolic problem\n",
    "**_Keywords: exact parametrized functions, POD-Galerkin_**\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "In this tutorial, we consider the FitzHugh-Nagumo (F-N) system. The F-N system is used to describe neuron excitable systems. The nonlinear parabolic problem for the F-N system is defined on the interval $I=[0,L]$. Let $x\\in I$, $t\\geq0$\n",
    "\n",
    "$$\\begin{cases} \n",
    "    \\varepsilon u_t(x,t) =\\varepsilon^2u_{xx}(x,t)+g(u(x,t))-\\omega(x,t)+c, & x\\in I,\\quad t\\geq 0, \\\\\n",
    "    \\omega_t(x,t) =bu(x,t)-\\gamma\\omega(x,t)+c, & x\\in I,\\quad t\\geq 0, \\\\\n",
    "    u(x,0) = 0,\\quad\\omega(x,0)=0, & x\\in I, \\\\\n",
    "    u_x(0,t)=-i_0(t),\\quad u_x(L,t)=0, & t\\geq 0,\n",
    "\\end{cases}$$\n",
    "\n",
    "where the nonlinear function is defined by\n",
    "$$g(u) = u(u-0.1)(1-u)$$\n",
    "\n",
    "and the parameters are given by $L = 1$, $\\varepsilon = 0.015$, $b = 0.5$, $\\gamma = 2$, and $c = 0.05$. The stimulus $i_0(t)=50000t^3\\exp(-15t)$. The variables $u$ and $\\omega$ represent the $\\textit{voltage}$ and the $\\textit{recovery of voltage}$, respectively. \n",
    "\n",
    "In order to obtain an exact solution of the problem we pursue a model reduction by means of a POD-Galerkin reduced order method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Formulation for the F-N system\n",
    "\n",
    "Let $u,\\omega$ the solutions in the domain $I$.\n",
    "\n",
    "For this problem we want to find $\\boldsymbol{u}=(u,\\omega)$ such that\n",
    "\n",
    "$$\n",
    "m\\left(\\partial_t\\boldsymbol{u}(t),\\boldsymbol{v}\\right)+a\\left(\\boldsymbol{u}(t),\\boldsymbol{v}\\right)+c\\left(u(t),v\\right)=f(\\boldsymbol{v})\\quad \\forall \\boldsymbol{v}=(v,\\tilde{v}), \\text{ with }v,\\tilde{v} \\in\\mathbb{V},\\quad\\forall t\\geq0\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "* the function space $\\mathbb{V}$ is defined as\n",
    "$$\n",
    "\\mathbb{V} = \\{v\\in L^2(I) : v|_{\\{0\\}}=0\\}\n",
    "$$\n",
    "* the bilinear form $m(\\cdot, \\cdot): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$m(\\partial\\boldsymbol{u}(t), \\boldsymbol{v})=\\varepsilon\\int_{I}\\frac{\\partial u}{\\partial t}v \\ d\\boldsymbol{x} \\ + \\ \\int_{I}\\frac{\\partial\\omega}{\\partial t}\\tilde{v} \\ d\\boldsymbol{x},$$\n",
    "* the bilinear form $a(\\cdot, \\cdot): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$a(\\boldsymbol{u}(t), \\boldsymbol{v})=\\varepsilon^2\\int_{I} \\nabla u\\cdot \\nabla v \\ d\\boldsymbol{x}+\\int_{I}\\omega v \\ d\\boldsymbol{x} \\ - \\ b\\int_{I} u\\tilde{v} \\ d\\boldsymbol{x}+\\gamma\\int_{I}\\omega\\tilde{v} \\ d\\boldsymbol{x},$$\n",
    "* the bilinear form $c(\\cdot, \\cdot): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$c(u, v)=-\\int_{I} g(u)v \\ d\\boldsymbol{x},$$\n",
    "* the linear form $f(\\cdot): \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$f(\\boldsymbol{v})= c\\int_{I}\\left(v+\\tilde{v}\\right) \\ d\\boldsymbol{x} \\ + \\ \\varepsilon^2i_0(t)\\int_{\\{0\\}}v \\ d\\boldsymbol{s}.$$\n",
    "\n",
    "The output of interest $s(t)$ is given by\n",
    "$$s(t) = c\\int_{I}\\left[u(t)+\\omega(t)\\right] \\ d\\boldsymbol{x} \\ + \\ \\varepsilon^2i_0(t)\\int_{\\{0\\}}u(t) \\ d\\boldsymbol{s} $$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from mlnics import NN, Losses, Normalization, RONNData, IO, Training, ErrorAnalysis\n",
    "from dolfin import *\n",
    "from rbnics import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Affine Decomposition \n",
    "\n",
    "We set the variables $u:=u_1$, $\\omega:=u_2$ and the test functions $v:=v_1$, $\\tilde{v}:=v_2$.\n",
    "For this problem the affine decomposition is straightforward:\n",
    "    $$m(\\boldsymbol{u},\\boldsymbol{v})=\\underbrace{\\varepsilon}_{\\Theta^{m}_0}\\underbrace{\\int_{I}u_1v_1 \\ d\\boldsymbol{x}}_{m_0(u_1,v_1)} \\ + \\ \\underbrace{1}_{\\Theta^{m}_1}\\underbrace{\\int_{I}u_2v_2 \\ d\\boldsymbol{x}}_{m_1(u_2,v_2)},$$\n",
    "$$a(\\boldsymbol{u},\\boldsymbol{v})=\\underbrace{\\varepsilon^2}_{\\Theta^{a}_0}\\underbrace{\\int_{I}\\nabla u_1 \\cdot \\nabla v_1 \\ d\\boldsymbol{x}}_{a_0(u_1,v_1)} \\ + \\ \\underbrace{1}_{\\Theta^{a}_1}\\underbrace{\\int_{I}u_2v_1 \\ d\\boldsymbol{x}}_{a_1(u_2,v_1)} \\ + \\ \\underbrace{-b}_{\\Theta^{a}_2}\\underbrace{\\int_{I}u_1v_2 \\ d\\boldsymbol{x}}_{a_2(u_1,v_2)} \\ + \\ \\underbrace{\\gamma}_{\\Theta^{a}_3}\\underbrace{\\int_{I}u_2v_2 \\ d\\boldsymbol{x}}_{a_3(u_2,v_2)},$$\n",
    "$$c(u,v)=\\underbrace{-1}_{\\Theta^{c}_0}\\underbrace{\\int_{I}g(u_1)v_1 \\ d\\boldsymbol{x}}_{c_0(u_1,v_1)},$$\n",
    "$$f(\\boldsymbol{v}) = \\underbrace{c}_{\\Theta^{f}_0} \\underbrace{\\int_{I}(v_1 + v_2) \\ d\\boldsymbol{x}}_{f_0(v_1,v_2)} \\ + \\ \\underbrace{\\varepsilon^2i_0(t)}_{\\Theta^{f}_1} \\underbrace{\\int_{\\{0\\}} v_1 \\ d\\boldsymbol{s}}_{f_1(v_1)}.$$\n",
    "We will implement the numerical discretization of the problem in the class\n",
    "```\n",
    "class FitzHughNagumo(NonlinearParabolicProblem):\n",
    "```\n",
    "by specifying the coefficients $\\Theta^{m}_*$, $\\Theta^{a}_*$, $\\Theta^{c}_*$ and $\\Theta^{f}_*$ in the method\n",
    "```\n",
    "    def compute_theta(self, term):\n",
    "```\n",
    "and the bilinear forms $m_*(\\boldsymbol{u}, \\boldsymbol{v})$, $a_*(\\boldsymbol{u}, \\boldsymbol{v})$, $c_*(u, v)$ and linear forms $f_*(\\boldsymbol{v})$ in\n",
    "```\n",
    "    def assemble_operator(self, term):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ExactParametrizedFunctions()\n",
    "class FitzHughNagumo(NonlinearParabolicProblem):\n",
    "\n",
    "    # Default initialization of members\n",
    "    def __init__(self, V, **kwargs):\n",
    "        # Call the standard initialization\n",
    "        NonlinearParabolicProblem.__init__(self, V, **kwargs)\n",
    "        # ... and also store FEniCS data structures for assembly\n",
    "        assert \"subdomains\" in kwargs\n",
    "        assert \"boundaries\" in kwargs\n",
    "        self.subdomains, self.boundaries = kwargs[\"subdomains\"], kwargs[\"boundaries\"]\n",
    "        self.du = TrialFunction(V)\n",
    "        (self.du1, self.du2) = split(self.du)\n",
    "        self.u = self._solution\n",
    "        (self.u1, self.u2) = split(self.u)\n",
    "        self.v = TestFunction(V)\n",
    "        (self.v1, self.v2) = split(self.v)\n",
    "        self.dx = Measure(\"dx\")(subdomain_data=self.subdomains)\n",
    "        self.ds = Measure(\"ds\")(subdomain_data=self.boundaries)\n",
    "        # Problem coefficients\n",
    "        self.epsilon = 0.015\n",
    "        self.b = 0.5\n",
    "        self.gamma = 2\n",
    "        self.c = 0.05\n",
    "        self.i0 = lambda t: 50000 * t**3 * exp(-15 * t)\n",
    "        self.g = lambda v: v * (v - 0.1) * (1 - v)\n",
    "        # Customize time stepping parameters\n",
    "        self._time_stepping_parameters.update({\n",
    "            \"report\": True,\n",
    "            \"snes_solver\": {\n",
    "                \"linear_solver\": \"umfpack\",\n",
    "                \"maximum_iterations\": 20,\n",
    "                \"report\": True\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Return custom problem name\n",
    "    def name(self):\n",
    "        return \"FitzHughNagumoExact\"\n",
    "\n",
    "    # Return theta multiplicative terms of the affine expansion of the problem.\n",
    "    @compute_theta_for_derivatives\n",
    "    def compute_theta(self, term):\n",
    "        if term == \"m\":\n",
    "            theta_m0 = self.epsilon\n",
    "            theta_m1 = 1.\n",
    "            return (theta_m0, theta_m1)\n",
    "        elif term == \"a\":\n",
    "            theta_a0 = self.epsilon**2\n",
    "            theta_a1 = 1.\n",
    "            theta_a2 = - self.b\n",
    "            theta_a3 = self.gamma\n",
    "            return (theta_a0, theta_a1, theta_a2, theta_a3)\n",
    "        elif term == \"c\":\n",
    "            theta_c0 = - 1.\n",
    "            return (theta_c0,)\n",
    "        elif term == \"f\":\n",
    "            t = self.t\n",
    "            theta_f0 = self.c\n",
    "            theta_f1 = self.epsilon**2 * self.i0(t)\n",
    "            return (theta_f0, theta_f1)\n",
    "        elif term == \"s\":\n",
    "            t = self.t\n",
    "            theta_s0 = self.c\n",
    "            theta_s1 = self.epsilon**2 * self.i0(t)\n",
    "            return (theta_s0, theta_s1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for compute_theta().\")\n",
    "\n",
    "    # Return forms resulting from the discretization of the affine expansion of the problem operators.\n",
    "    @assemble_operator_for_derivatives\n",
    "    def assemble_operator(self, term):\n",
    "        (v1, v2) = (self.v1, self.v2)\n",
    "        dx = self.dx\n",
    "        if term == \"m\":\n",
    "            (u1, u2) = (self.du1, self.du2)\n",
    "            m0 = u1 * v1 * dx\n",
    "            m1 = u2 * v2 * dx\n",
    "            return (m0, m1)\n",
    "        elif term == \"a\":\n",
    "            (u1, u2) = (self.du1, self.du2)\n",
    "            a0 = inner(grad(u1), grad(v1)) * dx\n",
    "            a1 = u2 * v1 * dx\n",
    "            a2 = u1 * v2 * dx\n",
    "            a3 = u2 * v2 * dx\n",
    "            return (a0, a1, a2, a3)\n",
    "        elif term == \"c\":\n",
    "            u1 = self.u1\n",
    "            c0 = self.g(u1) * v1 * dx\n",
    "            return (c0,)\n",
    "        elif term == \"f\":\n",
    "            ds = self.ds\n",
    "            f0 = v1 * dx + v2 * dx\n",
    "            f1 = v1 * ds(1)\n",
    "            return (f0, f1)\n",
    "        elif term == \"s\":\n",
    "            (v1, v2) = (self.v1, self.v2)\n",
    "            ds = self.ds\n",
    "            s0 = v1 * dx + v2 * dx\n",
    "            s1 = v1 * ds(1)\n",
    "            return (s0, s1)\n",
    "        elif term == \"inner_product\":\n",
    "            (u1, u2) = (self.du1, self.du2)\n",
    "            x0 = inner(grad(u1), grad(v1)) * dx + u2 * v2 * dx\n",
    "            return (x0,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for assemble_operator().\")\n",
    "\n",
    "\n",
    "# Customize the resulting reduced problem\n",
    "@CustomizeReducedProblemFor(NonlinearParabolicProblem)\n",
    "def CustomizeReducedNonlinearParabolic(ReducedNonlinearParabolic_Base):\n",
    "    class ReducedNonlinearParabolic(ReducedNonlinearParabolic_Base):\n",
    "        def __init__(self, truth_problem, **kwargs):\n",
    "            ReducedNonlinearParabolic_Base.__init__(self, truth_problem, **kwargs)\n",
    "            self._time_stepping_parameters.update({\n",
    "                \"report\": True,\n",
    "                \"nonlinear_solver\": {\n",
    "                    \"report\": True,\n",
    "                    \"line_search\": \"wolfe\"\n",
    "                }\n",
    "            })\n",
    "\n",
    "    return ReducedNonlinearParabolic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main program\n",
    "\n",
    "### 4.1. Read the mesh for this problem\n",
    "The mesh was generated by the [data/generate_mesh.ipynb](data/generate_mesh.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = Mesh(\"data/interval.xml\")\n",
    "subdomains = MeshFunction(\"size_t\", mesh, \"data/interval_physical_region.xml\")\n",
    "boundaries = MeshFunction(\"size_t\", mesh, \"data/interval_facet_region.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create Finite Element space (Lagrange P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = VectorFunctionSpace(mesh, \"Lagrange\", 1, dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Allocate an object of the FitzHughNagumo class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = FitzHughNagumo(V, subdomains=subdomains, boundaries=boundaries)\n",
    "mu_range = []\n",
    "problem.set_mu_range(mu_range)\n",
    "problem.set_time_step_size(0.02)\n",
    "problem.set_final_time(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Prepare reduction with a POD-Galerkin method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method = PODGalerkin(problem)\n",
    "reduction_method.set_Nmax(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Perform the offline phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Fit Reduction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduction_method.initialize_training_set(1)\n",
    "reduced_problem = reduction_method.offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_size, 60)\n",
    "        self.fc2 = nn.Linear(60, 60)\n",
    "        self.fc3 = nn.Linear(60, out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Get nonlinear_terms, parameters, N0\n",
    "nonlinear_terms = np.zeros((len(reduction_method.training_set), len(reduction_method.training_set), reduced_problem.N))\n",
    "params = np.array(reduction_method.training_set)\n",
    "N0 = lambda sol, param_idx: 0 # N0 identically 0\n",
    "\n",
    "solutions = []\n",
    "Basis_Matrix = np.array([v.vector()[:] for v in reduced_problem.basis_functions])\n",
    "\n",
    "for mu in reduction_method.training_set:\n",
    "    problem.set_mu(mu)\n",
    "    solution = problem.solve()\n",
    "    solutions.append(np.array(problem._solution.vector()[:]))\n",
    "\n",
    "for i, mu in enumerate(reduction_method.training_set):\n",
    "    problem.set_mu(mu)\n",
    "    operator_form = problem.assemble_operator('c')[0]\n",
    "    theta = problem.compute_theta('c')\n",
    "    \n",
    "    for j, solution in enumerate(solutions):\n",
    "        problem._solution.vector()[:] = solution\n",
    "        nonlinear_terms[i, j] = (theta * Basis_Matrix @ np.array(assemble(operator_form)[:]).reshape(-1, 1)).reshape(-1)\n",
    "        \n",
    "solutions_ = []\n",
    "for sol in solutions:\n",
    "    F = Function(V)\n",
    "    F.vector()[:] = sol\n",
    "    solutions_.append(np.array(reduced_problem.project(F).vector()[:]))\n",
    "solutions = solutions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_parameter_indices = []\n",
    "fixed_mu_networks = []\n",
    "\n",
    "# Step 2\n",
    "# 2a. Set mu_1\n",
    "errors = np.zeros(params.shape[0])\n",
    "for i, mu_i in enumerate(params):\n",
    "    # compute error\n",
    "    s = 0\n",
    "    for j, mu_j in enumerate(params):\n",
    "        s += np.sum((nonlinear_terms[i, j] - N0(solutions[j], i))**2)\n",
    "    errors[i] = s / params.shape[0]\n",
    "\n",
    "mu_1_idx = np.argmax(errors)\n",
    "mu_1 = params[mu_1_idx]\n",
    "chosen_parameter_indices.append(mu_1_idx)\n",
    "print(\"max error:\", np.max(errors))\n",
    "print(\"mu_1 index:\", mu_1_idx)\n",
    "\n",
    "# 2b. Train Network_{mu_1}(u) to approximate Nonlinearity(u; mu_1)\n",
    "print(\"\\nTraining network to approximate nonlinearity...\")\n",
    "Network_mu_1 = Net(reduced_problem.N, reduced_problem.N)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(Network_mu_1.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9999)\n",
    "\n",
    "x_normalization = Normalization.MinMaxNormalization(input_normalization=True)\n",
    "y_normalization = Normalization.MinMaxNormalization()\n",
    "\n",
    "x_data = x_normalization(torch.tensor(np.array(solutions)).float())\n",
    "y_data = y_normalization(torch.tensor(nonlinear_terms[mu_1_idx] / np.linalg.norm(nonlinear_terms[mu_1_idx])).float().T).T\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(30000):\n",
    "    optimizer.zero_grad()\n",
    "    output = Network_mu_1(x_data)\n",
    "    loss = criterion(output, y_data)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "Network_mu_1.eval()\n",
    "fixed_mu_networks.append(Network_mu_1)\n",
    "\n",
    "# 2c. Find theta_1_1(mu)\n",
    "print(\"\\nFinding theta...\")\n",
    "thetas = np.zeros((params.shape[0], 1))\n",
    "for i, mu_i in enumerate(params):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for j, mu_j in enumerate(params):\n",
    "        net_u_mu = y_normalization(Network_mu_1(x_data[j].view(1, -1)).T, normalize=False).T.detach().numpy().reshape(-1)\n",
    "        numerator += np.dot(nonlinear_terms[i, j], net_u_mu)\n",
    "        denominator += np.dot(net_u_mu, net_u_mu)\n",
    "        \n",
    "    theta_1_1_i = numerator / denominator\n",
    "    thetas[i] = theta_1_1_i\n",
    "    print(thetas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = []\n",
    "maximum_error = np.max(errors)\n",
    "y_normalization_list = [y_normalization]\n",
    "mean_errors = 1\n",
    "\n",
    "mean_errors_list = []\n",
    "mean_errors_list_all = []\n",
    "\n",
    "#while np.mean(mean_errors) > 0.0005:\n",
    "for iteration in range(7):\n",
    "    # Step 3\n",
    "    # 3a. Set mu_2\n",
    "    N1 = lambda sol, param_idx: sum([\n",
    "        thetas[param_idx][i] * y_normalization_list[i](\n",
    "            net(x_normalization(torch.tensor(sol).float().view(1, -1))).T, normalize=False\n",
    "        ).T.detach().numpy().reshape(-1)\\\n",
    "        for i, net in enumerate(fixed_mu_networks)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "\n",
    "    errors = np.zeros(params.shape[0])\n",
    "    for i, mu_i in enumerate(params):\n",
    "        # compute error\n",
    "        s = 0\n",
    "        for j, mu_j in enumerate(params):\n",
    "            s += np.sum((nonlinear_terms[i, j] - N1(solutions[j], i))**2)\n",
    "        errors[i] = s / params.shape[0]\n",
    "        \n",
    "    mean_errors = np.zeros(params.shape[0])\n",
    "    for i, mu_i in enumerate(params):\n",
    "        mean_errors[i] = np.linalg.norm(nonlinear_terms[i, i] - N1(solutions[i], i)) / np.linalg.norm(nonlinear_terms[i, i])\n",
    "    \n",
    "    mean_errors_all = np.zeros(params.shape[0])\n",
    "    for i, mu_i in enumerate(params):\n",
    "        for j, mu_j in enumerate(params):\n",
    "            mean_errors_all[i] += np.linalg.norm(nonlinear_terms[i, j] - N1(solutions[j], i)) / np.linalg.norm(nonlinear_terms[i, j])\n",
    "        mean_errors_all[i] /= params.shape[0]\n",
    "        \n",
    "    print(\"max error:\", np.max(errors))\n",
    "    errors[np.array(chosen_parameter_indices)] = -1 # don't choose already chosen parameters again\n",
    "    mu_2_idx = np.argmax(errors)\n",
    "    mu_2 = params[mu_1_idx]\n",
    "    chosen_parameter_indices.append(mu_2_idx)\n",
    "    \n",
    "    maximum_error = np.max(errors)\n",
    "    print(\"mean error:\", np.mean(mean_errors))\n",
    "    mean_errors_list.append(np.mean(mean_errors))\n",
    "    print(\"mean error all:\", np.mean(mean_errors_all))\n",
    "    mean_errors_list_all.append(np.mean(mean_errors_all))\n",
    "    print(\"mu_2 index:\", mu_2_idx)\n",
    "\n",
    "    # 3b. Train Network_{mu_2}(u) to approximate Nonlinearity(u; mu_2)\n",
    "    print(\"\\nTraining network to approximate nonlinearity...\")\n",
    "    Network_mu_2 = Net(reduced_problem.N, reduced_problem.N)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(Network_mu_2.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9999)\n",
    "    \n",
    "    \n",
    "    # need to do Gram-Schmidt on this matrix\n",
    "    y_data = nonlinear_terms[mu_2_idx]\n",
    "    \n",
    "    for net in fixed_mu_networks:\n",
    "        # form matrix of evaluations for this network\n",
    "        previous_net_matrix = np.zeros((params.shape[0], nonlinear_terms.shape[2]))\n",
    "        for i, mu_i in enumerate(params):\n",
    "            previous_net_matrix[i] = y_normalization(net(x_data[i].view(1, -1)).T, normalize=False).T.detach().numpy().reshape(-1)\n",
    "        \n",
    "        # subtract out projection of y_data onto previous_net_matrix from y_data\n",
    "        y_data -= np.sum(y_data * previous_net_matrix) / np.linalg.norm(previous_net_matrix) * previous_net_matrix\n",
    "    \n",
    "    y_data = torch.tensor(y_data / np.linalg.norm(y_data)).float()\n",
    "    y_normalization = Normalization.MinMaxNormalization()\n",
    "    y_data = y_normalization(y_data.T).T\n",
    "    \n",
    "    \n",
    "\n",
    "    for epoch in range(30000):\n",
    "        optimizer.zero_grad()\n",
    "        output = Network_mu_2(x_data)\n",
    "        loss = criterion(output, y_data)\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    Network_mu_2.eval()\n",
    "    fixed_mu_networks.append(Network_mu_2)\n",
    "    y_normalization_list.append(y_normalization)\n",
    "\n",
    "    # 3c. Find theta_1_2(mu), theta_2_2(mu)\n",
    "    print(\"\\nFinding theta...\")\n",
    "    num_nets = len(fixed_mu_networks)\n",
    "    thetas = np.zeros((params.shape[0], num_nets))\n",
    "    for i, mu_i in enumerate(params):\n",
    "        LHS_numerator = np.zeros((num_nets, num_nets))\n",
    "        LHS_denominator = np.zeros((num_nets, num_nets))\n",
    "        RHS_numerator = np.zeros((num_nets, 1))\n",
    "        RHS_denominator = np.zeros((num_nets, 1))\n",
    "\n",
    "        for j, mu_j in enumerate(params):\n",
    "            nets_u_mu = [y_normalization_list[i_net](net(x_data[j].view(1, -1)).T, normalize=False).T.detach().numpy().reshape(-1) for i_net, net in enumerate(fixed_mu_networks)]\n",
    "\n",
    "            for k1 in range(num_nets):\n",
    "                RHS_numerator[k1] += np.dot(nonlinear_terms[i, j], nets_u_mu[k1])\n",
    "                RHS_denominator[k1] += np.dot(nets_u_mu[k1], nets_u_mu[k1])\n",
    "                for k2 in range(num_nets):\n",
    "                    \n",
    "                    LHS_numerator[k1, k2] += np.dot(nets_u_mu[k1], nets_u_mu[k2])\n",
    "                    LHS_denominator[k1, k2] += np.dot(nets_u_mu[k1], nets_u_mu[k1])\n",
    "\n",
    "\n",
    "\n",
    "        LHS = LHS_numerator / LHS_denominator\n",
    "        RHS = RHS_numerator / RHS_denominator\n",
    "        matrices.append(LHS)\n",
    "        thetas[i] = np.linalg.solve(LHS, RHS).reshape(-1)\n",
    "        #print(thetas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NonlinearityApprox(u, mu):\n",
    "    #thetas_ = theta_normalization(theta_net(torch.tensor(mu).float().view(1, -1)).detach().T, normalize=False).T.numpy().reshape(-1, 1)\n",
    "    thetas_ = interpolate.griddata(params, thetas, mu, method='cubic').reshape(-1, 1)\n",
    "    if True in np.isnan(thetas_):\n",
    "        thetas_ = interpolate.griddata(params, thetas, mu, method='nearest').reshape(-1, 1)\n",
    "    net_evals = np.array([y_normalization_list[i](net(x_normalization(torch.tensor(u).float().view(1, -1))).T, normalize=False).T.detach().numpy().reshape(-1) for i, net in enumerate(fixed_mu_networks)])\n",
    "    return np.sum(thetas_ * net_evals, axis=0)\n",
    "\n",
    "def NonlinearityApprox2(u, mu):\n",
    "    #thetas_ = theta_normalization(theta_net(torch.tensor(mu).float().view(1, -1)).T, normalize=False).T.view(-1)\n",
    "    thetas_ = interpolate.griddata(params, thetas, mu, method='cubic').reshape(-1)\n",
    "    if True in np.isnan(thetas_):\n",
    "        thetas_ = interpolate.griddata(params, thetas, mu, method='nearest').reshape(-1)\n",
    "    s = 0\n",
    "    for i, net in enumerate(fixed_mu_networks):\n",
    "        s += thetas_[i] * y_normalization_list[i](net(x_normalization(u.view(1, -1))).T, normalize=False).T.view(-1)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rbnics.backends.basic.wrapping.delayed_transpose import DelayedTranspose\n",
    "from rbnics.backends.online import OnlineFunction, OnlineVector\n",
    "from rbnics.backends.common.time_series import TimeSeries\n",
    "from rbnics.backends.dolfin.parametrized_tensor_factory import ParametrizedTensorFactory\n",
    "from rbnics.backends.dolfin.evaluate import evaluate\n",
    "\n",
    "class Approx_PINN_Loss(Losses.RONN_Loss_Base):\n",
    "    \"\"\"\n",
    "    PINN_Loss\n",
    "\n",
    "    ronn: object of type RONN\n",
    "\n",
    "    RETURNS: loss function loss_fn(parameters, reduced order coefficients)\n",
    "    \"\"\"\n",
    "    def __init__(self, ronn, normalization=None, beta=1., mu=None):\n",
    "        super(Approx_PINN_Loss, self).__init__(ronn, mu)\n",
    "        self.operators = None\n",
    "        self.proj_snapshots = None\n",
    "        self.T0_idx = None\n",
    "        self.normalization = normalization\n",
    "        if self.normalization is None:\n",
    "            self.normalization = IdentityNormalization()\n",
    "\n",
    "        self.beta = beta\n",
    "\n",
    "        # if time dependent, we need the neural net to compute time derivative\n",
    "        self.time_dependent = False\n",
    "\n",
    "    def name(self):\n",
    "        return \"Approx_PINN\"\n",
    "\n",
    "    def _compute_operators(self):\n",
    "        self.operators_initialized = True\n",
    "\n",
    "        #self.operators = self.ronn.get_operator_matrices(self.mu)\n",
    "        self.operators = self.ronn.get_reduced_operator_matrices(self.mu)\n",
    "\n",
    "        if not self.normalization.initialized:\n",
    "            self.normalization(self.ronn.get_projected_snapshots())\n",
    "\n",
    "    def set_mu(self, mu):\n",
    "        self.mu = mu\n",
    "        self.operators_initialized = False\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        pred = kwargs[\"prediction_no_snap\"]\n",
    "        if not self.operators_initialized:\n",
    "            self._compute_operators()\n",
    "\n",
    "        pred = self.normalization(pred.T, normalize=False).T\n",
    "\n",
    "        ##### 1st equation in system #####\n",
    "        res1 = 0.0\n",
    "\n",
    "        # these two could be combined when both not None\n",
    "        if 'f' in self.operators:\n",
    "            res1 -= self.operators['f']\n",
    "        if 'c' in self.operators:\n",
    "            self.operators['c'] = torch.zeros_like(self.operators['c'])\n",
    "            for i, mu in enumerate(kwargs[\"input_normalization\"](kwargs[\"normalized_mu\"], normalize=False)):                \n",
    "                mu = np.array(mu)\n",
    "                sol = pred[i].float()\n",
    "                C = NonlinearityApprox2(sol, mu).view(-1, 1).double()#.detach()\n",
    "                self.operators['c'][i] = C[None, :, :]\n",
    "\n",
    "            res1 += self.operators['c']\n",
    "        \n",
    "        if 'a' in self.operators:\n",
    "            res1 += torch.matmul(self.operators['a'], pred[:, :, None].double())\n",
    "\n",
    "        loss1 = torch.mean(torch.sum(res1**2, dim=1)) if type(res1) is not float else res1\n",
    "        if self.ronn.problem.dirichlet_bc_are_homogeneous:\n",
    "            boundary_condition_loss = 0\n",
    "        else:\n",
    "            boundary_condition_loss = torch.mean((pred[:, 0] - 1.)**2)\n",
    "\n",
    "        self.value = loss1 + self.beta*boundary_condition_loss\n",
    "\n",
    "        return self.value\n",
    "\n",
    "    def reinitialize(self, mu):\n",
    "        normalization = self.normalization\n",
    "        beta = self.beta\n",
    "        return Approx_PINN_Loss(self.ronn, normalization, beta, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 Train PINN\n",
    "\n",
    "Given a training set $X_{PINN} = (\\boldsymbol{\\mu}^{(1)}, \\dots, \\boldsymbol{\\mu}^{(n)})$ of parameters for the PDE, we train a Physics-Informed Neural Network (PINN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PINN}(X_{PINN}; W) = \\frac1n \\sum_{i=1}^n \\left\\|M(\\boldsymbol{\\mu^{(i)}})\\frac{\\partial \\operatorname{N}_W}{\\partial t}(\\boldsymbol{\\mu^{(i)}}) + A(\\boldsymbol{\\mu^{(i)}}) \\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\boldsymbol{f}(\\boldsymbol{\\mu}^{(i)}) + \\boldsymbol{c}(\\boldsymbol{\\mu}^{(i)})\\right\\|_2^2$$\n",
    "\n",
    "over $W$, where for a given $\\boldsymbol{\\mu}$, $M(\\boldsymbol{\\mu})$ is the mass matrix corresponding to the bilinear form $m$, $A(\\boldsymbol{\\mu})$ is the assembled matrix corresponding to the bilinear form $a$, $\\boldsymbol{f}(\\boldsymbol{\\mu})$ is the assembled vector corresponding to the linear form $f$, and $\\boldsymbol{c}(\\boldsymbol{\\mu})$ is a vector corresponding to the nonlinear form $c$. The partial derivative $\\frac{\\partial \\operatorname{N}_W}{\\partial t}(\\boldsymbol{\\mu^{(i)}})$ is computed via automatic differentiation in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pinn = Normalization.MinMaxNormalization(input_normalization=True)\n",
    "output_normalization_pinn = Normalization.MinMaxNormalization()\n",
    "\n",
    "pinn_net  = NN.RONN(\"PINN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pinn_loss = Approx_PINN_Loss(pinn_net, output_normalization_pinn)\n",
    "data      = RONNData.RONNDataLoader(pinn_net, validation_proportion=0.2, \n",
    "                                    num_without_snapshots=1)\n",
    "optimizer = torch.optim.Adam(pinn_net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.99999)\n",
    "\n",
    "pinn_trainer = Training.PINNTrainer(\n",
    "    pinn_net, data, pinn_loss, optimizer, scheduler,\n",
    "    input_normalization_pinn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pinn_net, data, pinn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pinn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pinn_trainer, pinn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 Train PDNN\n",
    "\n",
    "Given a training set $X_{PDNN} = ((\\boldsymbol{\\mu}^{(1)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(1)})), \\dots, (\\boldsymbol{\\mu}^{(n)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(n)})))$ of parameter and high fidelity solution pairs for the PDE, we train a Projection-Driven Neural Network (PDNN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "$$L_{PDNN}(X_{PDNN}; W) = \\frac1n \\sum_{i=1}^n \\|\\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu}^{(i)})\\|_2^2,$$\n",
    "where for a given $\\boldsymbol{\\mu}$, $\\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu})$ is the projection of $\\operatorname{HF}(\\boldsymbol{\\mu})$ onto the reduced order solution space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pdnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pdnn = Normalization.StandardNormalization()\n",
    "\n",
    "pdnn_net  = NN.RONN(\"PDNN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pdnn_loss = Losses.PDNN_Loss(pdnn_net, output_normalization_pdnn)\n",
    "data      = RONNData.RONNDataLoader(pdnn_net, validation_proportion=0.2)\n",
    "optimizer = torch.optim.Adam(pdnn_net.parameters(), lr=0.001)\n",
    "\n",
    "pdnn_trainer = Training.PDNNTrainer(\n",
    "    pdnn_net, data, pdnn_loss, optimizer,\n",
    "    input_normalization_pdnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pdnn_net, data, pdnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pdnn_trainer, pdnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Train PRNN\n",
    "\n",
    "We train a Physics-Reinforced Neural Network (PRNN) $N_W(\\boldsymbol{\\mu})$ dependnent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PRNN}(X_{PINN}, X_{PDNN}; W) = L_{PINN}(X_{PINN}; W) + \\omega L_{PDNN}(X_{PDNN}; W),$$\n",
    "\n",
    "where $\\omega$ is a scaling parameter which can be chosen freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_prnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_prnn = Normalization.StandardNormalization()\n",
    "\n",
    "omega = 1.\n",
    "prnn_net  = NN.RONN(f\"PRNN_{omega}\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "prnn_loss = Losses.PRNN_Loss(prnn_net, output_normalization_prnn, omega=omega)\n",
    "data      = RONNData.RONNDataLoader(prnn_net, validation_proportion=0.2,\n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(prnn_net.parameters(), lr=0.001)\n",
    "\n",
    "prnn_trainer = Training.PRNNTrainer(\n",
    "    prnn_net, data, prnn_loss, optimizer,\n",
    "    input_normalization_prnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    prnn_net, data, prnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(prnn_trainer, prnn_net, separate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Perform an error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 Reduction Method Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduction_method.initialize_testing_set(1)\n",
    "#reduction_method.error_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 PINN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mu = torch.tensor(reduction_method.testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pinn_net, test_mu, input_normalization_pinn, output_normalization_pinn, relative=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pinn_net, tuple(), input_normalization_pinn, output_normalization_pinn, t=0, component=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3 PDNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pdnn_net, test_mu, input_normalization_pdnn, output_normalization_pdnn, relative=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pdnn_net, tuple(), input_normalization_pdnn, output_normalization_pdnn, t=0, component=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.4 PRNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    prnn_net, test_mu, input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    prnn_net, tuple(), input_normalization_prnn, output_normalization_prnn, t=0, component=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.5 Neural Network Error Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = dict()\n",
    "nets[\"pinn_net\"] = pinn_net\n",
    "nets[\"pdnn_net\"] = pdnn_net\n",
    "nets[\"prnn_net\"] = prnn_net\n",
    "\n",
    "input_normalizations = dict()\n",
    "input_normalizations[\"pinn_net\"] = input_normalization_pinn\n",
    "input_normalizations[\"pdnn_net\"] = input_normalization_pdnn\n",
    "input_normalizations[\"prnn_net\"] = input_normalization_prnn\n",
    "\n",
    "output_normalizations = dict()\n",
    "output_normalizations[\"pinn_net\"] = output_normalization_pinn\n",
    "output_normalizations[\"pdnn_net\"] = output_normalization_pdnn\n",
    "output_normalizations[\"prnn_net\"] = output_normalization_prnn\n",
    "\n",
    "_ = ErrorAnalysis.error_analysis_by_network(\n",
    "    nets, test_mu, input_normalizations, output_normalizations, relative=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Perform a speedup analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method.speedup_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
