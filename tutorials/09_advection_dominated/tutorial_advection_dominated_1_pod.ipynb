{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 09 - Advection Dominated problem\n",
    "**_Keywords: POD-Galerkin method, SUPG_**\n",
    "\n",
    "### 1. Introduction\n",
    "This tutorial addresses the POD-Galerkin method to the advection dominated worked problem in a two-dimensional domain $\\Omega=(0,1)^2$.\n",
    "\n",
    "We introduce a stabilization technique such as $\\textit{Streamline/Upwind Petrov-Galerkin}$ (SUPG) able to reduce the numerical oscillations on the approximation of the solution of parametrized advection-diffusion problem:\n",
    "\n",
    "$$\n",
    "-\\varepsilon(\\boldsymbol{\\mu})\\,\\Delta u(\\boldsymbol{\\mu})+\\beta(\\boldsymbol{\\mu})\\cdot\\nabla u(\\boldsymbol{\\mu})=f(\\boldsymbol{\\mu})\\quad\\text{on }\\,\\Omega(\\boldsymbol{\\mu}),\n",
    "$$\n",
    "\n",
    "where $\\beta(\\boldsymbol{\\mu})$ and $\\varepsilon(\\boldsymbol{\\mu})$ represent the advection and the diffusion term, respectively.\n",
    "\n",
    "For this problem, we consider on parameter $\\mu$, thus $P=1$. It is related to the PÃ©clet number:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}e_K(\\boldsymbol{\\mu})(x):=\\frac{|\\beta(\\boldsymbol{\\mu})(x)| h_K}{2\\,\\varepsilon(\\boldsymbol{\\mu})(x)}\\quad\\forall x\\in K\\quad\\forall\\boldsymbol{\\mu}\\in\\mathbb{P}.\n",
    "$$\n",
    "\n",
    "Here $h_K$ represents the diameter of $K\\in\\mathcal{T}_h$, where $\\mathcal{T}_h$ indicates a triangulation of our domain $\\Omega(\\boldsymbol{\\mu})$.\n",
    "\n",
    "The parameter domain is thus given by \n",
    "$$\n",
    "\\mathbb{P}=[0, 6].\n",
    "$$\n",
    "\n",
    "In this problem we consider two approaches:\n",
    "1. Offline-Online stabilized,\n",
    "2. Offline-only stabilized,\n",
    "\n",
    "in which while in the first one we apply the SUPG method both in the Offline and Online phases, in the second one only in the Offline phase it is applied.\n",
    "\n",
    "In order to obtain a faster approximation of the problem, we pursue a model reduction by means of a POD-Galerkin reduced order method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parametrized formulation\n",
    "\n",
    "Let $u(\\boldsymbol{\\mu})$ be the solution in the domain $\\Omega$.\n",
    "\n",
    "The PDE formulation of the parametrized problem is given by: for a given parameter $\\mu=\\boldsymbol{\\mu}\\in\\mathbb{P}$, find $u(\\boldsymbol{\\mu})$ such that\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\t-\\frac{1}{10\\,^{\\boldsymbol{\\mu}}}\\Delta\\,u(\\boldsymbol{\\mu})+(1,1)\\cdot\\nabla u(\\boldsymbol{\\mu})=0 & \\text{in }\\Omega,\\\\\n",
    "    u(\\boldsymbol{\\mu}) = 0 & \\text{on } \\Gamma_1\\cup\\Gamma_2, \\\\ \n",
    "\tu(\\boldsymbol{\\mu}) = 1 & \\text{on } \\Gamma_3\\cup\\Gamma_4.\n",
    "\\end{cases}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "The corresponding weak formulation reads: for a given parameter $\\boldsymbol{\\mu}\\in\\mathbb{P}$, find $u(\\boldsymbol{\\mu})\\in\\mathbb{V}$ such that\n",
    "\n",
    "$$a\\left(u(\\boldsymbol{\\mu}),v;\\boldsymbol{\\mu}\\right)=f(v;\\boldsymbol{\\mu})\\quad \\forall v\\in\\mathbb{V},$$\n",
    "\n",
    "where\n",
    "\n",
    "* the function space $\\mathbb{V}$ is defined as\n",
    "$$\n",
    "\\mathbb{V} = \\left\\{ v \\in H^1(\\Omega): v|_{\\Gamma_1\\cup\\Gamma_2} = 0, v|_{\\Gamma_3\\cup\\Gamma_4} = 1\\right\\},\n",
    "$$\n",
    "* the parametrized bilinear form $a(\\cdot, \\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$a(u,v;\\boldsymbol{\\mu}) = \\int_{\\Omega} \\frac{1}{10\\,^{\\boldsymbol{\\mu}}}\\nabla u \\cdot \\nabla v +\\left(\\partial_xu+\\partial_yu\\right)v\\ d\\boldsymbol{x},$$\n",
    "* the parametrized linear form $f(\\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$f(v; \\boldsymbol{\\mu}) = \\int_{\\Omega} v\\ d\\boldsymbol{x}.$$\n",
    "\n",
    "For the $\\textit{Offline-Online stabilized}$ approach we use a different bilinear form $a_{stab}$ instead of $a$;\n",
    "\n",
    "while in the $\\textit{Offline-only stabilized}$ approach we use the the bilinear form $a_{stab}$ during the Offline phase, performing the Online Galerkin projection with respect to the bilinear form $a$,\n",
    "\n",
    "* the parametrized bilinear stabilized form $a_{stab}(\\cdot, \\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\times \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$a_{stab}(u,v,\\boldsymbol{\\mu}) = a(u,v,\\boldsymbol{\\mu}) + s(u,v,\\boldsymbol{\\mu}),$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    a(u,v;\\boldsymbol{\\mu}) &= \\int_{\\Omega} \\frac{1}{10\\,^{\\boldsymbol{\\mu}}}\\nabla u \\cdot \\nabla v +\\left[(1,1)\\cdot\\nabla u\\right]v\\ d\\boldsymbol{x},\\\\\n",
    "    s(u,v;\\boldsymbol{\\mu}) &= \\sum_{K\\in\\mathcal{T}_h}\\delta_K\\int_K \n",
    "    \\left(-\\frac{1}{10\\,^{\\boldsymbol{\\mu}}}\\Delta u+(1,1)\\cdot\\nabla u\\right)\\left(\\frac{h_K}{\\sqrt{2}}(1,1)\\cdot\\nabla v\\right)\\ d\\boldsymbol{x},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and\n",
    "* the parametrized linear form $f_{stab}(\\cdot; \\boldsymbol{\\mu}): \\mathbb{V} \\to \\mathbb{R}$ is defined by\n",
    "$$\n",
    "f_{stab}(v;\\boldsymbol{\\mu}) = f(v;\\boldsymbol{\\mu}) + r(v;\\boldsymbol{\\mu})\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(v;\\boldsymbol{\\mu}) &= \\int_{\\Omega} v\\ d\\boldsymbol{x}, \\\\\n",
    "    r(v;\\boldsymbol{\\mu}) &= \\sum_{K\\in\\mathcal{T}_h}\\delta_K\\int_K \\left(\\frac{h_K}{\\sqrt{2}}(1,1)\\cdot\\nabla v\\right)\\ d\\boldsymbol{x}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from mlnics import NN, Losses, Normalization, RONNData, IO, Training, ErrorAnalysis\n",
    "from dolfin import *\n",
    "from rbnics import *\n",
    "from problems import *\n",
    "from reduction_methods import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Affine decomposition\n",
    "\n",
    "For this problem the affine decomposition is straightforward:\n",
    "\n",
    "$$a(u,v;\\boldsymbol{\\mu})=\\underbrace{\\frac{1}{10\\,^{\\boldsymbol{\\mu}}}}_{\\Theta^{a}_0(\\boldsymbol{\\mu})}\\underbrace{\\int_{\\Omega}\\nabla u \\cdot \\nabla v \\ d\\boldsymbol{x}}_{a_0(u,v)} \\ + \\  \\underbrace{1}_{\\Theta^{a}_1(\\boldsymbol{\\mu})}\\underbrace{\\int_{\\Omega}\\left[(1,1)\\cdot\\nabla u\\right]v \\ d\\boldsymbol{x}}_{a_1(u,v)},$$\n",
    "$$f(v; \\boldsymbol{\\mu}) = \\underbrace{1}_{\\Theta^{f}_0(\\boldsymbol{\\mu})} \\underbrace{\\int_{\\Omega}v \\ d\\boldsymbol{x}}_{f_0(v)}.$$\n",
    "\n",
    "Adding the following forms, we obtaing the affine decomposition for the stabilized approach:\n",
    "\n",
    "$$s(u,v;\\boldsymbol{\\mu}) = \\sum_{K\\in\\mathcal{T}_h}\\underbrace{\\frac{\\delta_K}{10\\,^{\\boldsymbol{\\mu}}}}_{\\Theta^{s}_0(\\boldsymbol{\\mu})}\\underbrace{\\int_K \n",
    "    \\Delta u\\left(\\frac{h_K}{\\sqrt{2}}(1,1)\\cdot\\nabla v\\right)\\ d\\boldsymbol{x}}_{s_0(u,v)} \\ + \\\n",
    "    \\sum_{K\\in\\mathcal{T}_h}\\underbrace{\\delta_K}_{\\Theta^{s}_1(\\boldsymbol{\\mu})}\\underbrace{\\int_K \n",
    "    \\left((1,1)\\cdot\\nabla u\\right)\\left(\\frac{h_K}{\\sqrt{2}}(1,1)\\cdot\\nabla v\\right)\\ d\\boldsymbol{x}}_{s_1(u,v)},$$\n",
    "$$r(v; \\boldsymbol{\\mu}) = \\sum_{K\\in\\mathcal{T}_h}\\underbrace{\\delta_K}_{\\Theta^{r}_0(\\boldsymbol{\\mu})} \\underbrace{\\int_K\\left(\\frac{h_K}{\\sqrt{2}}(1,1)\\cdot\\nabla v\\right)\\ d\\boldsymbol{x}}_{r_0(v)}.$$\n",
    "We will implement the numerical discretization of the problem in the class\n",
    "```\n",
    "class AdvectionDominated(EllipticCoerciveProblem):\n",
    "```\n",
    "by specifying the coefficients $\\Theta^{a}_*(\\boldsymbol{\\mu})$ and $\\Theta^{f}_*(\\boldsymbol{\\mu})$ in the method\n",
    "```\n",
    "    def compute_theta(self, term):     \n",
    "```\n",
    "and the bilinear forms $a_*(u, v)$ and linear forms $f_*(v)$ in\n",
    "```\n",
    "    def assemble_operator(self, term):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@OnlineStabilization()\n",
    "class AdvectionDominated(EllipticCoerciveProblem):\n",
    "\n",
    "    # Default initialization of members\n",
    "    def __init__(self, V, **kwargs):\n",
    "        # Call the standard initialization\n",
    "        EllipticCoerciveProblem.__init__(self, V, **kwargs)\n",
    "        # ... and also store FEniCS data structures for assembly\n",
    "        assert \"subdomains\" in kwargs\n",
    "        assert \"boundaries\" in kwargs\n",
    "        self.subdomains, self.boundaries = kwargs[\"subdomains\"], kwargs[\"boundaries\"]\n",
    "        self.u = TrialFunction(V)\n",
    "        self.v = TestFunction(V)\n",
    "        self.dx = Measure(\"dx\")(subdomain_data=subdomains)\n",
    "        self.ds = Measure(\"ds\")(subdomain_data=boundaries)\n",
    "        # Store advection and forcing expressions\n",
    "        self.beta = Constant((1.0, 1.0))\n",
    "        self.f = Constant(1.0)\n",
    "        # Store terms related to stabilization\n",
    "        self.delta = 0.5\n",
    "        self.h = CellDiameter(V.mesh())\n",
    "\n",
    "    # Return custom problem name\n",
    "    def name(self):\n",
    "        return \"AdvectionDominated1POD\"\n",
    "\n",
    "    # Return theta multiplicative terms of the affine expansion of the problem.\n",
    "    def compute_theta(self, term):\n",
    "        mu = self.mu\n",
    "        if term == \"a\":\n",
    "            theta_a0 = 10.0**(- mu[0])\n",
    "            theta_a1 = 1.0\n",
    "            if self.stabilized:\n",
    "                delta = self.delta\n",
    "                theta_a2 = - delta * 10.0**(- mu[0])\n",
    "                theta_a3 = delta\n",
    "            else:\n",
    "                theta_a2 = 0.0\n",
    "                theta_a3 = 0.0\n",
    "            return (theta_a0, theta_a1, theta_a2, theta_a3)\n",
    "        elif term == \"f\":\n",
    "            theta_f0 = 1.0\n",
    "            if self.stabilized:\n",
    "                delta = self.delta\n",
    "                theta_f1 = delta\n",
    "            else:\n",
    "                theta_f1 = 0.0\n",
    "            return (theta_f0, theta_f1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for compute_theta().\")\n",
    "\n",
    "    # Return forms resulting from the discretization of the affine expansion of the problem operators.\n",
    "    def assemble_operator(self, term):\n",
    "        v = self.v\n",
    "        dx = self.dx\n",
    "        if term == \"a\":\n",
    "            u = self.u\n",
    "            beta = self.beta\n",
    "            h = self.h\n",
    "            a0 = inner(grad(u), grad(v)) * dx\n",
    "            a1 = inner(beta, grad(u)) * v * dx\n",
    "            a2 = inner(div(grad(u)), h * inner(beta, grad(v))) * dx\n",
    "            a3 = inner(inner(beta, grad(u)), h * inner(beta, grad(v))) * dx\n",
    "            return (a0, a1, a2, a3)\n",
    "        elif term == \"f\":\n",
    "            f = self.f\n",
    "            beta = self.beta\n",
    "            h = self.h\n",
    "            f0 = f * v * dx\n",
    "            f1 = inner(f, h * inner(beta, grad(v))) * dx\n",
    "            return (f0, f1)\n",
    "        elif term == \"k\":\n",
    "            u = self.u\n",
    "            k0 = inner(grad(u), grad(v)) * dx\n",
    "            return (k0,)\n",
    "        elif term == \"m\":\n",
    "            u = self.u\n",
    "            m0 = inner(u, v) * dx\n",
    "            return (m0,)\n",
    "        elif term == \"dirichlet_bc\":\n",
    "            bc0 = [DirichletBC(self.V, Constant(0.0), self.boundaries, 1),\n",
    "                   DirichletBC(self.V, Constant(0.0), self.boundaries, 2)]\n",
    "            return (bc0,)\n",
    "        elif term == \"inner_product\":\n",
    "            u = self.u\n",
    "            x0 = inner(grad(u), grad(v)) * dx\n",
    "            return (x0,)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid term for assemble_operator().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main program\n",
    "\n",
    "### 4.1. Read the mesh for this problem\n",
    "The mesh was generated by the [data/generate_mesh.ipynb](data/generate_mesh.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = Mesh(\"data/square.xml\")\n",
    "subdomains = MeshFunction(\"size_t\", mesh, \"data/square_physical_region.xml\")\n",
    "boundaries = MeshFunction(\"size_t\", mesh, \"data/square_facet_region.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create Finite Element space (Lagrange P2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = FunctionSpace(mesh, \"Lagrange\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Allocate an object of the AdvectionDominated class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = AdvectionDominated(V, subdomains=subdomains, boundaries=boundaries)\n",
    "mu_range = [(0.0, 6.0)]\n",
    "problem.set_mu_range(mu_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Prepare reduction with a POD-Galerkin method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method = PODGalerkin(problem)\n",
    "reduction_method.set_Nmax(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Perform the offline phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Fit Reduction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduction_method.initialize_training_set(50)\n",
    "reduced_problem = reduction_method.offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 Train PINN\n",
    "\n",
    "Given a training set $X_{PINN} = (\\boldsymbol{\\mu}^{(1)}, \\dots, \\boldsymbol{\\mu}^{(n)})$ of parameters for the PDE, we train a Physics-Informed Neural Network (PINN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PINN}(X_{PINN}; W) = \\frac1n \\sum_{i=1}^n \\left\\|A(\\boldsymbol{\\mu^{(i)}}) \\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\boldsymbol{f}(\\boldsymbol{\\mu}^{(i)})\\right\\|_2^2$$\n",
    "\n",
    "over $W$, where for a given $\\boldsymbol{\\mu}$, $A(\\boldsymbol{\\mu})$ is the assembled matrix corresponding to the bilinear form $a$ and $\\boldsymbol{f}(\\boldsymbol{\\mu})$ is the assembled vector corresponding to the linear form $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pinn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pinn = Normalization.StandardNormalization()\n",
    "\n",
    "pinn_net  = NN.RONN(\"PINN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pinn_loss = Losses.PINN_Loss(pinn_net, output_normalization_pinn)\n",
    "data      = RONNData.RONNDataLoader(pinn_net, validation_proportion=0.2, \n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(pinn_net.parameters(), lr=0.001)\n",
    "\n",
    "pinn_trainer = Training.PINNTrainer(\n",
    "    pinn_net, data, pinn_loss, optimizer,\n",
    "    input_normalization_pinn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pinn_net, data, pinn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pinn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pinn_trainer, pinn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 Train PDNN\n",
    "\n",
    "Given a training set $X_{PDNN} = ((\\boldsymbol{\\mu}^{(1)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(1)})), \\dots, (\\boldsymbol{\\mu}^{(n)}, \\operatorname{HF}(\\boldsymbol{\\mu}^{(n)})))$ of parameter and high fidelity solution pairs for the PDE, we train a Projection-Driven Neural Network (PDNN) $\\operatorname{N}_W(\\boldsymbol{\\mu})$ dependent on the weights and biases $W$ of the network to minimize the loss function\n",
    "$$L_{PDNN}(X_{PDNN}; W) = \\frac1n \\sum_{i=1}^n \\|\\operatorname{N}_W(\\boldsymbol{\\mu}^{(i)}) - \\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu}^{(i)})\\|_2^2,$$\n",
    "where for a given $\\boldsymbol{\\mu}$, $\\tilde{\\operatorname{HF}}(\\boldsymbol{\\mu})$ is the projection of $\\operatorname{HF}(\\boldsymbol{\\mu})$ onto the reduced order solution space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_pdnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_pdnn = Normalization.StandardNormalization()\n",
    "\n",
    "pdnn_net  = NN.RONN(\"PDNN\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "pdnn_loss = Losses.PDNN_Loss(pdnn_net, output_normalization_pdnn)\n",
    "data      = RONNData.RONNDataLoader(pdnn_net, validation_proportion=0.2)\n",
    "optimizer = torch.optim.Adam(pdnn_net.parameters(), lr=0.001)\n",
    "\n",
    "pdnn_trainer = Training.PDNNTrainer(\n",
    "    pdnn_net, data, pdnn_loss, optimizer,\n",
    "    input_normalization_pdnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    pdnn_net, data, pdnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(pdnn_trainer, pdnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Train PRNN\n",
    "\n",
    "We train a Physics-Reinforced Neural Network (PRNN) $N_W(\\boldsymbol{\\mu})$ dependnent on the weights and biases $W$ of the network to minimize the loss function\n",
    "\n",
    "$$L_{PRNN}(X_{PINN}, X_{PDNN}; W) = L_{PINN}(X_{PINN}; W) + \\omega L_{PDNN}(X_{PDNN}; W),$$\n",
    "\n",
    "where $\\omega$ is a scaling parameter which can be chosen freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_normalization_prnn = Normalization.StandardNormalization(input_normalization=True)\n",
    "output_normalization_prnn = Normalization.StandardNormalization()\n",
    "\n",
    "omega = 1.\n",
    "prnn_net  = NN.RONN(f\"PRNN_{omega}\", problem, reduction_method, n_hidden=2, n_neurons=40)\n",
    "prnn_loss = Losses.PRNN_Loss(prnn_net, output_normalization_prnn, omega=omega)\n",
    "data      = RONNData.RONNDataLoader(prnn_net, validation_proportion=0.2,\n",
    "                                    num_without_snapshots=100)\n",
    "optimizer = torch.optim.Adam(prnn_net.parameters(), lr=0.001)\n",
    "\n",
    "prnn_trainer = Training.PRNNTrainer(\n",
    "    prnn_net, data, prnn_loss, optimizer,\n",
    "    input_normalization_prnn, num_epochs=10000\n",
    ")\n",
    "\n",
    "loaded, starting_epoch = IO.initialize_parameters(\n",
    "    prnn_net, data, prnn_trainer, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prnn_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Training.plot_loss(prnn_trainer, prnn_net, separate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Perform an error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 Reduction Method Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduction_method.initialize_testing_set(100)\n",
    "reduction_method.error_analysis(online_stabilization=True, filename=\"error_analysis_with_stabilization\")\n",
    "reduction_method.error_analysis(online_stabilization=False, filename=\"error_analysis_without_stabilization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 PINN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mu = torch.tensor(reduction_method.testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pinn_net, test_mu, input_normalization_pinn, output_normalization_pinn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pinn_net, (6.0,), input_normalization_pinn, output_normalization_pinn, colorbar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3 PDNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    pdnn_net, test_mu, input_normalization_pdnn, output_normalization_pdnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    pdnn_net, (6.0,), input_normalization_pdnn, output_normalization_pdnn, colorbar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.4 PRNN Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ErrorAnalysis.error_analysis_fixed_net(\n",
    "    prnn_net, test_mu, input_normalization_prnn, output_normalization_prnn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErrorAnalysis.plot_solution_difference(\n",
    "    prnn_net, (6.0,), input_normalization_prnn, output_normalization_prnn, colorbar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.5 Neural Network Error Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = dict()\n",
    "nets[\"pinn_net\"] = pinn_net\n",
    "nets[\"pdnn_net\"] = pdnn_net\n",
    "nets[\"prnn_net\"] = prnn_net\n",
    "\n",
    "input_normalizations = dict()\n",
    "input_normalizations[\"pinn_net\"] = input_normalization_pinn\n",
    "input_normalizations[\"pdnn_net\"] = input_normalization_pdnn\n",
    "input_normalizations[\"prnn_net\"] = input_normalization_prnn\n",
    "\n",
    "output_normalizations = dict()\n",
    "output_normalizations[\"pinn_net\"] = output_normalization_pinn\n",
    "output_normalizations[\"pdnn_net\"] = output_normalization_pdnn\n",
    "output_normalizations[\"prnn_net\"] = output_normalization_prnn\n",
    "\n",
    "_ = ErrorAnalysis.error_analysis_by_network(\n",
    "    nets, test_mu, input_normalizations, output_normalizations, euclidean=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Perform a speedup analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_method.speedup_analysis(online_stabilization=True, filename=\"speedup_analysis_with_stabilization\")\n",
    "reduction_method.speedup_analysis(online_stabilization=False, filename=\"speedup_analysis_without_stabilization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
